{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Classifier\n",
    "## Assignment Preamble\n",
    "Please ensure you carefully read all of the details and instructions on the assignment page, this section, and the rest of the notebook. If anything is unclear at any time please post on the forum or ask a tutor well in advance of the assignment deadline.\n",
    "\n",
    "In addition to all of the instructions in the body of the assignment below, you must also follow the following technical instructions for all assignments in this unit. *Failure to do so may result in a grade of zero.*\n",
    "* [At the bottom of the page](#Submission-Test) is some code which checks you meet the submission requirements. You **must** ensure that this runs correctly before submission.\n",
    "* Do not modify or delete any of the cells that are marked as test cells, even if they appear to be empty.\n",
    "* Do not duplicate any cells in the notebook – this can break the marking script. Instead, insert a new cell (e.g. from the menu) and copy across any contents as necessary.\n",
    "\n",
    "Remember to save and backup your work regularly, and double-check you are submitting the correct version.\n",
    "\n",
    "This notebook is the primary reference for your submission. You may write code in separate `.py` files but it must be clearly imported into the notebook so that it runs without needing to reference those files, and you must explain clearly what functionality is contained in those files (through comments, markdown cells, etc).\n",
    "\n",
    "As always, **the work you submit for this assignment must be entirely your own.** Do not copy or work with other students. Do not copy answers that you find online. These assignments are designed to help improve your understanding first and foremost – the process of doing the assignment is part of *learning*. They are also used to assess your ability, and so you must uphold academic integrity. Submitting plagiarised work risks your entire place on your degree.\n",
    "\n",
    "**The pass mark for this assignment is 40%.** We expect that students, on average, will be able to produce a submission which gets a mark between 50-70% within the normal workload allocation for the unit, but this will vary depending on individual backgrounds. Please ask for help if you are struggling.\n",
    "\n",
    "## Getting Started\n",
    "Spam refers to unwanted email, often in the form of advertisements. In the literature, an email that is **not** spam is called *ham*. Most email providers offer automatic spam filtering, where spam emails will be moved to a separate inbox based on their contents. Of course this requires being able to scan an email and determine whether it is spam or ham, a classification problem. This is the subject of this assignment.\n",
    "\n",
    "This assignment has two parts. Each part is worth 50% of the overall grade for this assignment.\n",
    "\n",
    "For part one you will write a supervised learning based classifier to determine whether a given email is spam or ham. You must write and submit the code in this notebook. The training data is provided for you. You may use any classification method. Marks will be awarded primarily based on the accuracy of your classifier on unseen test data, but there are also marks for estimating how accurate you think your classifier will be.\n",
    "\n",
    "In part two you will produce a short video explaining your implementation, any decisions or extensions you made, and what parameter values you used. This part is explained in more detail on the assignment page. The video file must be submitted with your assignment.\n",
    "\n",
    "### Choice of Algorithm\n",
    "While the classification method is a completely free choice, the assignment folder includes [a separate notebook file](data/naivebayes.ipynb) which can help you implement a Naïve Bayes solution. If you do use this notebook, you are still responsible for porting your code into *this* notebook for submission. A good implementation should give a high  enough accuracy to get a good grade on this section (50-70%).\n",
    "\n",
    "You could also consider a k-nearest neighbour algorithm, but this may be less accurate. Logistic regression is another option that you may wish to consider.\n",
    "\n",
    "If you are looking to go beyond the scope of the unit, you might be interested in building something more advanced, like an artificial neural network. This is possible just using `numpy`, but will require significant self-directed learning. *Extensions like this are left unguided and are not factored into the unit workload estimates.*\n",
    "\n",
    "**Note:** you may use helper functions in libraries like `numpy` or `scipy`, but you **must not** import code which builds entire models for you. This includes but is not limited to use of libraries like `scikit-learn`, `tensorflow`, or `pytorch` – there will be plenty of opportunities for these libraries in later units. The point of this assignment is to understand code the actual algorithm yourself. ***If you are in any doubt about any particular library or function please ask a tutor.*** Submissions which ignore this will receive penalties or even zero marks.\n",
    "\n",
    "If you choose to implement more than one algorithm, please feel free to include your code and talk about it in part two (your video presentation), but only the code in this notebook will be used in the automated testing.\n",
    "\n",
    "## Training Data\n",
    "The training data is described below and has 1000 rows. There is also a 500 row set of test data. These are functionally identical to the training data, they are just in a separate csv file to encourage you to split out your training and test data. You should consider how to best make use of all available data without overfitting, and to help produce an unbiased estimate for your classifier's accuracy.\n",
    "\n",
    "The cell below loads the training data into a variable called `training_spam`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the spam training data set: (1000, 55)\n",
      "[[1 0 0 ... 0 0 0]\n",
      " [0 0 1 ... 1 0 0]\n",
      " [0 0 0 ... 1 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 1]\n",
      " [1 1 1 ... 1 1 0]\n",
      " [1 0 0 ... 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from IPython.display import HTML,Javascript, display\n",
    "\n",
    "training_spam = np.loadtxt(open(\"data/training_spam.csv\"), delimiter=\",\").astype(int)\n",
    "print(\"Shape of the spam training data set:\", training_spam.shape)\n",
    "print(training_spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your training set consists of 1000 rows and 55 columns. Each row corresponds to one email message. The first column is the _response_ variable and describes whether a message is spam `1` or ham `0`. The remaining 54 columns are _features_ that you will use to build a classifier. These features correspond to 54 different keywords (such as \"money\", \"free\", and \"receive\") and special characters (such as \":\", \"!\", and \"$\"). A feature has the value `1` if the keyword appears in the message and `0` otherwise.\n",
    "\n",
    "As mentioned there is also a 500 row set of *test data*. It contains the same 55 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the spam testing data set: (500, 55)\n",
      "[[1 0 0 ... 1 1 1]\n",
      " [1 1 0 ... 1 1 1]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 1 0 0]\n",
      " [0 0 0 ... 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "testing_spam = np.loadtxt(open(\"data/testing_spam.csv\"), delimiter=\",\").astype(int)\n",
    "print(\"Shape of the spam testing data set:\", testing_spam.shape)\n",
    "print(testing_spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part One\n",
    "Write all of the code for your classifier below this cell. There is some very rough skeleton code in the cell directly below. You may insert more cells below this if you wish, but you must not duplicate any cells as this can break the grading script.\n",
    "\n",
    "### Submission Requirements\n",
    "Your code must provide a variable with the name `classifier`. This object must have a method called `predict` which takes input data and returns class predictions. The input will be a single $n \\times 54$ numpy array, your classifier should return a numpy array of length $n$ with classifications. There is a demo in the cell below, and a test you can run before submitting to check your code is working correctly.\n",
    "\n",
    "Your code must run on our test machine in under 30 seconds. If you wish to train a more complicated model (e.g. neural network) which will take longer, you are welcome to save the model's weights as a file and then load these in the cell below so we can test it. You must include the code which computes the original weights, but this must not run when we run the notebook – comment out the code which actually executes the routine and make sure it is clear what we need to change to get it to run. Remember that we will be testing your final classifier on additional hidden data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This skeleton code simply classifies every input as ham\n",
    "#\n",
    "# Here you can see there is a parameter k that is unused, the\n",
    "# point is to show you how you could set up your own. You might\n",
    "# also pass in extra data via a train method (also does nothing\n",
    "# here). Modify this code as much as you like so long as the \n",
    "# accuracy test in the cell below runs.\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "class SpamClassifier:\n",
    "    def __init__(self):\n",
    "        self.tree=None\n",
    "        self.rules=None\n",
    "        \n",
    "    # ------------------------------------------------------------\n",
    "    def entropy(self, prob_yes):\n",
    "        '''\n",
    "        Entropy is a measure of the impurity in the data.\n",
    "        It's 0 if every record has the same value (either NO or YES).\n",
    "        It's 1 if there is an equal number of records with YES and NO.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        prob_yes : float\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        entropy : float\n",
    "            defined as E = -(P_yes * log2P_yes + P_no * log2P_no)\n",
    "            if prob_yes is 0 or 1, entropy is 0 \n",
    "        '''\n",
    "        if prob_yes in [0,1]: # only works for binary outcomes because 1 is the max\n",
    "            return 0.0\n",
    "\n",
    "        return -(prob_yes*np.log2(prob_yes)+(1-prob_yes)*np.log2(1-prob_yes)).round(5)\n",
    "\n",
    "    \n",
    "    # ------------------------------------------------------------\n",
    "    def conditional_entropy(self, data, attribute, values=[0,1]):\n",
    "        '''\n",
    "        Estimate the entropy of a given attribute. An attribute with two distinct values divides\n",
    "        the training set into two subsets. Each subset has pk positive examples and nk negative examples.\n",
    "        Here, the remainder(attribute) is calculated as the sum of the calculated entropy of .. \n",
    "        .. the \"yes probability\" (email is spam) for each subset, weighted by the probability of ..\n",
    "        .. (pk+nk)/training set size. For more information see ..\n",
    "        .. p. 662, Chapter 19, AI A Modern Approach 4th Edition by Russell & Norvig\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        data : numpy array of N rows x M=55 columns,\n",
    "               where M_0 refers to the response variable\n",
    "               and M_1 to M_54 refer to the attributes\n",
    "        \n",
    "        attribute : float ranging from 1 to 54 inclusive\n",
    "        \n",
    "        values : a list of values (this implementations supports only binary values)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        remainder : float          \n",
    "        '''\n",
    "        remainder = 0\n",
    "        # for each attribute possible value (i.e. 0 and 1)\n",
    "        for value in values:\n",
    "            # create a filter for the given attribute e.g. data[:, 2]==0\n",
    "            filt = data[:,attribute]==value\n",
    "            # and reduce the (response variable) data to those records only\n",
    "            filtered_data = data[:,0][filt]\n",
    "            # calculate entropy for that set if the array is non-empty\n",
    "            if filtered_data.shape[0]>0:\n",
    "                # first calculate the probability of \"yes\" for the current attribute value\n",
    "                prob = filtered_data.sum()/filtered_data.shape[0]\n",
    "                # calculate its entropy and weight by the number of responses in the data\n",
    "                remainder += (filtered_data.shape[0]/data.shape[0])*self.entropy(prob)\n",
    "            # if the array is empty, move on\n",
    "            else:\n",
    "                remainder += 0\n",
    "\n",
    "        return remainder\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    def info_gain(self, data, avail_attributes):\n",
    "        '''\n",
    "        Estimate the information gain which is defined as the expected reduction in entropy.\n",
    "        That is IG(S,A) = Entropy(S) - Remainder(S, A), where A is an attribute and S the dataset.\n",
    "        So, for a given training set S we must select the A for which the IG is the biggest.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        data : numpy array of N rows x M=55 columns,\n",
    "               where M_0 refers to the response variable\n",
    "               and M_1 to M_54 refer to the attributes\n",
    "        \n",
    "        avail_attributes : list of floats, ranging from 1 to 54 inclusive\n",
    "                           attributes are dropped as the tree is being built\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        a list of tuples where each tuple is (information gain, attribute), for example (0.2, 54)\n",
    "        '''\n",
    "        \n",
    "        try:\n",
    "            # the training set size (number of records)\n",
    "            total_counts = data[:,0].shape[0]\n",
    "\n",
    "            if total_counts>0:\n",
    "                # count spam instances\n",
    "                positive = data[:,0].sum()\n",
    "                # calc spam share as % of total\n",
    "                pos_prob = positive/total_counts\n",
    "                # calculate entropy\n",
    "                B = self.entropy(pos_prob)\n",
    "                \n",
    "                # calc ham share as % of total\n",
    "                # neg_prob = 1-pos_prob\n",
    "                # B = -(pos_prob*np.log2(pos_prob) + neg_prob*np.log2(neg_prob))\n",
    "                \n",
    "                info_gain_list = []\n",
    "                # for every attribute on the list\n",
    "                for att in avail_attributes:\n",
    "                    # calculate the remainder value\n",
    "                    remainder = self.conditional_entropy(data, att)\n",
    "                    # store in the list\n",
    "                    info_gain_list.append(((B-remainder).round(4),att))\n",
    "                \n",
    "                # return a sorted list of attributes (based on IG)\n",
    "                return sorted(info_gain_list,reverse=True)\n",
    "            \n",
    "            # if there are no records return an empty list\n",
    "            else:\n",
    "                return []\n",
    "        except:\n",
    "            return []\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    def most_common_value(self, numpy_bin_arr):\n",
    "        '''\n",
    "        Return the most common value in a binary array.\n",
    "        Specifically, it returns 1 if the sum of the array is >= than half ..\n",
    "        .. the size of the array and 0 otherwise.\n",
    "        Parameters\n",
    "        ----------\n",
    "        numpy_bin_arr : numpy array comprised of 0s and 1s\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        return 1 or 0\n",
    "        '''\n",
    "        return (numpy_bin_arr.sum()>=numpy_bin_arr.shape[0]*0.5)*1\n",
    "    \n",
    "    # ------------------------------------------------------------\n",
    "    def decision_tree_learning(self, examples, attributes, parent_examples, min_sample, max_depth, counter):\n",
    "        '''\n",
    "        This function estimates the decision tree. It uses the information gain criterion to select the \n",
    "        attribute that reduces entropy the most and then splits the dataset based on the selected attribute.\n",
    "        Recursion is used to continue selecting the best attribute and splitting the training set into subsets.\n",
    "        This process continues until one of the following stopping criteria are met:\n",
    "        - all examples in the subset have the same class\n",
    "        - there are no attributes to split the data with\n",
    "        - the number of observations in the subset is less than a minimum number of observations (min_sample)\n",
    "        - maximum depth is reached\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        examples : numpy array of N rows x M=55 columns,\n",
    "                   where M_0 refers to the response variable\n",
    "                   and M_1 to M_54 refer to the attributes\n",
    "        \n",
    "        attributes : list of floats, ranging from 1 to 54 inclusive\n",
    "                     attributes are dropped as the tree is being built\n",
    "                     \n",
    "        parent_examples : int, 1 or 0 (provided by the most_common_value function)\n",
    "                          represents the default value in case no records exist for the selected\n",
    "                          combination of attribute values\n",
    "        \n",
    "        min_sample : int, for example 50\n",
    "        \n",
    "        max_depth : int, for example 4\n",
    "        \n",
    "        counter : int, for example 0\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        tree : a nested dictionary with the following keys\n",
    "                feature, samples, ig, level, condition, prediction, reason\n",
    "        '''\n",
    "        n_obs = examples.shape[0]\n",
    "        \n",
    "        #print('counter',counter,'\\n','list',str(attributes),'\\n')\n",
    "        #print('missing', [i for i in range(1,55) if i not in attributes])\n",
    "        #print('-------')\n",
    "        \n",
    "        # if the number of records is less than the min sample required, return the default class\n",
    "        if n_obs < min_sample: #this condition contains the case where n_obs = 0 as min_sample>0\n",
    "            return {'prediction':parent_examples,'level':counter,'samples':n_obs,'reason':'not adequate sample'}\n",
    "\n",
    "        # if every example has the same class, return that class\n",
    "        elif np.all(examples[:,0]==examples[0,0]):\n",
    "            return {'prediction':examples[0,0],'level':counter,'samples':n_obs,'reason':'single class'}\n",
    "\n",
    "        # if there are no attributes left to check, return the most common (examples) class\n",
    "        elif len(attributes)==0:\n",
    "            # this returns 1 if the \"1s\" are mte to 50% of the observations, and 0 otherwise\n",
    "            return {'prediction':self.most_common_value(examples[:,0]),'level':counter,\n",
    "                    'samples':n_obs,'reason':'no attributes left'}\n",
    "\n",
    "        else:\n",
    "            # if the max depth hasn't been reached\n",
    "            if max_depth>0:\n",
    "                # calculate the information gain for each attribute\n",
    "                list_of_att_by_ig = self.info_gain(examples, attributes)\n",
    "\n",
    "                # extract the best attribute and gain\n",
    "                best_info_gain, best_attribute = list_of_att_by_ig[0]\n",
    "\n",
    "                tree = {'feature':best_attribute,\n",
    "                         'samples':n_obs,\n",
    "                         'ig':best_info_gain,\n",
    "                         'level':counter\n",
    "                         }\n",
    "\n",
    "                # remove the attribute from the list of attributes to consider\n",
    "                attributes_temp = [a for a in attributes if a!=best_attribute]\n",
    "\n",
    "                # the attribute's value is equal to either 0 or 1\n",
    "                mapp = {0:'left',1:'right'}\n",
    "                for v in mapp.keys():\n",
    "\n",
    "                    # reduce examples using the best attribute's values\n",
    "                    examples_temp = examples[examples[:,best_attribute]==v]\n",
    "\n",
    "\n",
    "                    # go down the branch for the subset given v (recursion)\n",
    "                    subtree = self.decision_tree_learning(\n",
    "                                         examples=examples_temp,\n",
    "                                         attributes=attributes_temp,\n",
    "                                         parent_examples=self.most_common_value(examples_temp[:,0]),#examples[:,0]\n",
    "                                         min_sample=min_sample,\n",
    "                                         max_depth=max_depth-1,# reduce remaining depth\n",
    "                                         counter=counter+1 # increase level ie counter\n",
    "                                        )\n",
    "                    # store the tree and add \"condition\" which is a label of the attribute's column\n",
    "                    # and the current value, e.g. Feat3==1\n",
    "                    tree[mapp[v]] = {**subtree,\n",
    "                                         'condition':f\"Feat{str(tree['feature'])}==\"+str(v)}\n",
    "\n",
    "                return tree\n",
    "\n",
    "            # when/if the max depth is reached\n",
    "            else:\n",
    "\n",
    "                return {'prediction':self.most_common_value(examples[:,0]),\n",
    "                        'level':counter,'samples':n_obs,'reason':'max depth reached'}\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    def extract_rules(self, tree):\n",
    "        '''\n",
    "        Parse the tree to generate its rules.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        tree : a nested dictionary with the following keys\n",
    "                feature, samples, ig, level, condition, prediction, reason\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        rules_dict : a nested dictionary with rules for each tree attribute\n",
    "        \n",
    "        '''\n",
    "        rules_dict = dict()\n",
    "        if 'prediction' in tree:\n",
    "            feat = tree['condition'].replace(\"Feat\",\"\")\n",
    "            rules_dict[feat] = tree['prediction']\n",
    "            return rules_dict\n",
    "        else:\n",
    "            if 'condition' not in tree:\n",
    "                tree['condition'] = None\n",
    "            if tree['condition'] is not None:\n",
    "                feat = tree['condition'].replace(\"Feat\",\"\")\n",
    "                rules_dict[feat] = {**self.extract_rules(tree['left']), **self.extract_rules(tree['right'])}\n",
    "                return rules_dict\n",
    "            else:\n",
    "                return {**self.extract_rules(tree['left']),**self.extract_rules(tree['right'])}\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    def predict_one(self, tesing_data_arr, rules):\n",
    "        '''\n",
    "        Predict the response variable based on the test data for a single record.\n",
    "        Uses recursion to read the parsed rules.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        tesing_data_arr : numpy array of 1 row x M=55 columns,\n",
    "               where M_0 refers to the response variable\n",
    "               and M_1 to M_54 refer to the attributes\n",
    "        \n",
    "        rules : dictionary of the rules (as provided by the function extract_rules)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        int : 0 or 1\n",
    "        '''\n",
    "        for i in rules.keys():\n",
    "            col = int(i.split('==')[0])\n",
    "            val = int(i.split('==')[1])\n",
    "\n",
    "            if tesing_data_arr[col] == val:\n",
    "                if isinstance(rules[i], dict):\n",
    "                    return self.predict_one(tesing_data_arr, rules[i])\n",
    "\n",
    "                else:\n",
    "                    return rules[i]\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    def predict(self, data):\n",
    "        '''\n",
    "        Predict the response variable based on the test data.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        data : numpy array of N rows x 54 columns with attributes\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        a numpy array of size N\n",
    "        '''\n",
    "        # if the data is one dimensional [N=1, or shape = (54,)] then reshape into (1,54)\n",
    "        if data.ndim==1:\n",
    "            data = data.reshape(1,data.shape[0]).copy()\n",
    "    \n",
    "        # IMPORTANT!\n",
    "        # data is an N x 54 array\n",
    "        # so we need to add an additional column at position 0 (it will not be used but must be there ..\n",
    "        # .. to ensure rules consistency, otherwise a rule which is based for example on column 1 ..\n",
    "        # .. will be read as a rule at column 0 and so forth)\n",
    "        revised_data = np.column_stack([np.zeros(data.shape[0]),data])\n",
    "        \n",
    "        predictions = []\n",
    "        # predictions are made individually for each email record\n",
    "        for row in range(revised_data.shape[0]):\n",
    "            predictions.append(self.predict_one(revised_data[row], self.rules))\n",
    "        return np.array(predictions)\n",
    "    \n",
    "    # ------------------------------------------------------------\n",
    "    def train(self, training_data, min_sample=None, max_depth=None):\n",
    "        \n",
    "        if max_depth is None:\n",
    "            max_depth = 100 #training_data.shape[0]\n",
    "        if min_sample is None:\n",
    "            min_sample = 2\n",
    "        \n",
    "        \n",
    "        # learn the tree\n",
    "        # the attributes param is a list of integer values [1,2,3,...,54] representing attributes columns\n",
    "        tree = self.decision_tree_learning(training_data,\n",
    "                                           attributes=[i for i in range(1,training_data.shape[1])],\n",
    "                                           parent_examples=None,#self.most_common_value(training_data[:,0])\n",
    "                                           min_sample=min_sample, # e.g n=50\n",
    "                                           max_depth=max_depth, # max depth set at 4 node levels\n",
    "                                           counter=0 # initialise the counter at 0\n",
    "                                          )\n",
    "        # parse rules\n",
    "        self.rules = self.extract_rules(tree)\n",
    "        \n",
    "        # set internal tree parameter\n",
    "        self.tree = tree\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# def cross_validation(data, k, min_sample=None, max_depth=None):\n",
    "#     '''\n",
    "#     Model cross validation based on training dataset data\n",
    "#     '''\n",
    "#     # split data into k folds\n",
    "#     N = data.shape[0]\n",
    "#     fold_size = N//k\n",
    "    \n",
    "#     # shuffle indices\n",
    "#     indices = np.random.permutation(N)\n",
    "    \n",
    "#     accuracy_ls = []\n",
    "#     for fold in range(k):\n",
    "#         # create validation test\n",
    "#         validation_indices = indices[fold * fold_size: (fold + 1) * fold_size]\n",
    "#         validation_set = data[validation_indices,:]\n",
    "        \n",
    "#         # create training set\n",
    "#         training_indices = np.concatenate([indices[:fold * fold_size], indices[(fold + 1) * fold_size:]])\n",
    "#         training_set = data[training_indices,:]\n",
    "        \n",
    "#         if len(set(validation_indices)&set(training_indices))>0:\n",
    "#             print(\"Common indices found\")\n",
    "        \n",
    "#         cl = create_classifier(training_set, min_sample, max_depth)\n",
    "#         pred = cl.predict(validation_set[:,1:])\n",
    "#         labels = validation_set[:,0]\n",
    "#         accur = np.count_nonzero(pred == labels)/labels.shape[0]\n",
    "#         accuracy_ls.append(accur)\n",
    "\n",
    "#     cval_res = [k, min_sample, max_depth, fold_size, np.mean(accuracy_ls).round(3), np.median(accuracy_ls).round(3),\n",
    "#                 np.std(accuracy_ls).round(3), np.max(accuracy_ls).round(3), np.min(accuracy_ls).round(3),\n",
    "#                 accuracy_ls\n",
    "#                ]\n",
    "#     return cval_res\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# def print_tree(tree,prefix='-'):\n",
    "#     if 'condition' not in tree:\n",
    "#         tree['condition'] = None\n",
    "#     if 'feature' not in tree:\n",
    "#         tree['feature'] = None\n",
    "#     if 'ig' not in tree:\n",
    "#         tree['ig'] = None\n",
    "\n",
    "        \n",
    "#     if 'prediction' not in tree.keys():\n",
    "#         print(prefix[:-1]+\"|\",\"Lvl:\", tree['level'], '| Cond:', tree['condition'], \n",
    "#               '| Feat:', tree['feature'], '| IG:',tree['ig'],'| Samples:',tree['samples'])\n",
    "#     if 'left' in tree.keys():\n",
    "#         print_tree(tree['left'],prefix=prefix+'-')\n",
    "#     if 'right' in tree.keys():\n",
    "#         print_tree(tree['right'],prefix=prefix+'-')\n",
    "#     if 'prediction' in tree.keys():\n",
    "#         print(prefix[:-1]+'>',\"Lvl:\",tree['level'], '| Cond:', tree['condition'], \n",
    "#               '| Feat:', tree['feature'], '| IG:',tree['ig'],'| Samples:',\n",
    "#               tree['samples'],' | Prediction:',tree['prediction'],'| Reason:',tree['reason'])\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "def create_classifier(training_data, min_sample=None, max_depth=None):\n",
    "    \n",
    "    classifier = SpamClassifier()\n",
    "    \n",
    "    classifier.train(training_data=training_data, \n",
    "                     min_sample=min_sample,\n",
    "                     max_depth=max_depth)\n",
    "    \n",
    "    return classifier\n",
    "# ------------------------------------------------------------\n",
    "classifier = create_classifier(training_spam, min_sample=50, max_depth=5)\n",
    "\n",
    "# print out tree and rules\n",
    "# print(\"Rules:\",\"\\n\", classifier.rules,\"\\n\")\n",
    "# print(\"Tree:\")\n",
    "# print_tree(classifier.tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Estimate\n",
    "In the cell below there is a function called `my_accuracy_estimate()` which returns `0.5`. Before you submit the assignment, write your best guess for the accuracy of your classifier into this function, as a percentage between `0` and `1`. So if you think you will get 80% of inputs correct, return the value `0.8`. This will form a small part of the marking criteria for the assignment, to encourage you to test your own code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_accuracy_estimate():\n",
    "    return 0.85"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write all of the code for your classifier above this cell.\n",
    "\n",
    "### Testing Details\n",
    "Your classifier will be tested against some hidden data from the same source as the original. The accuracy (percentage of classifications correct) will be calculated, then benchmarked against common methods. At the very high end of the grading scale, your accuracy will also be compared to the best submissions from other students (in your own cohort and others!). Your estimate from the cell above will also factor in, and you will be rewarded for being close to your actual accuracy (overestimates and underestimates will be treated the same).\n",
    "\n",
    "#### Test Cell\n",
    "The following code will run your classifier against the provided test data. To enable it, set the constant `SKIP_TESTS` to `False`.\n",
    "\n",
    "The original skeleton code above classifies every row as ham, but once you have written your own classifier you can run this cell again to test it. So long as your code sets up a variable called `classifier` with a method called `predict`, the test code will be able to run. \n",
    "\n",
    "Of course you may wish to test your classifier in additional ways, but you *must* ensure this version still runs before submitting.\n",
    "\n",
    "**IMPORTANT**: you must set `SKIP_TESTS` back to `True` before submitting this file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SKIP_TESTS = True\n",
    "\n",
    "if not SKIP_TESTS:\n",
    "    testing_spam = np.loadtxt(open(\"data/testing_spam.csv\"), delimiter=\",\").astype(int)\n",
    "    test_data = testing_spam[:, 1:]\n",
    "    test_labels = testing_spam[:, 0]\n",
    "\n",
    "    predictions = classifier.predict(test_data)\n",
    "    accuracy = np.count_nonzero(predictions == test_labels)/test_labels.shape[0]\n",
    "    print(f\"Accuracy on test data is: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "57d38fa58f242d4d856fbaa18e9f8768",
     "grade": false,
     "grade_id": "cell-b6c47ab23c28b2b9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;32m[✓]\u001b[0m 'SKIP_TESTS' is set to true.\u001b[0m\n",
      "\u001b[1;32m[✓]\u001b[0m The notebook name is correct.\u001b[0m\n",
      "\u001b[1;32m[✓]\u001b[0m The create_classifier function has been defined.\u001b[0m\n",
      "\u001b[1;32m[✓]\u001b[0m The classifer variable has been correctly defined.\u001b[0m\n",
      "\u001b[1;32m[✓]\u001b[0m The my_accuracy_estimate function has been defined correctly.\u001b[0m\n",
      "\u001b[1;32m[✓]\u001b[0m Success running test set - Accuracy was 87.00%.\u001b[0m\n",
      "\u001b[1m\n",
      "\n",
      "\n",
      "╔═══════════════════════════════════════════════════════════════╗\n",
      "║                        Congratulations!                       ║\n",
      "║                                                               ║\n",
      "║            Your work meets all the required criteria          ║\n",
      "║                   and is ready for submission.                ║\n",
      "╚═══════════════════════════════════════════════════════════════╝\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pathlib\n",
    "\n",
    "fail = False;\n",
    "\n",
    "success = '\\033[1;32m[✓]\\033[0m'\n",
    "issue = '\\033[1;33m[!]'\n",
    "error = '\\033[1;31m\\t✗'\n",
    "\n",
    "#######\n",
    "##\n",
    "## Skip Tests check.\n",
    "##\n",
    "## Test to ensure the SKIP_TESTS variable is set to True to prevent it slowing down the automarker.\n",
    "##\n",
    "#######\n",
    "\n",
    "if not SKIP_TESTS:\n",
    "    fail = True;\n",
    "    print(\"{} \\'SKIP_TESTS\\' is incorrectly set to False.\\033[0m\".format(issue))\n",
    "    print(\"{} You must set the SKIP_TESTS constant to True in the cell above.\\033[0m\".format(error))\n",
    "else:\n",
    "    print('{} \\'SKIP_TESTS\\' is set to true.\\033[0m'.format(success))\n",
    "\n",
    "#######\n",
    "##\n",
    "## File Name check.\n",
    "##\n",
    "## Test to ensure file has the correct name. This is important for the marking system to correctly process the submission.\n",
    "##\n",
    "#######\n",
    "    \n",
    "p3 = pathlib.Path('./spamclassifier.ipynb')\n",
    "if not p3.is_file():\n",
    "    fail = True\n",
    "    print(\"{} The notebook name is incorrect.\\033[0m\".format(issue))\n",
    "    print(\"{} This notebook file must be named spamclassifier.ipynb\\033[0m\".format(error))\n",
    "else:\n",
    "    print('{} The notebook name is correct.\\033[0m'.format(success))\n",
    "\n",
    "#######\n",
    "##\n",
    "## Create classifier function check.\n",
    "##\n",
    "## Test that checks the create_classifier function exists. The function should train the classifier and return it so that it can be evaluated by the marking system.\n",
    "##\n",
    "#######\n",
    "\n",
    "if \"create_classifier\" not in dir():\n",
    "    fail = True;\n",
    "    print(\"{} The create_classifier function has not been defined.\\033[0m\".format(issue))\n",
    "    print(\"{} Your code must include a create_classifier function as described in the coursework specification.\\033[0m\".format(error))\n",
    "    print(\"{} If you believe you have, \\'restart & run-all\\' to clear this error.\\033[0m\".format(error))\n",
    "else:\n",
    "    print('{} The create_classifier function has been defined.\\033[0m'.format(success))\n",
    "\n",
    "#######\n",
    "##\n",
    "## Classifier variable check.\n",
    "##\n",
    "## Test that checks the classifier variable exists. The marking system will use this variable to make predictions based on a set of random features you have not seen. Your score will be based on how well your classifier predicts the hidden labels.\n",
    "##\n",
    "#######\n",
    "\n",
    "if 'classifier' not in vars():\n",
    "    fail = True;\n",
    "    print(\"{} The classifer variable has not been defined.\\033[0m\".format(issue))\n",
    "    print(\"{} Your code must create a variable called \\'classifier\\' as described in the coursework specification.\\033[0m\".format(error))\n",
    "    print(\"{} This variable should contain the trained classifier you have created.\\033[0m\".format(error))\n",
    "else:\n",
    "    print('{} The classifer variable has been correctly defined.\\033[0m'.format(success))\n",
    "\n",
    "#######\n",
    "##\n",
    "## Accuracy Estimation check.\n",
    "##\n",
    "## Test that checks the accuracy estimation function exists and is a reasonable value. This is a requirement of the coursework specification and is used by the marking system when generating your final grade.\n",
    "##\n",
    "#######\n",
    "\n",
    "if \"my_accuracy_estimate\" not in dir():\n",
    "    fail = True;\n",
    "    print(\"{} The my_accuracy_estimate function has not been defined.\\033[0m\".format(issue))\n",
    "    print(\"{} Your code must include a my_accuracy_estimate function as described in the coursework specification.\\033[0m\".format(error))\n",
    "    print(\"{} If you believe you have, \\'restart & run-all\\' to clear this error.\\033[0m\".format(error))\n",
    "else:\n",
    "    if my_accuracy_estimate() == 0.5:\n",
    "        print(\"{} my_accuracy_estimate function warning.\\033[0m\".format(issue))\n",
    "        print(\"{} my_accuracy_estimate function returns a value of 0.5 - Your classifier is no better than random chance.\\033[0m\".format(error))\n",
    "        print(\"{} Are you sure this is correct.\\033[0m\".format(error))\n",
    "    else:\n",
    "        print('{} The my_accuracy_estimate function has been defined correctly.\\033[0m'.format(success))\n",
    "\n",
    "#######\n",
    "##\n",
    "## Test set check.\n",
    "##\n",
    "## Test that checks your classifier actually works. The calls made here are the same made by the automarker - albeit with different data. If your work fails this test it will score 0 in the automarker.\n",
    "##\n",
    "#######\n",
    "\n",
    "try:\n",
    "    testing_spam = np.loadtxt(open(\"data/testing_spam.csv\"), delimiter=\",\").astype(int)\n",
    "    test_data = testing_spam[:, 1:]\n",
    "    test_labels = testing_spam[:, 0]\n",
    "    \n",
    "    try:\n",
    "        predictions = classifier.predict(test_data)\n",
    "        accuracy = np.count_nonzero(predictions == test_labels)/test_labels.shape[0]\n",
    "        print('{0} Success running test set - Accuracy was {1:.2f}%.\\033[0m'.format(success, (accuracy*100)))\n",
    "    except Exception as e:\n",
    "        fail = True\n",
    "        print(\"{} Error running test set.\\033[0m\".format(issue))\n",
    "        print(\"{} Your code produced the following error. This error will result in a zero from the automarker, please fix.\\033[0m\".format(error))\n",
    "#         print(\"{} {}\\033[0m\".format(error, e))\n",
    "        print(e)\n",
    "except:\n",
    "    sys.stderr.write(\"Unable to run one test as the file \\'data/testing_spam.csv\\' could not be found.\")\n",
    "\n",
    "#######\n",
    "##\n",
    "## Final Summary\n",
    "##\n",
    "## Prints the final results of the submission tests.\n",
    "##\n",
    "#######\n",
    "\n",
    "if fail:\n",
    "    sys.stderr.write(\"Your submission is not ready! Please read and follow the instructions above.\")\n",
    "else:\n",
    "    print(\"\\033[1m\\n\\n\")\n",
    "    print(\"╔═══════════════════════════════════════════════════════════════╗\")\n",
    "    print(\"║                        Congratulations!                       ║\")\n",
    "    print(\"║                                                               ║\")\n",
    "    print(\"║            Your work meets all the required criteria          ║\")\n",
    "    print(\"║                   and is ready for submission.                ║\")\n",
    "    print(\"╚═══════════════════════════════════════════════════════════════╝\")\n",
    "    print(\"\\033[0m\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "136737e66133e8cd4881060775030b30",
     "grade": true,
     "grade_id": "cell-b64bc40ab6485b50",
     "locked": true,
     "points": 50,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a test cell. Please do not modify or delete."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
