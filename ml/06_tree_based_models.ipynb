{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f92d6a6",
   "metadata": {},
   "source": [
    "#### 1. Decision trees for classification\n",
    "\n",
    " - Sequence of if-else statements about individual features\n",
    " - Captures non-linear relationships between features and labels\n",
    " - Objective is to infer class labels\n",
    " - Divides the feature space into regions\n",
    "     - These are called decision regions and inside them all feature instances are assigned to one class label\n",
    "     - Decision boundaries: surface separating different decision regions\n",
    "     - Classification trees produce rectangular decision regions (while linear classifiers separate the plane using straight line boundaries)\n",
    "     \n",
    " - Hierarchy of nodes:\n",
    "     - Each node shows a question or a prediction\n",
    "     - Root node = no parent node, gives rise to two children\n",
    "     - Internal node = one parent node, gives rise to two children\n",
    "     - Leaf node = prediction is made here, no children nodes\n",
    "     - So the whole tree is made in such a way so that each leaf has a single predominant class label\n",
    " - To produce the purest leaf possible, the tree is using \"Information Gain\"\n",
    "     - The nodes are grown recursively = each internal node or leaf depends on the state of its predecessors\n",
    "     - To produce the purest leafs possible, at each node the tree asks a questions about one feature and a split point\n",
    "     - To select which feature and split point to ask about, the tree maximises the IG obtained after each split\n",
    "     - If the IG is null, the node is declared a leaf (or if you've reached the max depth specified)\n",
    "     - Unclear if the tree searches many several features and split points, calculating the IG for each and keeping the ones that have maximised it) ?\n",
    "     - Many criteria are used to calculate the information gain (impurity of a node) e.g. entropy or Gini index\n",
    "     - Reduction of entropy = information gain\n",
    "     \n",
    "     <img src=\"ml_assets/ig_tree_class.png\" style=\"height: 100px;\"/>\n",
    "\n",
    "\n",
    " - Learn more: https://en.wikipedia.org/wiki/Decision_tree_learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33d7b844",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f2f97a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cure = load_breast_cancer()\n",
    "features = pd.DataFrame(scale(cure['data']), columns = cure['feature_names'])\n",
    "target = pd.DataFrame(cure['target'], columns=['Type'])\n",
    "df = pd.concat([target,features],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8903e5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=30, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "292eb9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "# Import DecisionTreeClassifier from sklearn.tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Instantiate a DecisionTreeClassifier 'dt' with a maximum depth of 6\n",
    "dt = DecisionTreeClassifier(max_depth=6, random_state=12)\n",
    "\n",
    "# Fit dt to the training set\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred = dt.predict(X_test)\n",
    "print(y_pred[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1e8aa6a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy achieved by using entropy: 0.933\n",
      "Accuracy achieved by using the gini index: 0.900\n"
     ]
    }
   ],
   "source": [
    "# Import DecisionTreeClassifier from sklearn.tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Import accuracy_score which corresponds to the fraction of correct predictions made on the test set\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Instantiate dt_entropy, set 'entropy' as the information criterion\n",
    "dt_entropy = DecisionTreeClassifier(max_depth=8, criterion='entropy', random_state=1)\n",
    "dt_gini = DecisionTreeClassifier(max_depth=8, criterion='gini', random_state=24)\n",
    "\n",
    "# Fit dt_entropy to the training set\n",
    "dt_entropy.fit(X_train, y_train)\n",
    "dt_gini.fit(X_train, y_train)\n",
    "\n",
    "# Use dt_entropy to predict test set labels\n",
    "y_pred = dt_entropy.predict(X_test)\n",
    "y_pred_gini = dt_gini.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy_entropy\n",
    "accuracy_entropy = accuracy_score(y_test, y_pred)\n",
    "accuracy_gini = accuracy_score(y_test, y_pred_gini)\n",
    "\n",
    "# Print accuracy_entropy\n",
    "print(f'Accuracy achieved by using entropy: {accuracy_entropy:.3f}')\n",
    "\n",
    "# Print accuracy_gini\n",
    "print(f'Accuracy achieved by using the gini index: {accuracy_gini:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f8144ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "# def plot_labeled_decision_regions(X,y, models):\n",
    "#     '''Function producing a scatter plot of the instances contained \n",
    "#     in the 2D dataset (X,y) along with the decision \n",
    "#     regions of two trained classification models contained in the\n",
    "#     list 'models'.\n",
    "    \n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     X: pandas DataFrame corresponding to two numerical features \n",
    "#     y: pandas Series corresponding the class labels\n",
    "#     models: list containing two trained classifiers \n",
    "    \n",
    "#     '''\n",
    "#     if len(models) != 2:\n",
    "#         raise Exception('''Models should be a list containing only two trained classifiers.''')\n",
    "#     if not isinstance(X, pd.DataFrame):\n",
    "#         raise Exception('''X has to be a pandas DataFrame with two numerical features.''')\n",
    "#     if not isinstance(y, pd.Series):\n",
    "#         raise Exception('''y has to be a pandas Series corresponding to the labels.''')\n",
    "#     fig, ax = plt.subplots(1, 2, figsize=(10.0, 5), sharey=True)\n",
    "#     for i, model in enumerate(models):\n",
    "#         plot_decision_regions(X.values, y.values, model, legend= 2, ax = ax[i])\n",
    "#         ax[i].set_title(model.__class__.__name__)\n",
    "#         ax[i].set_xlabel(X.columns[0])\n",
    "#         if i == 0:\n",
    "#             ax[i].set_ylabel(X.columns[1])\n",
    "#             ax[i].set_ylim(X.values[:,1].min(), X.values[:,1].max())\n",
    "#             ax[i].set_xlim(X.values[:,0].min(), X.values[:,0].max())\n",
    "#     plt.tight_layout()\n",
    "\n",
    "# # Import LogisticRegression from sklearn.linear_model\n",
    "# from sklearn.linear_model import  LogisticRegression\n",
    "\n",
    "# # Instatiate logreg\n",
    "# logreg = LogisticRegression(random_state=1)\n",
    "\n",
    "# # Fit logreg to the training set\n",
    "# logreg.fit(X_train, y_train)\n",
    "\n",
    "# # Define a list called clfs containing the two classifiers logreg and dt\n",
    "# clfs = [logreg, dt]\n",
    "\n",
    "# # Review the decision regions of the two classifiers\n",
    "# plot_labeled_decision_regions(X_test, y_test, clfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83ae1dd",
   "metadata": {},
   "source": [
    "#### 2. Decision trees for regression\n",
    "\n",
    " - Greater flexibility to capture non-linearity in the data (but not fully)\n",
    " - Linear regression models cannot capture such relationships in the data\n",
    "      <img src=\"ml_assets/dec_tree_reg.png\" style=\"height: 300px;\"/>\n",
    " - The node's impurity is measured by the mean squared error of that node's targets\n",
    " - So the regression tree tries to find the splits that produce the leafs where in each leaf the target values are on average the closest possible to the mean-value of the labels in that particular leaf\n",
    " \n",
    "      <img src=\"ml_assets/ig_tree_reg.png\" style=\"height: 200px;\"/>\n",
    "\n",
    " - Prediction: for a new instance of features which has \"reached\" a certain leaf, the predicted \"y\" is computed as the average of the target variables contained in the leaf (i.e. the mean predicted \"y\" of the leaf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f667c650",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "features_boston = pd.DataFrame(boston['data'], columns = boston['feature_names'])\n",
    "target_boston = pd.DataFrame(boston['target'], columns=['Type'])\n",
    "df_boston = pd.concat([target_boston,features_boston],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d9ecefe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features_boston, target_boston,\n",
    "                                                    test_size=30, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ba675bc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=8, min_samples_leaf=0.13, random_state=3)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import DecisionTreeRegressor from sklearn.tree\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Instantiate dt\n",
    "dt = DecisionTreeRegressor(max_depth=8,\n",
    "                           min_samples_leaf=0.13,\n",
    "                           random_state=3)\n",
    "\n",
    "# Fit dt to the training set\n",
    "dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "678e6862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set RMSE of dt: 3.88\n"
     ]
    }
   ],
   "source": [
    "# Import mean_squared_error from sklearn.metrics as MSE\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# Compute y_pred\n",
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "# Compute mse_dt\n",
    "mse_dt = MSE(y_test, y_pred)\n",
    "\n",
    "# Compute rmse_dt\n",
    "rmse_dt = mse_dt**(1/2)\n",
    "\n",
    "# Print rmse_dt\n",
    "print(\"Test set RMSE of dt: {:.2f}\".format(rmse_dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8dce20cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d10b83e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression test set RMSE: 4.49\n",
      "Regression Tree test set RMSE: 3.88\n"
     ]
    }
   ],
   "source": [
    "# Predict test set labels \n",
    "y_pred_lr = lr.predict(X_test)\n",
    "\n",
    "# Compute mse_lr\n",
    "mse_lr = MSE(y_test, y_pred_lr)\n",
    "\n",
    "# Compute rmse_lr\n",
    "rmse_lr = mse_lr**(1/2)\n",
    "\n",
    "# Print rmse_lr\n",
    "print('Linear Regression test set RMSE: {:.2f}'.format(rmse_lr))\n",
    "\n",
    "# Print rmse_dt\n",
    "print('Regression Tree test set RMSE: {:.2f}'.format(rmse_dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2838da1",
   "metadata": {},
   "source": [
    "#### 3. Variance-Bias trade off\n",
    "\n",
    " - Overfitting: a model learns \"perfectly\" the training set data but does not approximate/predict effectively new data\n",
    "     - Has low predictive power / high test set error\n",
    "     - Has low training set error\n",
    " - Underfitting: a model is inflexible on the training set / not flexible enough to capture the relationship between features and labels\n",
    "     - It has roughly equal training set and test set errors, but both are quite high\n",
    " \n",
    " - Generalization error: how well can a model generalize on unseen data\n",
    "     - > GE of f = Bias error ** 2 + Variance error + Irreducible error\n",
    "     - Bias tells how much on average f_hat and f are different\n",
    "         - High bias models lead to underfitting\n",
    "     - Variance tells you how much f_hat is inconsistent over different training sets\n",
    "         - It follows data so closely that it loses the actual model - can lead to overfitting\n",
    "\n",
    "      <img src=\"ml_assets/bias_variance.png\" style=\"height: 400px;\"/>\n",
    "      <img src=\"ml_assets/bias_variance_visual.png\" style=\"height: 400px;\"/>\n",
    "\n",
    " - The generalization error cannot be estimated directly because f is unknown, there's only one dataset and noise cannot be predicted\n",
    " - Instead, the generalization error can be approximated as the test set error that is measured by splitting the data into training and test set\n",
    " - Use cross-validation for multiple tests and training sets\n",
    "     - In K-fold cross validation the CV error is the average of error from each fold\n",
    " - If the issue is high variance (that is if CV error > than training set error of f)\n",
    "     - f overfits the data so decrease model complexity or gather more data\n",
    " - If the issue is high bias (that is if CV error ~= training set error of f > desired error)\n",
    "     - f underfits the data so increase model complexity or gather more relevant features\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1dccda52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set SEED for reproducibility\n",
    "SEED = 1\n",
    "\n",
    "# Split the data into 70% train and 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.3, random_state=SEED)\n",
    "\n",
    "# Instantiate a DecisionTreeRegressor dt\n",
    "dt = DecisionTreeRegressor(max_depth=4, min_samples_leaf=0.26, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1ec8805b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV RMSE: 0.24\n"
     ]
    }
   ],
   "source": [
    "# Compute the array containing the 10-folds CV MSEs\n",
    "MSE_CV_scores = - cross_val_score(dt, X_train, y_train, cv=10, \n",
    "                                  scoring='neg_mean_squared_error', \n",
    "                                  n_jobs=-1) \n",
    "\n",
    "# Compute the 10-folds CV RMSE\n",
    "RMSE_CV = (MSE_CV_scores.mean())**(1/2)\n",
    "\n",
    "# Print RMSE_CV\n",
    "print('CV RMSE: {:.2f}'.format(RMSE_CV))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cb916de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE: 0.23\n"
     ]
    }
   ],
   "source": [
    "# Fit dt to the training set\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the training set\n",
    "y_pred_train = dt.predict(X_train)\n",
    "\n",
    "# Evaluate the training set RMSE of dt\n",
    "RMSE_train = (MSE(y_train, y_pred_train))**(1/2)\n",
    "\n",
    "# Print RMSE_train\n",
    "print('Train RMSE: {:.2f}'.format(RMSE_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db43795",
   "metadata": {},
   "source": [
    "#### 4. Ensemble methods - voting classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58dc4f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "50896ecd",
   "metadata": {},
   "source": [
    "#### 5. Bagging and Random forests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c762bc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1961752a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b563c71",
   "metadata": {},
   "source": [
    "#### 6. Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5000c106",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea3fac1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa6f48d3",
   "metadata": {},
   "source": [
    "#### 7. Hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4663f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
