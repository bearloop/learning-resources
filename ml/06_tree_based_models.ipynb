{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ba6c325",
   "metadata": {},
   "source": [
    "#### 1. Decision trees for classification\n",
    "\n",
    " - Sequence of if-else statements about individual features\n",
    " - Captures non-linear relationships between features and labels\n",
    " - Objective is to infer class labels\n",
    " - Divides the feature space into regions\n",
    "     - These are called decision regions and inside them all feature instances are assigned to one class label\n",
    "     - Decision boundaries: surface separating different decision regions\n",
    "     - Classification trees produce rectangular decision regions (while linear classifiers separate the plane using straight line boundaries)\n",
    "     \n",
    " - Hierarchy of nodes:\n",
    "     - Each node shows a question or a prediction\n",
    "     - Root node = no parent node, gives rise to two children\n",
    "     - Internal node = one parent node, gives rise to two children\n",
    "     - Leaf node = prediction is made here, no children nodes\n",
    "     - So the whole tree is made in such a way so that each leaf has a single predominant class label\n",
    " - To produce the purest leaf possible, the tree is using \"Information Gain\"\n",
    "     - The nodes are grown recursively = each internal node or leaf depends on the state of its predecessors\n",
    "     - To produce the purest leafs possible, at each node the tree asks a questions about one feature and a split point\n",
    "     - To select which feature and split point to ask about, the tree maximises the IG obtained after each split\n",
    "     - If the IG is null, the node is declared a leaf (or if you've reached the max depth specified)\n",
    "     - Unclear if the tree searches many several features and split points, calculating the IG for each and keeping the ones that have maximised it) ?\n",
    "     - Many criteria are used to calculate the information gain (impurity of a node) e.g. entropy or Gini index\n",
    "     \n",
    "     <img src=\"ml_assets/ig_tree_class.png\" style=\"height: 100px;\"/>\n",
    "\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a74b525b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cc1442e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cure = load_breast_cancer()\n",
    "features = pd.DataFrame(scale(cure['data']), columns = cure['feature_names'])\n",
    "target = pd.DataFrame(cure['target'], columns=['Type'])\n",
    "df = pd.concat([target,features],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb6a9bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=30, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f51c0456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "# Import DecisionTreeClassifier from sklearn.tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Instantiate a DecisionTreeClassifier 'dt' with a maximum depth of 6\n",
    "dt = DecisionTreeClassifier(max_depth=6, random_state=12)\n",
    "\n",
    "# Fit dt to the training set\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred = dt.predict(X_test)\n",
    "print(y_pred[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d7b19cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy achieved by using entropy: 0.933\n",
      "Accuracy achieved by using the gini index: 0.900\n"
     ]
    }
   ],
   "source": [
    "# Import DecisionTreeClassifier from sklearn.tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Import accuracy_score which corresponds to the fraction of correct predictions made on the test set\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Instantiate dt_entropy, set 'entropy' as the information criterion\n",
    "dt_entropy = DecisionTreeClassifier(max_depth=8, criterion='entropy', random_state=1)\n",
    "dt_gini = DecisionTreeClassifier(max_depth=8, criterion='gini', random_state=24)\n",
    "\n",
    "# Fit dt_entropy to the training set\n",
    "dt_entropy.fit(X_train, y_train)\n",
    "dt_gini.fit(X_train, y_train)\n",
    "\n",
    "# Use dt_entropy to predict test set labels\n",
    "y_pred = dt_entropy.predict(X_test)\n",
    "y_pred_gini = dt_gini.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy_entropy\n",
    "accuracy_entropy = accuracy_score(y_test, y_pred)\n",
    "accuracy_gini = accuracy_score(y_test, y_pred_gini)\n",
    "\n",
    "# Print accuracy_entropy\n",
    "print(f'Accuracy achieved by using entropy: {accuracy_entropy:.3f}')\n",
    "\n",
    "# Print accuracy_gini\n",
    "print(f'Accuracy achieved by using the gini index: {accuracy_gini:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1c0aeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "# def plot_labeled_decision_regions(X,y, models):\n",
    "#     '''Function producing a scatter plot of the instances contained \n",
    "#     in the 2D dataset (X,y) along with the decision \n",
    "#     regions of two trained classification models contained in the\n",
    "#     list 'models'.\n",
    "    \n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     X: pandas DataFrame corresponding to two numerical features \n",
    "#     y: pandas Series corresponding the class labels\n",
    "#     models: list containing two trained classifiers \n",
    "    \n",
    "#     '''\n",
    "#     if len(models) != 2:\n",
    "#         raise Exception('''Models should be a list containing only two trained classifiers.''')\n",
    "#     if not isinstance(X, pd.DataFrame):\n",
    "#         raise Exception('''X has to be a pandas DataFrame with two numerical features.''')\n",
    "#     if not isinstance(y, pd.Series):\n",
    "#         raise Exception('''y has to be a pandas Series corresponding to the labels.''')\n",
    "#     fig, ax = plt.subplots(1, 2, figsize=(10.0, 5), sharey=True)\n",
    "#     for i, model in enumerate(models):\n",
    "#         plot_decision_regions(X.values, y.values, model, legend= 2, ax = ax[i])\n",
    "#         ax[i].set_title(model.__class__.__name__)\n",
    "#         ax[i].set_xlabel(X.columns[0])\n",
    "#         if i == 0:\n",
    "#             ax[i].set_ylabel(X.columns[1])\n",
    "#             ax[i].set_ylim(X.values[:,1].min(), X.values[:,1].max())\n",
    "#             ax[i].set_xlim(X.values[:,0].min(), X.values[:,0].max())\n",
    "#     plt.tight_layout()\n",
    "\n",
    "# # Import LogisticRegression from sklearn.linear_model\n",
    "# from sklearn.linear_model import  LogisticRegression\n",
    "\n",
    "# # Instatiate logreg\n",
    "# logreg = LogisticRegression(random_state=1)\n",
    "\n",
    "# # Fit logreg to the training set\n",
    "# logreg.fit(X_train, y_train)\n",
    "\n",
    "# # Define a list called clfs containing the two classifiers logreg and dt\n",
    "# clfs = [logreg, dt]\n",
    "\n",
    "# # Review the decision regions of the two classifiers\n",
    "# plot_labeled_decision_regions(X_test, y_test, clfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df21b1f",
   "metadata": {},
   "source": [
    "#### 2. Decision trees for regression\n",
    "\n",
    " - Greater flexibility to capture non-linearity in the data (but not fully)\n",
    " - Linear regression models cannot capture such relationships in the data\n",
    "      <img src=\"ml_assets/dec_tree_reg.png\" style=\"height: 300px;\"/>\n",
    " - The node's impurity is measured by the mean squared error of that node's targets\n",
    " - So the regression tree tries to find the splits that produce the leafs where in each leaf the target values are on average the closest possible to the mean-value of the labels in that particular leaf\n",
    " \n",
    "      <img src=\"ml_assets/ig_tree_reg.png\" style=\"height: 200px;\"/>\n",
    "\n",
    " - Prediction: for a new instance of features which has \"reached\" a certain leaf, the predicted \"y\" is computed as the average of the target variables contained in the leaf (i.e. the mean predicted \"y\" of the leaf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "220f24d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "features_boston = pd.DataFrame(boston['data'], columns = boston['feature_names'])\n",
    "target_boston = pd.DataFrame(boston['target'], columns=['Type'])\n",
    "df_boston = pd.concat([target_boston,features_boston],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f5966a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features_boston, target_boston,\n",
    "                                                    test_size=30, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1e13048f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=8, min_samples_leaf=0.13, random_state=3)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import DecisionTreeRegressor from sklearn.tree\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Instantiate dt\n",
    "dt = DecisionTreeRegressor(max_depth=8,\n",
    "                           min_samples_leaf=0.13,\n",
    "                           random_state=3)\n",
    "\n",
    "# Fit dt to the training set\n",
    "dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4d107dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set RMSE of dt: 3.88\n"
     ]
    }
   ],
   "source": [
    "# Import mean_squared_error from sklearn.metrics as MSE\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# Compute y_pred\n",
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "# Compute mse_dt\n",
    "mse_dt = MSE(y_test, y_pred)\n",
    "\n",
    "# Compute rmse_dt\n",
    "rmse_dt = mse_dt**(1/2)\n",
    "\n",
    "# Print rmse_dt\n",
    "print(\"Test set RMSE of dt: {:.2f}\".format(rmse_dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e009bbcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "002e3537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression test set RMSE: 4.49\n",
      "Regression Tree test set RMSE: 3.88\n"
     ]
    }
   ],
   "source": [
    "# Predict test set labels \n",
    "y_pred_lr = lr.predict(X_test)\n",
    "\n",
    "# Compute mse_lr\n",
    "mse_lr = MSE(y_test, y_pred_lr)\n",
    "\n",
    "# Compute rmse_lr\n",
    "rmse_lr = mse_lr**(1/2)\n",
    "\n",
    "# Print rmse_lr\n",
    "print('Linear Regression test set RMSE: {:.2f}'.format(rmse_lr))\n",
    "\n",
    "# Print rmse_dt\n",
    "print('Regression Tree test set RMSE: {:.2f}'.format(rmse_dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef52716",
   "metadata": {},
   "source": [
    "#### 3. Variance-Bias trade off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3c7342",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab4550e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "25138d47",
   "metadata": {},
   "source": [
    "#### 4. Bagging and Random forests (ensemble methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58701048",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70389800",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8b0a22a",
   "metadata": {},
   "source": [
    "#### 5. Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1454a695",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6cdd74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eec2ca00",
   "metadata": {},
   "source": [
    "#### 6. Hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1b9e9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
