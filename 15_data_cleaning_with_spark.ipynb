{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a831fe50",
   "metadata": {},
   "source": [
    "### What is data cleaning?\n",
    "\n",
    " - Preparing raw data for use in data processing pipelines\n",
    " - Tasks include reformatting or replacing text, performing calculations, removing garbage or incomplete data etc\n",
    " - For billions of pieces of data, performance will be an issue, hence the use of Spark which can handle big data / The primary limit to Spark's abilities is the level of RAM in the Spark cluster\n",
    " - Spark schemas: define the various data types used, can filter garbage data during import, imporves read performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da0a78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pyspark.sql.types library\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Define a new schema using the StructType method\n",
    "people_schema = StructType([\n",
    "  # Define a StructField for each field\n",
    "  StructField('name', StringType(), False),\n",
    "  StructField('age', IntegerType(), False),\n",
    "  StructField('city', StringType(), False)\n",
    "])\n",
    "\n",
    "# Reading files and enforcing the schema\n",
    " people_df = spark.read.format('csv').load(name='rawdata.csv', schema=people_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecc90df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pyspark.sql.types library\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Define a new schema using the StructType method\n",
    "people_schema = StructType([\n",
    "  # Define a StructField for each field\n",
    "  StructField('name', StringType(), False),\n",
    "  StructField('age', IntegerType(), False),\n",
    "  StructField('city', StringType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57111c4f",
   "metadata": {},
   "source": [
    "#### Immutability and lazy processing\n",
    "\n",
    " - Normally in Python variables are mutable\n",
    " - This while adding to flexibility presents a problem when there are multiple concurrent components trying to modify the same data\n",
    " - Spark is designed to use immutable variables (variables that are defined once and are not modifieable after initialization)\n",
    " - Variables are re-created if reassigned\n",
    " - This allows Spark to share data efficiently without worrying about concurrent data objects\n",
    " - When you make changes to a Spark DataFrame the original object is destroyed and a new one takes its name/place\n",
    " - That doesn't mean that the original data (e.g. the file that was read to create the first DataFrame) is changed\n",
    "\n",
    "\n",
    "Lazy processing: the idea that very little actually happens until an action is performed\n",
    " - Funcionality is broken down to transformations and actions\n",
    " - Transformations are like instructions of what we want to accomplish\n",
    " - Actions are like \"triggers\" that begin the process based on the instructions provided\n",
    " - Lazy processing operations will usually return in about the same amount of time regardless of the actual quantity of data. Remember that this is due to Spark not performing any transformations until an action is requested.\n",
    " - Note the amount of time required for the transformations to complete when defined vs when the data is actually queried. These differences may be short, but they will be noticeable. When working with a full Spark cluster with larger quantities of data the difference will be more apparent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60850f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "aa_dfw_df = spark.read.format('csv').options(Header=True).load('AA_DFW_2018.csv.gz')\n",
    "\n",
    "# Add the airport column using the F.lower() method\n",
    "aa_dfw_df = aa_dfw_df.withColumn('airport', F.lower(aa_dfw_df['Destination Airport']))\n",
    "\n",
    "# Drop the Destination Airport column\n",
    "aa_dfw_df = aa_dfw_df.drop(aa_dfw_df['Destination Airport'])\n",
    "\n",
    "# Show the DataFrame\n",
    "aa_dfw_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9654b1",
   "metadata": {},
   "source": [
    "### CSVs vs Parquet file formats\n",
    "\n",
    "CSVs\n",
    " - CSVs are slow to import and parse / the files cannot be shared among Spark workers and if no schema is defined all data must be read before a schema can be inferred\n",
    " - Files cannot be filtered via \"predicate pushdown\" (the idea of ordering tasks to do the least amount of work / filtering data prior to processing is one of the primary optimizations of predicate pushdown drastically reducing the amount of information that must be processed in large data sets / you cannot filter the CSV data via predicate pushdown)\n",
    " - Spark processes are often multi-step and may utilize an intermediate file representation / these representations allow data to be used later without regenerating data from source / using CSV would require a significant amount of extra work defining schemas, encoding formats etc\n",
    "\n",
    "Parquet files\n",
    " - Parquet is a compressed columnar data format developed for use in any Hadoop based system (Apache Spark, Hadoop, Impala)\n",
    " - The format is structured with data accessible in chunks allowing efficient read-write operations without processing the entire file\n",
    " - It supports the predicate pushdown functionality, providing significant performance improvement\n",
    " - The Parquet format is a columnar data store, allowing Spark to use predicate pushdown. This means Spark will only process the data necessary to complete the operations you define versus reading the entire dataset. This gives Spark more flexibility in accessing the data and often drastically improves performance on large datasets.\n",
    " - Automatically includes schema information and handle data encoding\n",
    " - Parquet files are a binary file format and can only be used with the proper tools / in contrast to CSV files which can be edited with any text editor\n",
    " - Parquet files are perfect as a backing data store for SQL queries in Spark. While it is possible to run the same queries directly via Spark's Python functions, sometimes it's easier to run SQL queries alongside the Python options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c308cfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two methods to read/write (used interchangeably)\n",
    "\n",
    "# Reading parquet files\n",
    "df = spark.read.format('parquet').load('filename.parquet')\n",
    "df = spark.read.parquet('filename.parquet')\n",
    "\n",
    "# Writing parquet files\n",
    "df.write.format('parquet').save('filename.parquet')\n",
    "df.write.parquet('filename.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88da9168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run SQL queries use createOrReplaceTempView\n",
    "# after reading the parquet file\n",
    "\n",
    "flight_df = spark.read.parquet('flights.parquet')\n",
    "\n",
    "flight_df.createOrReplaceTempView('flights')\n",
    "\n",
    "short_flights_df = spark.sql('SELECT * FROM flights WHERE flightduration < 100')\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd17d7fc",
   "metadata": {},
   "source": [
    "### Manipulating DataFrames\n",
    "\n",
    "DataFrames\n",
    " - Made up of rows and columns and generally analogous to a database table\n",
    " - Are immutable as any change to the structure or content creates a new DataFrame\n",
    " - Are modified through the use of transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd1ffbf",
   "metadata": {},
   "source": [
    "#### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743409d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return rows where name starts with \"M\"\n",
    "voter_df.filter(voter_df.name.like('M%'))\n",
    "# Return name and position only\n",
    "voters = voter_df.select('name', 'position')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331efadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter\n",
    "voter_df.filter(voter_df.date > '1/1/2019') # or voter_df.where(...)\n",
    "voter_df.filter(voter_df['name'].isNotNull()) # remove nulls\n",
    "voter_df.filter(voter_df.date.year > 1800) # remove old entries\n",
    "\n",
    "# Where\n",
    "voter_df.where(voter_df['_c0'].contains('VOTE')) # split data from combined sources\n",
    "voter_df.where(~ voter_df._c1.isNull()) # negate with ~\n",
    " \n",
    "# Select\n",
    "voter_df.select(voter_df.name)\n",
    "\n",
    "# withColumn\n",
    "voter_df.withColumn('year', voter_df.date.year)\n",
    "\n",
    "# drop\n",
    "voter_df.drop('unused_column')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa7b572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column string transformations\n",
    "# Contsined in pyspark.sql.functions\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Applied per column transformation\n",
    "voter_df.withColumn('upper', F.upper('name'))\n",
    "\n",
    "# Create intermediary columns\n",
    "voter_df.withColumn('splits', F.split('name', ' '))\n",
    "\n",
    "# Cast to other types\n",
    "voter_df.withColumn('year', voter_df['_c4'].cast(IntegerType()))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b41acac",
   "metadata": {},
   "source": [
    "#### ArrayType column functions\n",
    " - .size(column) returns length of arrayType() column\n",
    " - .getItem(index) retrieves a specific item at index of list column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77cf79d",
   "metadata": {},
   "source": [
    "#### More examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4325cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the distinct VOTER_NAME entries\n",
    "voter_df.select('VOTER_NAME').distinct().show(40, truncate=False)\n",
    "\n",
    "# Filter voter_df where the VOTER_NAME is 1-20 characters in length\n",
    "voter_df = voter_df.filter('length(VOTER_NAME) > 0 and length(VOTER_NAME) < 20')\n",
    "\n",
    "# Filter out voter_df where the VOTER_NAME contains an underscore\n",
    "voter_df = voter_df.filter(~ F.col('VOTER_NAME').contains('_'))\n",
    "\n",
    "# Show the distinct VOTER_NAME entries again\n",
    "voter_df.select('VOTER_NAME').distinct().show(40, truncate=False)\n",
    "\n",
    "# Add a new column called splits separated on whitespace\n",
    "voter_df = voter_df.withColumn('splits', F.split(voter_df.VOTER_NAME, '\\s+'))\n",
    "\n",
    "# Create a new column called first_name based on the first item in splits\n",
    "voter_df = voter_df.withColumn('first_name', voter_df.splits.getItem(0))\n",
    "\n",
    "# Get the last entry of the splits list and create a column called last_name\n",
    "voter_df = voter_df.withColumn('last_name', voter_df.splits.getItem(F.size('splits') - 1))\n",
    "\n",
    "# Drop the splits column\n",
    "voter_df = voter_df.drop('splits')\n",
    "\n",
    "# Show the voter_df DataFrame\n",
    "voter_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
