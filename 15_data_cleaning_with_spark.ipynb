{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "231efaa1",
   "metadata": {},
   "source": [
    "### What is data cleaning?\n",
    "\n",
    " - Preparing raw data for use in data processing pipelines\n",
    " - Tasks include reformatting or replacing text, performing calculations, removing garbage or incomplete data etc\n",
    " - For billions of pieces of data, performance will be an issue, hence the use of Spark which can handle big data / The primary limit to Spark's abilities is the level of RAM in the Spark cluster\n",
    " - Spark schemas: define the various data types used, can filter garbage data during import, imporves read performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9acf3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pyspark.sql.types library\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Define a new schema using the StructType method\n",
    "people_schema = StructType([\n",
    "  # Define a StructField for each field\n",
    "  StructField('name', StringType(), False),\n",
    "  StructField('age', IntegerType(), False),\n",
    "  StructField('city', StringType(), False)\n",
    "])\n",
    "\n",
    "# Reading files and enforcing the schema\n",
    " people_df = spark.read.format('csv').load(name='rawdata.csv', schema=people_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eccc314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pyspark.sql.types library\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Define a new schema using the StructType method\n",
    "people_schema = StructType([\n",
    "  # Define a StructField for each field\n",
    "  StructField('name', StringType(), False),\n",
    "  StructField('age', IntegerType(), False),\n",
    "  StructField('city', StringType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca880c5a",
   "metadata": {},
   "source": [
    "#### Immutability and lazy processing\n",
    "\n",
    " - Normally in Python variables are mutable\n",
    " - This while adding to flexibility presents a problem when there are multiple concurrent components trying to modify the same data\n",
    " - Spark is designed to use immutable variables (variables that are defined once and are not modifieable after initialization)\n",
    " - Variables are re-created if reassigned\n",
    " - This allows Spark to share data efficiently without worrying about concurrent data objects\n",
    " - When you make changes to a Spark DataFrame the original object is destroyed and a new one takes its name/place\n",
    " - That doesn't mean that the original data (e.g. the file that was read to create the first DataFrame) is changed\n",
    "\n",
    "\n",
    "Lazy processing: the idea that very little actually happens until an action is performed\n",
    " - Funcionality is broken down to transformations and actions\n",
    " - Transformations are like instructions of what we want to accomplish\n",
    " - Actions are like \"triggers\" that begin the process based on the instructions provided\n",
    " - Lazy processing operations will usually return in about the same amount of time regardless of the actual quantity of data. Remember that this is due to Spark not performing any transformations until an action is requested.\n",
    " - Note the amount of time required for the transformations to complete when defined vs when the data is actually queried. These differences may be short, but they will be noticeable. When working with a full Spark cluster with larger quantities of data the difference will be more apparent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ade2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "aa_dfw_df = spark.read.format('csv').options(Header=True).load('AA_DFW_2018.csv.gz')\n",
    "\n",
    "# Add the airport column using the F.lower() method\n",
    "aa_dfw_df = aa_dfw_df.withColumn('airport', F.lower(aa_dfw_df['Destination Airport']))\n",
    "\n",
    "# Drop the Destination Airport column\n",
    "aa_dfw_df = aa_dfw_df.drop(aa_dfw_df['Destination Airport'])\n",
    "\n",
    "# Show the DataFrame\n",
    "aa_dfw_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2f5d18",
   "metadata": {},
   "source": [
    "### CSVs vs Parquet file formats\n",
    "\n",
    "CSVs\n",
    " - CSVs are slow to import and parse / the files cannot be shared among Spark workers and if no schema is defined all data must be read before a schema can be inferred\n",
    " - Files cannot be filtered via \"predicate pushdown\" (the idea of ordering tasks to do the least amount of work / filtering data prior to processing is one of the primary optimizations of predicate pushdown drastically reducing the amount of information that must be processed in large data sets / you cannot filter the CSV data via predicate pushdown)\n",
    " - Spark processes are often multi-step and may utilize an intermediate file representation / these representations allow data to be used later without regenerating data from source / using CSV would require a significant amount of extra work defining schemas, encoding formats etc\n",
    "\n",
    "Parquet files\n",
    " - Parquet is a compressed columnar data format developed for use in any Hadoop based system (Apache Spark, Hadoop, Impala)\n",
    " - The format is structured with data accessible in chunks allowing efficient read-write operations without processing the entire file\n",
    " - It supports the predicate pushdown functionality, providing significant performance improvement\n",
    " - The Parquet format is a columnar data store, allowing Spark to use predicate pushdown. This means Spark will only process the data necessary to complete the operations you define versus reading the entire dataset. This gives Spark more flexibility in accessing the data and often drastically improves performance on large datasets.\n",
    " - Automatically includes schema information and handle data encoding\n",
    " - Parquet files are a binary file format and can only be used with the proper tools / in contrast to CSV files which can be edited with any text editor\n",
    " - Parquet files are perfect as a backing data store for SQL queries in Spark. While it is possible to run the same queries directly via Spark's Python functions, sometimes it's easier to run SQL queries alongside the Python options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697515d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two methods to read/write (used interchangeably)\n",
    "\n",
    "# Reading parquet files\n",
    "df = spark.read.format('parquet').load('filename.parquet')\n",
    "df = spark.read.parquet('filename.parquet')\n",
    "\n",
    "# Writing parquet files\n",
    "df.write.format('parquet').save('filename.parquet')\n",
    "df.write.parquet('filename.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc72ffdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run SQL queries use createOrReplaceTempView\n",
    "# after reading the parquet file\n",
    "\n",
    "flight_df = spark.read.parquet('flights.parquet')\n",
    "\n",
    "flight_df.createOrReplaceTempView('flights')\n",
    "\n",
    "short_flights_df = spark.sql('SELECT * FROM flights WHERE flightduration < 100')\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3c55d9",
   "metadata": {},
   "source": [
    "### Manipulating DataFrames\n",
    "\n",
    "DataFrames\n",
    " - Made up of rows and columns and generally analogous to a database table\n",
    " - Are immutable as any change to the structure or content creates a new DataFrame\n",
    " - Are modified through the use of transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f013af",
   "metadata": {},
   "source": [
    "#### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d26a0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return rows where name starts with \"M\"\n",
    "voter_df.filter(voter_df.name.like('M%'))\n",
    "# Return name and position only\n",
    "voters = voter_df.select('name', 'position')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de36cbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter\n",
    "voter_df.filter(voter_df.date > '1/1/2019') # or voter_df.where(...)\n",
    "voter_df.filter(voter_df['name'].isNotNull()) # remove nulls\n",
    "voter_df.filter(voter_df.date.year > 1800) # remove old entries\n",
    "\n",
    "# Where\n",
    "voter_df.where(voter_df['_c0'].contains('VOTE')) # split data from combined sources\n",
    "voter_df.where(~ voter_df._c1.isNull()) # negate with ~\n",
    " \n",
    "# Select\n",
    "voter_df.select(voter_df.name)\n",
    "\n",
    "# withColumn\n",
    "voter_df.withColumn('year', voter_df.date.year)\n",
    "\n",
    "# drop\n",
    "voter_df.drop('unused_column')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377a7412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column string transformations\n",
    "# Contsined in pyspark.sql.functions\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Applied per column transformation\n",
    "voter_df.withColumn('upper', F.upper('name'))\n",
    "\n",
    "# Create intermediary columns\n",
    "voter_df.withColumn('splits', F.split('name', ' '))\n",
    "\n",
    "# Cast to other types\n",
    "voter_df.withColumn('year', voter_df['_c4'].cast(IntegerType()))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93735231",
   "metadata": {},
   "source": [
    "#### ArrayType column functions\n",
    " - .size(column) returns length of arrayType() column\n",
    " - .getItem(index) retrieves a specific item at index of list column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7816cf5",
   "metadata": {},
   "source": [
    "#### More examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330a79e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the distinct VOTER_NAME entries\n",
    "voter_df.select('VOTER_NAME').distinct().show(40, truncate=False)\n",
    "\n",
    "# Filter voter_df where the VOTER_NAME is 1-20 characters in length\n",
    "voter_df = voter_df.filter('length(VOTER_NAME) > 0 and length(VOTER_NAME) < 20')\n",
    "\n",
    "# Filter out voter_df where the VOTER_NAME contains an underscore\n",
    "voter_df = voter_df.filter(~ F.col('VOTER_NAME').contains('_'))\n",
    "\n",
    "# Show the distinct VOTER_NAME entries again\n",
    "voter_df.select('VOTER_NAME').distinct().show(40, truncate=False)\n",
    "\n",
    "# Add a new column called splits separated on whitespace\n",
    "voter_df = voter_df.withColumn('splits', F.split(voter_df.VOTER_NAME, '\\s+'))\n",
    "\n",
    "# Create a new column called first_name based on the first item in splits\n",
    "voter_df = voter_df.withColumn('first_name', voter_df.splits.getItem(0))\n",
    "\n",
    "# Get the last entry of the splits list and create a column called last_name\n",
    "voter_df = voter_df.withColumn('last_name', voter_df.splits.getItem(F.size('splits') - 1))\n",
    "\n",
    "# Drop the splits column\n",
    "voter_df = voter_df.drop('splits')\n",
    "\n",
    "# Show the voter_df DataFrame\n",
    "voter_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de281cb",
   "metadata": {},
   "source": [
    "### Conditional clauses\n",
    " \n",
    " - The when() clause lets you conditionally modify a Data Frame based on its content\n",
    " - The otherwise() clause is like else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5e9c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column to voter_df for any voter with the title **Councilmember**\n",
    "voter_df = voter_df.withColumn('random_val', when(voter_df.TITLE=='Councilmember', F.rand()))\n",
    "\n",
    "# Show some of the DataFrame rows, noting whether the when clause worked\n",
    "voter_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd203fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column to voter_df for a voter based on their position\n",
    "voter_df = voter_df.withColumn('random_val', when(\n",
    "                                voter_df.TITLE == 'Councilmember', F.rand()).when(\n",
    "                                voter_df.TITLE == 'Mayor', 2).otherwise(0))\n",
    "\n",
    "# Show some of the DataFrame rows\n",
    "voter_df.show()\n",
    "\n",
    "# Use the .filter() clause with random_val\n",
    "voter_df.filter(voter_df.random_val==0).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68f52c7",
   "metadata": {},
   "source": [
    "### User-defined functions\n",
    " \n",
    " - UDF is a Python method that the user writes to perform a specific bit of logic\n",
    " - Wrapped via the pyspark.sql.functions.udf method\n",
    " - Stored as a variable\n",
    " - Called like a normal Spark function\n",
    "\n",
    "#### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826eac00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse string UDF\n",
    "def reverseString(mystr):\n",
    "    return mystr[::-1]\n",
    "\n",
    "udfReverseString = udf(reverseString, StringType())\n",
    "\n",
    "user_df = user_df.withColumn('ReverseName', udfReverseString(user_df.Name))\n",
    "\n",
    "# Argument less example\n",
    "def sortingCap():\n",
    "    return random.choice(['G', 'H', 'R', 'S'])\n",
    "\n",
    "udfSortingCap = udf(sortingCap, StringType())\n",
    "\n",
    "user_df = user_df.withColumn('Class', udfSortingCap())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ba5e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFirstAndMiddle(names):\n",
    "  # Return a space separated string of names\n",
    "  return ' '.join(names)\n",
    "\n",
    "# Define the method as a UDF\n",
    "udfFirstAndMiddle = F.udf(getFirstAndMiddle, StringType())\n",
    "\n",
    "# Create a new column using your UDF\n",
    "voter_df = voter_df.withColumn('first_and_middle_name', udfFirstAndMiddle(voter_df.splits))\n",
    "\n",
    "# Show the DataFrame\n",
    "voter_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2beda88c",
   "metadata": {},
   "source": [
    "### Partitioning\n",
    "\n",
    " - Spark breaks DFs into partitions or chunks of data\n",
    " - The partitions size can vary based on the type of Spark cluster being used\n",
    " - Monotonically increasing IDs\n",
    "     - integer (64-bit), increases in value, unique\n",
    "     - not necessarily sequential (gaps exist)\n",
    "     - completely parallel\n",
    "     - up to 8.4 billion ids per partition\n",
    " - When working with data, you sometimes only want to access certain fields and perform various operations. In this case, find all the unique voter names from the DataFrame and add a unique ID number. Remember that Spark IDs are assigned based on the DataFrame partition - as such the ID values may be much greater than the actual number of rows in the DataFrame.\n",
    " - With Spark's lazy processing, the IDs are not actually generated until an action is performed and can be somewhat random depending on the size of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8512e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all the unique council voters\n",
    "voter_df = df.select(df[\"VOTER NAME\"]).distinct()\n",
    "\n",
    "# Count the rows in voter_df\n",
    "print(\"\\nThere are %d rows in the voter_df DataFrame.\\n\" % voter_df.count())\n",
    "\n",
    "# Add a ROW_ID\n",
    "voter_df = voter_df.withColumn('ROW_ID', F.monotonically_increasing_id())\n",
    "\n",
    "# Show the rows with 10 highest IDs in the set\n",
    "voter_df.orderBy(voter_df.ROW_ID.desc()).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05917c0",
   "metadata": {},
   "source": [
    "#### IDs with different partitions\n",
    "\n",
    "You've just completed adding an ID field to a DataFrame. Now, take a look at what happens when you do the same thing on DataFrames containing a different number of partitions.\n",
    "\n",
    "To check the number of partitions, use the method .rdd.getNumPartitions() on a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4c1758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the number of partitions in each DataFrame\n",
    "print(\"\\nThere are %d partitions in the voter_df DataFrame.\\n\" % voter_df.rdd.getNumPartitions())\n",
    "print(\"\\nThere are %d partitions in the voter_df_single DataFrame.\\n\" % voter_df_single.rdd.getNumPartitions())\n",
    "\n",
    "# Add a ROW_ID field to each DataFrame\n",
    "voter_df = voter_df.withColumn('ROW_ID', F.monotonically_increasing_id())\n",
    "voter_df_single = voter_df_single.withColumn('ROW_ID',F.monotonically_increasing_id())\n",
    "\n",
    "# Show the top 10 IDs in each DataFrame \n",
    "voter_df.orderBy(voter_df.ROW_ID.desc()).show(10)\n",
    "voter_df_single.orderBy(voter_df_single.ROW_ID.desc()).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa10f38",
   "metadata": {},
   "source": [
    "#### More ID tricks\n",
    "\n",
    "Once you define a Spark process, you'll likely want to use it many times. Depending on your needs, you may want to start your IDs at a certain value so there isn't overlap with previous runs of the Spark task. This behavior is similar to how IDs would behave in a relational database. You have been given the task to make sure that the IDs output from a monthly Spark task start at the highest value from the previous month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d84de77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the highest ROW_ID and save it in previous_max_ID\n",
    "previous_max_ID = voter_df_march.select('ROW_ID').rdd.max()[0]\n",
    "\n",
    "# Add a ROW_ID column to voter_df_april starting at the desired value\n",
    "voter_df_april = voter_df_april.withColumn('ROW_ID', F.monotonically_increasing_id() + previous_max_ID)\n",
    "\n",
    "# Show the ROW_ID from both DataFrames and compare\n",
    "voter_df_march.select('ROW_ID').show()\n",
    "voter_df_april.select('ROW_ID').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d79fb7",
   "metadata": {},
   "source": [
    "### Caching\n",
    "\n",
    " - Refers to storing the results of a DataFrame in memory or on disk of the processing nodes in a cluster\n",
    " - Improves the speed for subsequent transformations or actions as the data no longer needs to be retrieved from the original data source\n",
    " - Reduces the resource utilization of the cluster as there is less need to access the storage, networking and CPU of the Spark nodes as the data is likely already present\n",
    "\n",
    "Caching disadvantages\n",
    " - Very large datasets may not fit in memory reserved for cached DataFrames\n",
    " - So performance might not be improved at all depending on the later transformations\n",
    " - If data is not cached in memory it might be persisted on disk, but depending on the cluster's configuration this might not be a signifcant performance boost\n",
    " \n",
    "So caching is very useful but only if a DataFrame is planned to be used again, otherwise it's not worth caching\n",
    "\n",
    "Implementing caching\n",
    " - call cache() on the DataFrame before an action\n",
    " - cache() is a Spark transformation i.e. nothing will be cached until an Spark action is called\n",
    " - check if a DataFrame is cached with is_cached()\n",
    " - remove an object from the cache by calling unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5826ea47",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Add caching to the unique rows in departures_df\n",
    "departures_df = departures_df.distinct().cache()\n",
    "\n",
    "# Count the unique rows in departures_df, noting how long the operation takes\n",
    "print(\"Counting %d rows took %f seconds\" % (departures_df.count(), time.time() - start_time))\n",
    "\n",
    "# Count the rows again, noting the variance in time of a cached DataFrame\n",
    "start_time = time.time()\n",
    "print(\"Counting %d rows again took %f seconds\" % (departures_df.count(), time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d14c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine if departures_df is in the cache\n",
    "print(\"Is departures_df cached?: %s\" % departures_df.is_cached)\n",
    "print(\"Removing departures_df from cache\")\n",
    "\n",
    "# Remove departures_df from the cache\n",
    "departures_df.unpersist()\n",
    "\n",
    "# Check the cache status again\n",
    "print(\"Is departures_df cached?: %s\" % departures_df.is_cached)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b291b564",
   "metadata": {},
   "source": [
    "### Improving performance\n",
    "\n",
    " - Spark clusters consist of a driver process and as many worker processes as required\n",
    " - The driver handles task assignments and consolidation of the data results from the workers\n",
    " - The workers typically handle the actual transformation / action tasks of a Spark job / they operate fairly independently and report  results back to the driver\n",
    " - When importing files a larger number of smaller sized objects will perform better than a single large file\n",
    " - Objects should be of similar size generally speaking\n",
    " - Having a well-defined schemas also improves drastically performance as Spark doesn't have to read multiples times the dataset to infer the schema / saves time from cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c351ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the full and split files into DataFrames\n",
    "full_df = spark.read.csv('departures_full.txt.gz')\n",
    "split_df = spark.read.csv('departures_*.txt.gz')\n",
    "\n",
    "# Print the count and run time for each DataFrame\n",
    "start_time_a = time.time()\n",
    "print(\"Total rows in full DataFrame:\\t%d\" % full_df.count())\n",
    "print(\"Time to run: %f\" % (time.time() - start_time_a))\n",
    "\n",
    "start_time_b = time.time()\n",
    "print(\"Total rows in split DataFrame:\\t%d\" % split_df.count())\n",
    "print(\"Time to run: %f\" % (time.time() - start_time_b))### Improving performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffc9f79",
   "metadata": {},
   "source": [
    "### Configuration options\n",
    "\n",
    " - Spark contains many configuration settings\n",
    " - To read config settings via the command line use\n",
    " > spark.conf.get(configuration name)\n",
    " - To write config settings via the command line use\n",
    " > spark.conf.set(configuration name)\n",
    "\n",
    "#### Cluster types\n",
    " - Single node: deployinh all components on a single system (physical / VM / container)\n",
    " - Standalone clusters: with dedicated machines as the driver and workers\n",
    " - Managed clusters: the cluster is managed by 3rd party cluster managers such as YARN, Mesos, Kubernetes\n",
    "\n",
    "\n",
    "#### Driver\n",
    " - Handling task assignment to the various nodes / process in the cluster\n",
    " - Monitors the state of all processes and tasks and handles any task retries\n",
    " - Consolidates results from other processes in the cluster\n",
    " - Handles any access to shared data / variables and confirms each worker process has the necessary resources (code, data, etc)\n",
    "\n",
    "#### Worker\n",
    " - Runs actual tasks assigned by the driver and communicates those results back to the driver\n",
    " - Should have access to all resources necessary to complete a task otherwise pauses to obtain these resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3652e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the Spark application instance\n",
    "app_name = spark.conf.get('spark.app.name')\n",
    "\n",
    "# Driver TCP port\n",
    "driver_tcp_port = spark.conf.get('spark.driver.port')\n",
    "\n",
    "# Number of join partitions\n",
    "num_partitions = spark.conf.get('spark.sql.shuffle.partitions')\n",
    "\n",
    "# Show the results\n",
    "print(\"Name: %s\" % app_name)\n",
    "print(\"Driver TCP port: %s\" % driver_tcp_port)\n",
    "print(\"Number of partitions: %s\" % num_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc4a45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the number of partitions in variable\n",
    "before = departures_df.rdd.getNumPartitions()\n",
    "\n",
    "# Configure Spark to use 500 partitions\n",
    "spark.conf.set('spark.sql.shuffle.partitions', 500)\n",
    "\n",
    "# Recreate the DataFrame using the departures data file\n",
    "departures_df = spark.read.csv('departures.txt.gz').distinct()\n",
    "\n",
    "# Print the number of partitions for each instance\n",
    "print(\"Partition count before change: %d\" % before)\n",
    "print(\"Partition count after change: %d\" % departures_df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db37dd72",
   "metadata": {},
   "source": [
    "#### Shuffling\n",
    " - Refers to the moving of data fragments to various workers as required to complete certain tasks\n",
    " - This, although hides complexity from the user, it's generally slow as it lowers the cluster's throughput since the workers wait for data to be transferred\n",
    " - Limiting shuffling sometimes is a good practice\n",
    "     - use repartition(num_partitions)\n",
    "      - use coalesce(num_partitions) instead which consolidates data without requiring a full data shuffle\n",
    "     - use join()\n",
    "     - use broadcast() -> provides a copy of an object to each worker so that there is less need for communication between nodes -> limits data shuffling\n",
    "     > from pyspark.sql.functions import broadcast<br>\n",
    "     > combined_df = df_1.join(broadcast(df_2))\n",
    "\n",
    "Using broadcasting on Spark joins\n",
    "\n",
    "Remember that table joins in Spark are split between the cluster workers. If the data is not local, various shuffle operations are required and can have a negative impact on performance. Instead, we're going to use Spark's broadcast operations to give each node a copy of the specified data.\n",
    "\n",
    "A couple tips:\n",
    "\n",
    "Broadcast the smaller DataFrame. The larger the DataFrame, the more time required to transfer to the worker nodes.\n",
    "On small DataFrames, it may be better skip broadcasting and let Spark figure out any optimization on its own.\n",
    "If you look at the query execution plan, a broadcastHashJoin indicates you've successfully configured broadcasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc155e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the flights_df and aiports_df DataFrames\n",
    "normal_df = flights_df.join(airports_df, \\\n",
    "    flights_df[\"Destination Airport\"] == airports_df[\"IATA\"] )\n",
    "\n",
    "# Show the query plan\n",
    "normal_df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e415247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the broadcast method from pyspark.sql.functions\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# Join the flights_df and airports_df DataFrames using broadcasting\n",
    "broadcast_df = flights_df.join(broadcast(airports_df), \\\n",
    "    flights_df[\"Destination Airport\"] == airports_df[\"IATA\"] )\n",
    "\n",
    "# Show the query plan and compare against the original\n",
    "broadcast_df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722b2e67",
   "metadata": {},
   "source": [
    "Comparing broadcast vs normal joins\n",
    "\n",
    "You've created two types of joins, normal and broadcasted. Now your manager would like to know what the performance improvement is by using Spark optimizations. If the results are promising, you'll be given more opportunity to tweak the Spark setup as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5d49e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# Count the number of rows in the normal DataFrame\n",
    "normal_count = normal_df.count()\n",
    "normal_duration = time.time() - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "# Count the number of rows in the broadcast DataFrame\n",
    "broadcast_count = broadcast_df.count()\n",
    "broadcast_duration = time.time() - start_time\n",
    "\n",
    "# Print the counts and the duration of the tests\n",
    "print(\"Normal count:\\t\\t%d\\tduration: %f\" % (normal_count, normal_duration))\n",
    "print(\"Broadcast count:\\t%d\\tduration: %f\" % (broadcast_count, broadcast_duration))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee5cbe9",
   "metadata": {},
   "source": [
    "<img src=\"assets/spark/pipeline.png\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a870eb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data to a DataFrame\n",
    "departures_df = spark.read.csv('2015-departures.csv.gz', header=True)\n",
    "\n",
    "# Remove any duration of 0\n",
    "departures_df = departures_df.filter(\n",
    "    F.col('Actual elapsed time (Minutes)').alias('duration')!=0)\n",
    "\n",
    "# Add an ID column\n",
    "departures_df = departures_df.withColumn('id', F.monotonically_increasing_id())\n",
    "\n",
    "# Write the file out to JSON format\n",
    "departures_df.write.json('output.json', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fde82aa",
   "metadata": {},
   "source": [
    "### Data pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9942c081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the file to a DataFrame and perform a row count\n",
    "annotations_df = spark.read.csv('annotations.csv.gz', sep='|')\n",
    "full_count = annotations_df.count()\n",
    "\n",
    "# Count the number of rows beginning with '#'\n",
    "comment_count = annotations_df.where(col('_c0').startswith('#')).count()\n",
    "\n",
    "# Import the file to a new DataFrame, without commented rows\n",
    "no_comments_df = spark.read.csv('annotations.csv.gz', sep='|', comment='#')\n",
    "\n",
    "# Count the new DataFrame and verify the difference is as expected\n",
    "no_comments_count = no_comments_df.count()\n",
    "print(\"Full count: %d\\nComment count: %d\\nRemaining count: %d\" % (full_count, comment_count, no_comments_count))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
