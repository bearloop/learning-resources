{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fcd2880",
   "metadata": {},
   "source": [
    "### Airflow\n",
    "\n",
    " - Data engineering: Taking any action involving data and turning it into a reliable, repeatable and maintanable process\n",
    " - Workflow: A set of steps to accomplish a given data engineering task e.g. copying, downloading files, filtering information, writing to a database\n",
    " - Varying levels of complexity for a workflow (from 2-3 steps to 100s)\n",
    " - Airflow is a platform to program workflows including, creation, scheduling and monitoring of tasks. It can handle complex data engineering pipelines in production\n",
    " - Airflow adds scheduling, error handling, and reporting to workflows\n",
    " - It implements workflows as DAGs - Directed Acyclic Graphs which is a set of tasks and dependencies between them\n",
    " - It is accessed via code, command-line or via web interface\n",
    " - Other workflow tools include Luigi, SSIS or Bash scripting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdef13fd",
   "metadata": {},
   "source": [
    "Airflow run command\n",
    " - airflow run dag_id task_id start_date\n",
    " - airflow run etl_pipeline download_file 2020-01-08\n",
    "\n",
    "Airflow help\n",
    " - \" airflow -h \" obtains further information about any Airflow command\n",
    " - \" airflow list_dags \" shows a list of the available DAGs\n",
    "\n",
    "Airflow port\n",
    " - \" airflow webserver -p PORT \" runs the server workers on PORT\n",
    " - \" airflow webserver -p 9880 \" runs the server workers on 9880"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e62968",
   "metadata": {},
   "source": [
    "#### DAG\n",
    "\n",
    " - Directed: an inherent flow representing dependencies between components \n",
    " - These dependencies even implicit ones provide context on how to order the running of components\n",
    " - Acyclic: does not loop or repeat, the individual components are only executed once per run\n",
    " - Graph: a graph represents the components and their relationships between them\n",
    " - In Airflow DAGs are written in python but can use components written in other languages\n",
    " - DAGs are made up of components (typically tasks) to be executed such as operators, sensors etc\n",
    " - Dependencies are defined either explicitly or implicitly (?) so that Airflow knows which components should be run at what point within a workflow\n",
    "\n",
    "<img src=\"assets/airflow/command_line_vs_python.png\" style=\"width: 600px;\"/>\n",
    "\n",
    "#### Simple DAG examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c988d3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the DAG object\n",
    "from airflow.models import DAG\n",
    "\n",
    "# Define the default_args dictionary\n",
    "default_args = {\n",
    "  'owner': 'dsmith',\n",
    "  'start_date': datetime(2020, 1, 14),\n",
    "  'retries': 2\n",
    "  'email':'bla@blabla.com'\n",
    "}\n",
    "\n",
    "# Instantiate the DAG object\n",
    "etl_dag = DAG(dag_id='example_etl', default_args=default_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc84310a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.models import DAG\n",
    "\n",
    "default_args = {\n",
    "  'owner': 'jdoe',\n",
    "  'email': 'jdoe@datacamp.com'\n",
    "}\n",
    "dag = DAG( 'refresh_data', default_args=default_args )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3a0559",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.models import DAG\n",
    "default_args = {\n",
    "  'owner': 'jdoe',\n",
    "  'start_date': '2019-01-01'\n",
    "}\n",
    "dag = DAG( dag_id=\"etl_update\", default_args=default_args )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ce25c4",
   "metadata": {},
   "source": [
    "#### Airflow web interface\n",
    " - A web interface that should make it easier to schedule tasks, review processes and correct issues\n",
    " - The Tree View lists the tasks and any ordering between them in a tree structure, with the ability to compress / expand the nodes.\n",
    " - The Graph View shows any tasks and their dependencies in a graph structure, along with the ability to access further details about task runs.\n",
    "- The Code view provides full access to the Python code that makes up the DAG.\n",
    "\n",
    "#### Operators\n",
    " - Represent a single task in a workflow\n",
    " - Run independently (usually), meaning that all resources needed to complete the task are contained within the operator\n",
    " - Generally do not share information (to simplify the workflows)\n",
    " - There are various operators to perform different tasks\n",
    " - For instance, the DummyOperator(task_id='example', dag=dag) can be used to represent a task for trubleshooting or a task that has not yet been implemented\n",
    " - The BashOperator executes a given Bash task or script, it requires 3 arguments and is defined as BashOperator(task_id='example', bash_command='script.sh', dag=dag)\n",
    " - Can specify environment variables for the command\n",
    " - The BashOperator allows you to specify any given Shell command or script and add it to an Airflow workflow. This can be a great start to implementing Airflow in your environment\n",
    "\n",
    "Operator \"gotchas\":\n",
    " - Not guaranteed to run in the same location (directory)\n",
    " - May require extensive use of env variables\n",
    " - Can be difficult to run tasks with elevated privileges (different user access)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d671b5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the BashOperator\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "\n",
    "# Define the BashOperator \n",
    "cleanup = BashOperator(\n",
    "    task_id='cleanup_task',\n",
    "    # Define the bash_command\n",
    "    bash_command='cleanup.sh',\n",
    "    # Add the task to the dag\n",
    "    dag=analytics_dag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab9b04b",
   "metadata": {},
   "source": [
    "#### Multiple BashOperators\n",
    "\n",
    "Airflow DAGs can contain many operators, each performing their defined tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0af78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a second operator to run the `consolidate_data.sh` script\n",
    "consolidate = BashOperator(\n",
    "    task_id='consolidate_task',\n",
    "    bash_command='consolidate_data.sh',\n",
    "    dag=analytics_dag)\n",
    "\n",
    "# Define a final operator to execute the `push_data.sh` script\n",
    "push_data = BashOperator(\n",
    "    task_id='pushdata_task',\n",
    "    bash_command='push_data.sh',\n",
    "    dag=analytics_dag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099c731a",
   "metadata": {},
   "source": [
    "#### Tasks\n",
    " - Instances of operators\n",
    " - Assigned to a variable in Python\n",
    " - Within Airflow tasks are defined by their task id not the variable name\n",
    " \n",
    " \n",
    " - Tasks are either upstream or downstream\n",
    "  - Upstream tasks are those that must be completed prior other any downstream tasks\n",
    " - Dependencies can be defined using the bitshift operators\n",
    "  - \">>\" is the upstream operator\n",
    "  - \"<<\" is the downstream operator\n",
    " - Upstreams means \"before\", downstream means \"after\". Upstream tasks must be completed before downstream tasks\n",
    " \n",
    " \n",
    " - Multiple dependencies can be set like this:\n",
    "  - task1 >> task2 >> task3 >> task4\n",
    "  \n",
    "  \n",
    " - Task dependencies in the Airflow UI:\n",
    " <img src=\"assets/airflow/task_dependencies.png\" style=\"width: 600px;\"/>\n",
    " \n",
    " \n",
    "  - Chained and mixed-dependencies:\n",
    " <img src=\"assets/airflow/chained_dependencies.png\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92672f5",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Define the tasks\n",
    "task1 = BashOperator(task_id='first_task',\n",
    "                     bash_command='echo 1',\n",
    "                     dag=example_dag)\n",
    "task2 = BashOperator(task_id='second_task',\n",
    "                     bash_command='echo 2',\n",
    "                     dag=example_dag)\n",
    "# Set first_task to run before second_task\n",
    "task1 >> task2   # or task2 << task1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf595c7",
   "metadata": {},
   "source": [
    " - Define a BashOperator called pull_sales with a bash command of wget https://salestracking/latestinfo?json.\n",
    " - Set the pull_sales operator to run before the cleanup task.\n",
    " - Configure consolidate to run next, using the downstream operator.\n",
    " - Set push_data to run last using either bitshift operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e47849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new pull_sales task\n",
    "pull_sales = BashOperator(\n",
    "    task_id='pullsales_task',\n",
    "    bash_command='wget https://salestracking/latestinfo?json',\n",
    "    dag=analytics_dag\n",
    ")\n",
    "\n",
    "# Set pull_sales to run prior to cleanup\n",
    "pull_sales >> cleanup\n",
    "\n",
    "# Configure consolidate to run after cleanup\n",
    "consolidate << cleanup\n",
    "\n",
    "# Set push_data to run last\n",
    "consolidate >> push_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ebeb37",
   "metadata": {},
   "source": [
    "#### PythonOperator\n",
    " - Executes a Python function / callable\n",
    " - Operates similarly to BashOperator with more options\n",
    " - Can pass in arguments to the Python code\n",
    " - Arguments can be positional or keyword\n",
    " - Use the op_kwargs dictionary to pass keyword arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f68941",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.operators.python_operator import PythonOperators\n",
    "\n",
    "def printme():\n",
    "    print(\"This goes in the logs!\")\n",
    "python_task = PythonOperator(\n",
    "    task_id='simple_print',\n",
    "    python_callable=printme,\n",
    "    dag=example_dag\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6e7544",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sleep(length_of_time):\n",
    "    time.sleep(length_of_time)\n",
    "\n",
    "sleep_task = PythonOperator(\n",
    "    task_id='sleep',\n",
    "    python_callable=sleep,\n",
    "    op_kwargs={'length_of_time': 5}\n",
    "    dag=example_dag\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4064b2",
   "metadata": {},
   "source": [
    "#### Additional operators\n",
    " - Found in airflow.operators or airflow.contrib.operators libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2e2ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.operators.email_operator import EmailOperator\n",
    "\n",
    "email_task = EmailOperator(\n",
    "    task_id='email_sales_report',\n",
    "    to='sales_manager@example.com',\n",
    "    subject='Automated Sales Report',\n",
    "    html_content='Attached is the latest sales report',\n",
    "    files='latest_sales.xlsx',\n",
    "    dag=example_dag\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00e7c14",
   "metadata": {},
   "source": [
    "#### Example with PythonOperator and EmailOperator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96770d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "\n",
    "def pull_file(URL, savepath):\n",
    "    r = requests.get(URL)\n",
    "    with open(savepath, 'wb') as f:\n",
    "        f.write(r.content)   \n",
    "    # Use the print method for logging\n",
    "    print(f\"File pulled from {URL} and saved to {savepath}\")\n",
    "\n",
    "\n",
    "    \n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "\n",
    "# Create the task\n",
    "pull_file_task = PythonOperator(\n",
    "    task_id='pull_file',\n",
    "    # Add the callable\n",
    "    python_callable=pull_file,\n",
    "    # Define the arguments\n",
    "    op_kwargs={'URL':'http://dataserver/sales.json', 'savepath':'latestsales.json'},\n",
    "    dag=process_sales_dag\n",
    ")\n",
    "\n",
    "\n",
    "# Add another Python task\n",
    "parse_file_task = PythonOperator(\n",
    "    task_id='parse_file',\n",
    "    # Set the function to call\n",
    "    python_callable=parse_file,\n",
    "    # Add the arguments\n",
    "    op_kwargs={'inputfile':'latestsales.json', 'outputfile':'parsedfile.json'},\n",
    "    # Add the DAG\n",
    "    dag=process_sales_dag\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381f7d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Operator\n",
    "from airflow.operators.email_operator import EmailOperator\n",
    "\n",
    "# Define the task\n",
    "email_manager_task = EmailOperator(\n",
    "    task_id='email_manager',\n",
    "    to='manager@datacamp.com',\n",
    "    subject='Latest sales JSON',\n",
    "    html_content='Attached is the latest sales JSON file as requested.',\n",
    "    files='parsedfile.json',\n",
    "    dag=process_sales_dag\n",
    ")\n",
    "\n",
    "# Set the order of tasks\n",
    "pull_file_task >> parse_file_task >> email_manager_task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996f8034",
   "metadata": {},
   "source": [
    "#### DAG Runs\n",
    "\n",
    " - A specific instance of a workflow at a point in time\n",
    " - Can be run manually or via schedule_interval\n",
    " - Maintain state for each workflow and the tasks within\n",
    "     - running, failed, success\n",
    " - In the web interface you can find them at \"Browse Dag Runs\"\n",
    " \n",
    " - Scheduling details:\n",
    "     - start_date: datetime Python object for initial schedule of the DAG run\n",
    "     - end_date: optional attribute to stop running new DAG instances\n",
    "     - max_tries: optional attribute for how many attempts to make\n",
    "     - schedule_interval: how often to run / schedule the DAG for execution, it occurs between the start_date and end_date\n",
    "     - scheduler presets: @once, @hourly, @daily, @weekly or you can use cron format\n",
    "\n",
    "Example:\n",
    " - Set the start date of the DAG to November 1, 2019.\n",
    " - Configure the retry_delay to 20 minutes. You will learn more about the timedelta object in Chapter 3. For now, you just need to know it expects an integer value.\n",
    " - Use the cron syntax to configure a schedule of every Wednesday at 12:30pm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d039f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the scheduling arguments as defined\n",
    "\n",
    "default_args = {\n",
    "  'owner': 'Engineering',\n",
    "  'start_date': datetime(2019, 11, 1),\n",
    "  'email': ['airflowresults@datacamp.com'],\n",
    "  'email_on_failure': False,\n",
    "  'email_on_retry': False,\n",
    "  'retries': 3,\n",
    "  'retry_delay': timedelta(minutes=20)\n",
    "}\n",
    "\n",
    "dag = DAG('update_dataflows',\n",
    "          default_args=default_args,\n",
    "          schedule_interval='30 12 * * 3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a9ead3",
   "metadata": {},
   "source": [
    "#### Full Example\n",
    " - Note that this will not be triggered before a month has passed after the start_date of Feb 15, 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd17e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from datetime import datetime\n",
    "from airflow.models import DAG\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from airflow.operators.email_operator import EmailOperator\n",
    "\n",
    "\n",
    "default_args = {\n",
    "    'owner':'sales_eng',\n",
    "    'start_date': datetime(2020, 2, 15),\n",
    "}\n",
    "\n",
    "process_sales_dag = DAG(dag_id='process_sales', default_args=default_args, schedule_interval='@monthly')\n",
    "\n",
    "\n",
    "def pull_file(URL, savepath):\n",
    "    r = requests.get(URL)\n",
    "    with open(savepath, 'w') as f:\n",
    "        f.write(r.content)\n",
    "    print(f\"File pulled from {URL} and saved to {savepath}\")\n",
    "    \n",
    "\n",
    "pull_file_task = PythonOperator(\n",
    "    task_id='pull_file',\n",
    "    # Add the callable\n",
    "    python_callable=pull_file,\n",
    "    # Define the arguments\n",
    "    op_kwargs={'URL':'http://dataserver/sales.json', 'savepath':'latestsales.json'},\n",
    "    dag=process_sales_dag\n",
    ")\n",
    "\n",
    "def parse_file(inputfile, outputfile):\n",
    "    with open(inputfile) as infile:\n",
    "        data=json.load(infile)\n",
    "        with open(outputfile, 'w') as outfile:\n",
    "            json.dump(data, outfile)\n",
    "        \n",
    "parse_file_task = PythonOperator(\n",
    "    task_id='parse_file',\n",
    "    # Set the function to call\n",
    "    python_callable=parse_file,\n",
    "    # Add the arguments\n",
    "    op_kwargs={'inputfile':'latestsales.json', 'outputfile':'parsedfile.json'},\n",
    "    # Add the DAG\n",
    "    dag=process_sales_dag\n",
    ")\n",
    "\n",
    "email_manager_task = EmailOperator(\n",
    "    task_id='email_manager',\n",
    "    to='manager@datacamp.com',\n",
    "    subject='Latest sales JSON',\n",
    "    html_content='Attached is the latest sales JSON file as requested.',\n",
    "    files='parsedfile.json',\n",
    "    dag=process_sales_dag\n",
    ")\n",
    "\n",
    "pull_file_task >> parse_file_task >> email_manager_task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54563576",
   "metadata": {},
   "source": [
    "#### Sensors\n",
    " - Use sensors when:\n",
    "     - uncertain when a condition will be true\n",
    "     - if you don't want to fail the intire DAG immediately but want to continue checking if a condition has been met\n",
    "     - add task repetition wihout loops\n",
    "     \n",
    "     \n",
    " - Sensors are special operators that wait for a certain condition to be true\n",
    " - Conditions can include the creation of a file, upload of a database record, certain response from a web request\n",
    " - Can define how often to check for the condition to be true\n",
    "     - mode determines how to check for the condition\n",
    "     - mode = 'poke' means it checks repeatedly\n",
    "     - mode = 'reschedule' means it give up task slot and try again later\n",
    " - Are assigned to tasks like normal operators\n",
    " - Derived from \" airflow.sensors.base_sensor_operator \"\n",
    " - poke_interval refers to how often to wait between checks\n",
    " - timeout refers to how long to wait before failing task (timeout must be significantly shorter than the schedule interval)\n",
    "\n",
    "#### Useful sensors\n",
    "\n",
    "File sensor:\n",
    " - checks for the existence of a file at a certain location\n",
    " - can check if any files exist within a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "795912a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.contrib.sensors.file_sensor import FileSensor\n",
    "\n",
    "file_sensor_task = FileSensor(task_id='file_sense',\n",
    "                              filepath='salesdata.csv',\n",
    "                              poke_interval=300,\n",
    "                              dag=sales_report_dag)\n",
    "\n",
    "init_sales_cleanup >> file_sensor_task >> generate_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e608539e",
   "metadata": {},
   "source": [
    "ExternalTaskSensor\n",
    " - waits for a task in another DAG to complete\n",
    "\n",
    "HttpSensor\n",
    " - makes requests to a web URL and checks for content\n",
    "\n",
    "SqlSensor\n",
    " - runs a SQL query to check for content\n",
    "\n",
    "Find more sensor operators under\n",
    " - airflow.sensors\n",
    " - airflow.contrib.sensors\n",
    "\n",
    "\n",
    "#### Full sensor example\n",
    " - Note that this DAG is waiting for the file salesdata_ready.csv to be present before it can start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761d2202",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.models import DAG\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from airflow.operators.http_operator import SimpleHttpOperator\n",
    "from airflow.contrib.sensors.file_sensor import FileSensor\n",
    "\n",
    "dag = DAG(\n",
    "   dag_id = 'update_state',\n",
    "   default_args={\"start_date\": \"2019-10-01\"}\n",
    ")\n",
    "\n",
    "precheck = FileSensor(\n",
    "   task_id='check_for_datafile',\n",
    "   filepath='salesdata_ready.csv',\n",
    "   dag=dag)\n",
    "\n",
    "part1 = BashOperator(\n",
    "   task_id='generate_random_number',\n",
    "   bash_command='echo $RANDOM',\n",
    "   dag=dag\n",
    ")\n",
    "\n",
    "import sys\n",
    "def python_version():\n",
    "    return sys.version\n",
    "\n",
    "part2 = PythonOperator(\n",
    "   task_id='get_python_version',\n",
    "   python_callable=python_version,\n",
    "   dag=dag)\n",
    "   \n",
    "part3 = SimpleHttpOperator(\n",
    "   task_id='query_server_for_external_ip',\n",
    "   endpoint='https://api.ipify.org',\n",
    "   method='GET',\n",
    "   dag=dag)\n",
    "   \n",
    "precheck >> part3 >> part2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bed3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.models import DAG\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "from airflow.contrib.sensors.file_sensor import FileSensor\n",
    "from datetime import datetime\n",
    "\n",
    "report_dag = DAG(\n",
    "    dag_id = 'execute_report',\n",
    "    schedule_interval = \"0 0 * * *\"\n",
    ")\n",
    "\n",
    "precheck = FileSensor(\n",
    "    task_id='check_for_datafile',\n",
    "    filepath='salesdata_ready.csv',\n",
    "    start_date=datetime(2020,2,20),\n",
    "    mode='reschedule',\n",
    "    dag=report_dag\n",
    ")\n",
    "\n",
    "generate_report_task = BashOperator(\n",
    "    task_id='generate_report',\n",
    "    bash_command='generate_report.sh',\n",
    "    start_date=datetime(2020,2,20),\n",
    "    dag=report_dag\n",
    ")\n",
    "\n",
    "precheck >> generate_report_task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2893284c",
   "metadata": {},
   "source": [
    "#### Airflow executors\n",
    " - An executor is the component that runs the tasks defined in a workflow\n",
    " - Different executors handle running the tasks differently, some may run a single task at a time on a local system, while others might split individual tasks among all the systems in a cluster\n",
    " - This is oftenr referred to as the number of worker slots available\n",
    " - Example executors:\n",
    "   - SequentialExecutor\n",
    "   - LocalExecutor\n",
    "   - CeleryExecutor\n",
    "\n",
    "SequentialExecutor:\n",
    " - default Airflow executor\n",
    " - runs one task at a time\n",
    " - useful for debugging\n",
    " - not recommended for production due to its limitations of task resources\n",
    "\n",
    "LocalExecutor:\n",
    " - runs on a single system\n",
    " - treats each task as a process on the local system and can start as many concurrent tasks as desired, requested and permitted by the system resources (CPU cores, memory etc)\n",
    " - concurrency allows for parallelism as defined by the user either unlimited or limited to a certain number of simultaneous tasks\n",
    " - can utilise all the resources of a given host system\n",
    "\n",
    "CeleryExecutor\n",
    " - uses a Celery backend as task manager\n",
    " - Celery is a general queuing system written in Python that allows multiple systems to communicate as a basic cluster\n",
    " - using a CeleryExecutor multiple Airflow systems can be configured as workflows for a given set of tasks\n",
    " - is difficult to setup and configure\n",
    " - it is a powerful choice however when one expects to have large number of DAGs or expects their processing needs to grow\n",
    "\n",
    "Determining which executor is being used:\n",
    " - in the command line use << cat airflow/airflow.cfg | grep \"executor=\" >>\n",
    " - in the command line use << airflow list_dags >> and look for the INFO output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ae737e",
   "metadata": {},
   "source": [
    "#### Troubleshooting\n",
    "\n",
    "Common issues\n",
    " - DAGs won't run on schedule\n",
    "     - Check if the scheduler is running (the Airflow scheduler handles DAG run and task scheduling, if it is not running no tasks can run)\n",
    "     - An Error that says \" the scheduler does not appear to be running \" will normally show up\n",
    "     - Fix by running \" airflow scheduler \" from the command line\n",
    "     - Another reason might be that the \" schedule_interval \" argument hasn't passed.\n",
    "     - Modify accordingly to meet your requirements\n",
    "     - Lastly, it might be that there are not enought free tasks within the executor to run\n",
    "     - If so change the executor type (or add more resources!)\n",
    "     \n",
    "     \n",
    " - DAGs won't load\n",
    "     - DAG not in web UI\n",
    "     - DAG not in \" airflow list_dags \"\n",
    "     - Verify DAG files are in the correct folder\n",
    "     - Determine the DAG folder by examining the airflow.cfg file\n",
    "     - Use \" head airflow/airflow.cfg \", the dags_folder shows the path\n",
    "     \n",
    "     <img src=\"assets/airflow/airflow_cfg.png\" style=\"width: 600px;\"/>\n",
    "\n",
    "\n",
    " - Syntax errors\n",
    "     - Most common reason a DAG file won't appear\n",
    "     - Kinda difficult to find errors in DAG\n",
    "     - Run \" airflow list_dags \" -> python3 <dag_file.py>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9122badc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the command line run:\n",
    "airflow list_dags\n",
    "\n",
    "# you'll find out what is the dags_folder\n",
    "cd /home/repl/workspace/dags/\n",
    "\n",
    "# open the dag to figure out what is going on\n",
    "nano dag_file.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5a6d4f",
   "metadata": {},
   "source": [
    "#### Missing DAG\n",
    "\n",
    "Lol: Your manager calls you before you're about to leave for the evening and wants to know why a new DAG workflow she's created isn't showing up in the system. She needs this DAG called execute_report to appear in the system so she can properly schedule it for some tests before she leaves on a trip.\n",
    "\n",
    " - Airflow is configured using the ~/airflow/airflow.cfg file.\n",
    "\n",
    " - Examine the DAG for any errors and fix those.\n",
    " - Determine if the DAG has loaded after fixing the errors.\n",
    " - If not, determine why the DAG has not loaded and fix the final issue. - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df00c344",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.models import DAG\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "from datetime import datetime\n",
    "\n",
    "sample_dag = DAG(\n",
    "    dag_id = 'sample_dag',\n",
    "    schedule_interval = \"0 0 * * *\"\n",
    ")\n",
    "\n",
    "sample_task = BashOperator(\n",
    "    task_id='sample',\n",
    "    bash_command='generate_sample.sh',\n",
    "    start_date=datetime(2020,2,20),\n",
    "    dag=sample_dag\n",
    ")\n",
    "\n",
    "\n",
    "from airflow.models import DAG\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "from airflow.contrib.sensors.file_sensor import FileSensor\n",
    "from datetime import datetime\n",
    "\n",
    "report_dag = DAG(\n",
    "    dag_id = 'execute_report',\n",
    "    schedule_interval = \"0 0 * * *\"\n",
    ")\n",
    "\n",
    "precheck = FileSensor(\n",
    "    task_id='check_for_datafile',\n",
    "    filepath='salesdata_ready.csv',\n",
    "    start_date=datetime(2020,2,20),\n",
    "    mode='poke',\n",
    "    dag=report_dag)\n",
    "\n",
    "generate_report_task = BashOperator(\n",
    "    task_id='generate_report',\n",
    "    bash_command='generate_report.sh',\n",
    "    start_date=datetime(2020,2,20),\n",
    "    dag=report_dag\n",
    ")\n",
    "\n",
    "precheck >> generate_report_task\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abf94e1",
   "metadata": {},
   "source": [
    "#### SLAs\n",
    " - SLA = service level agreement\n",
    " - Within Airflow, this is the amount of time a task or a DAG should require to run\n",
    " - An SLA miss is any time the task or the dag does not meet the expected timing\n",
    " - If an SLA is missed, an email is sent out and a log is stored\n",
    " - SLA misses can be viewed in the web UI (Browse -> SLA Misses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59232214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to define an SLA\n",
    "task1 = BashOperator(task_id='sla_task',\n",
    "                   bash_command='runcode.sh',\n",
    "                   sla=timedelta(seconds=30), dag=dag)\n",
    "\n",
    "default_args={\n",
    " 'sla': timedelta(minutes=20)\n",
    " 'start_date': datetime(2020,2,20)\n",
    "}\n",
    "\n",
    "dag = DAG('sla_dag', default_args=default_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e800e26e",
   "metadata": {},
   "source": [
    "### SLA example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390e938f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the timedelta object\n",
    "from datetime import timedelta\n",
    "### SLA example 1\n",
    "# Create the dictionary entry\n",
    "default_args = {\n",
    "  'start_date': datetime(2020, 2, 20),\n",
    "  'sla': timedelta(minutes=30)\n",
    "}\n",
    "\n",
    "# Add to the DAG\n",
    "test_dag = DAG('test_workflow', default_args=default_args, schedule_interval='@None')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672bc70c",
   "metadata": {},
   "source": [
    "### SLA example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57365861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the timedelta object\n",
    "from datetime import timedelta\n",
    "\n",
    "test_dag = DAG('test_workflow', start_date=datetime(2020,2,20), schedule_interval='@None')\n",
    "\n",
    "# Create the task with the SLA\n",
    "task1 = BashOperator(task_id='first_task',\n",
    "                     sla=timedelta(hours=3),\n",
    "                     bash_command='initialize_data.sh',\n",
    "                     dag=test_dag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd330f8",
   "metadata": {},
   "source": [
    "### SLA example 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40221395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the email task\n",
    "email_report = EmailOperator(\n",
    "        task_id='email_report',\n",
    "        to='airflow@datacamp.com',\n",
    "        subject='Airflow Monthly Report',\n",
    "        html_content=\"\"\"Attached is your monthly workflow report - please refer to it for more detail\"\"\",\n",
    "        files=['monthly_report.pdf'],\n",
    "        dag=report_dag\n",
    ")\n",
    "\n",
    "# Set the email task to run after the report is generated\n",
    "email_report << generate_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3f95b7",
   "metadata": {},
   "source": [
    "### SLA example 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3dc4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.models import DAG\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "from airflow.contrib.sensors.file_sensor import FileSensor\n",
    "from datetime import datetime\n",
    "\n",
    "default_args={\n",
    "    'email': ['airflowalerts@datacamp.com','airflowadmin@datacamp.com'],\n",
    "    'email_on_failure': True,\n",
    "    'email_on_success': True,\n",
    "}\n",
    "\n",
    "report_dag = DAG(\n",
    "    dag_id = 'execute_report',\n",
    "    schedule_interval = \"0 0 * * *\",\n",
    "    default_args=default_args\n",
    ")\n",
    "\n",
    "precheck = FileSensor(\n",
    "    task_id='check_for_datafile',\n",
    "    filepath='salesdata_ready.csv',\n",
    "    start_date=datetime(2020,2,20),\n",
    "    mode='reschedule',\n",
    "    dag=report_dag)\n",
    "\n",
    "generate_report_task = BashOperator(\n",
    "    task_id='generate_report',\n",
    "    bash_command='generate_report.sh',\n",
    "    start_date=datetime(2020,2,20),\n",
    "    dag=report_dag\n",
    ")\n",
    "\n",
    "precheck >> generate_report_task\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
