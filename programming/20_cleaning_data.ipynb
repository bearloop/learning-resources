{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4acd65da",
   "metadata": {},
   "source": [
    "## Cleaning data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b3e9ce",
   "metadata": {},
   "source": [
    "<img src=\"assets/cleaning_data/data_types.png\" style=\"height: 200px;\"/>\n",
    "\n",
    "<img src=\"assets/cleaning_data/gigo.png\" style=\"height: 150px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "378b8f43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25760 entries, 0 to 25759\n",
      "Data columns (total 10 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   Unnamed: 0       25760 non-null  int64 \n",
      " 1   duration         25760 non-null  object\n",
      " 2   station_A_id     25760 non-null  int64 \n",
      " 3   station_A_name   25760 non-null  object\n",
      " 4   station_B_id     25760 non-null  int64 \n",
      " 5   station_B_name   25760 non-null  object\n",
      " 6   bike_id          25760 non-null  int64 \n",
      " 7   user_type        25760 non-null  int64 \n",
      " 8   user_birth_year  25760 non-null  int64 \n",
      " 9   user_gender      25760 non-null  object\n",
      "dtypes: int64(6), object(4)\n",
      "memory usage: 2.0+ MB\n",
      "None\n",
      "count    25760.000000\n",
      "mean         2.008385\n",
      "std          0.704541\n",
      "min          1.000000\n",
      "25%          2.000000\n",
      "50%          2.000000\n",
      "75%          3.000000\n",
      "max          3.000000\n",
      "Name: user_type, dtype: float64\n",
      "count     25760\n",
      "unique        3\n",
      "top           2\n",
      "freq      12972\n",
      "Name: user_type_cat, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "ride_sharing = pd.read_csv('assets/cleaning_data/ride_sharing_new.csv')\n",
    "\n",
    "# Print the information of ride_sharing\n",
    "print(ride_sharing.info())\n",
    "\n",
    "# Print summary statistics of user_type column\n",
    "print(ride_sharing['user_type'].describe())\n",
    "\n",
    "# Convert user_type from integer to category\n",
    "ride_sharing['user_type_cat'] = ride_sharing['user_type'].astype('category')\n",
    "\n",
    "# Write an assert statement confirming the change\n",
    "assert ride_sharing['user_type_cat'].dtype == 'category'\n",
    "\n",
    "# Print new summary statistics \n",
    "print(ride_sharing['user_type_cat'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0c48f6",
   "metadata": {},
   "source": [
    "#### Summing strings and concatenating numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d82b0ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         duration duration_trim  duration_time\n",
      "0      12 minutes           12              12\n",
      "1      24 minutes           24              24\n",
      "2       8 minutes            8               8\n",
      "3       4 minutes            4               4\n",
      "4      11 minutes           11              11\n",
      "...           ...           ...            ...\n",
      "25755  11 minutes           11              11\n",
      "25756  10 minutes           10              10\n",
      "25757  14 minutes           14              14\n",
      "25758  14 minutes           14              14\n",
      "25759  29 minutes           29              29\n",
      "\n",
      "[25760 rows x 3 columns]\n",
      "11.389052795031056\n"
     ]
    }
   ],
   "source": [
    "# Strip duration of minutes\n",
    "ride_sharing['duration_trim'] = ride_sharing['duration'].str.strip('minutes')\n",
    "\n",
    "# Convert duration to integer\n",
    "ride_sharing['duration_time'] = ride_sharing['duration_trim'].astype(int)\n",
    "\n",
    "# Write an assert statement making sure of conversion\n",
    "assert ride_sharing['duration_time'].dtype == 'int'\n",
    "\n",
    "# Print formed columns and calculate average ride duration \n",
    "print(ride_sharing[['duration','duration_trim','duration_time']])\n",
    "print(ride_sharing['duration_time'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2691ccd",
   "metadata": {},
   "source": [
    "#### Dealing with out of range data\n",
    "\n",
    " - Dropping data\n",
    " - Setting custom minimum and maximum values\n",
    " - Treat as missing and impute\n",
    " - Setting custom value depending on business assumptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed6c774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tire_sizes to integer\n",
    "ride_sharing['tire_sizes'] = ride_sharing['tire_sizes'].astype('int')\n",
    "\n",
    "# Set all values above 27 to 27\n",
    "ride_sharing.loc[ride_sharing['tire_sizes'] > 27, 'tire_sizes'] = 27\n",
    "\n",
    "# Reconvert tire_sizes back to categorical\n",
    "ride_sharing['tire_sizes'] = ride_sharing['tire_sizes'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793f27d2",
   "metadata": {},
   "source": [
    "Convert ride_date to a datetime object using to_datetime(), then convert the datetime object into a date and store it in ride_dt column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4222123b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert ride_date to date\n",
    "ride_sharing['ride_dt'] = pd.to_datetime(ride_sharing['ride_date']).dt.date\n",
    "\n",
    "# Save today's date\n",
    "today = dt.date.today()\n",
    "\n",
    "# Set all in the future to today's date\n",
    "ride_sharing.loc[ride_sharing['ride_dt'] > today, 'ride_dt'] = today"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77975ec4",
   "metadata": {},
   "source": [
    "#### Duplicate values\n",
    "\n",
    "<img src=\"assets/cleaning_data/duplicates.png\" style=\"height: 200px;\"/>\n",
    "<img src=\"assets/cleaning_data/find_duplicates.png\" style=\"height: 200px;\"/>\n",
    "<img src=\"assets/cleaning_data/treat_duplicates.png\" style=\"height: 200px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f493e323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find duplicates\n",
    "duplicates = ride_sharing.duplicated('ride_id', keep=False)\n",
    "\n",
    "# Sort your duplicated rides\n",
    "duplicated_rides = ride_sharing[duplicates].sort_values('ride_id')\n",
    "\n",
    "# Print relevant columns of duplicated_rides\n",
    "print(duplicated_rides[['ride_id','duration','user_birth_year']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10da48de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop complete duplicates from ride_sharing\n",
    "ride_dup = ride_sharing.drop_duplicates()\n",
    "\n",
    "# Create statistics dictionary for aggregation function\n",
    "statistics = {'user_birth_year': 'min', 'duration': 'mean'}\n",
    "\n",
    "# Group by ride_id and compute new statistics\n",
    "ride_unique = ride_dup.groupby('ride_id').aggregate(statistics).reset_index()\n",
    "\n",
    "# Find duplicated values again\n",
    "duplicates = ride_unique.duplicated(subset = 'ride_id', keep = False)\n",
    "duplicated_rides = ride_unique[duplicates == True]\n",
    "\n",
    "# Assert duplicates are processed\n",
    "assert duplicated_rides.shape[0] == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf05014",
   "metadata": {},
   "source": [
    "#### Membership constraints\n",
    " - Data type constraints, data range constrains, uniqueness constraints, and membership constraints for categorical values\n",
    " - Due to data entry or parsing errors\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06231cfc",
   "metadata": {},
   "source": [
    "Print the categories DataFrame and take a close look at all possible correct categories of the survey columns.\n",
    "\n",
    "Print the unique values of the survey columns in airlines using the .unique() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2184ce30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      cleanliness           safety          satisfaction\n",
      "0           Clean          Neutral        Very satisfied\n",
      "1         Average        Very safe               Neutral\n",
      "2  Somewhat clean    Somewhat safe    Somewhat satisfied\n",
      "3  Somewhat dirty      Very unsafe  Somewhat unsatisfied\n",
      "4           Dirty  Somewhat unsafe      Very unsatisfied\n",
      "Cleanliness:  ['Clean' 'Average' 'Somewhat clean' 'Somewhat dirty' 'Dirty'] \n",
      "\n",
      "Safety:  ['Neutral' 'Very safe' 'Somewhat safe' 'Very unsafe' 'Somewhat unsafe'] \n",
      "\n",
      "Satisfaction:  ['Very satisfied' 'Neutral' 'Somewhat satsified' 'Somewhat unsatisfied'\n",
      " 'Very unsatisfied'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "airlines = pd.read_csv('assets/cleaning_data/airlines_final.csv')\n",
    "\n",
    "categories = pd.DataFrame({'cleanliness':['Clean','Average','Somewhat clean','Somewhat dirty','Dirty'],\n",
    " 'safety':['Neutral','Very safe','Somewhat safe','Very unsafe','Somewhat unsafe'],\n",
    " 'satisfaction':['Very satisfied','Neutral','Somewhat satisfied','Somewhat unsatisfied','Very unsatisfied']\n",
    "})\n",
    "\n",
    "# Print categories DataFrame\n",
    "print(categories)\n",
    "\n",
    "# Print unique values of survey columns in airlines\n",
    "print('Cleanliness: ', airlines['cleanliness'].unique(), \"\\n\")\n",
    "print('Safety: ', airlines['safety'].unique(), \"\\n\")\n",
    "print('Satisfaction: ', airlines['satisfaction'].unique(), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0825dbad",
   "metadata": {},
   "source": [
    "Create a set out of the cleanliness column in airlines using set() and find the inconsistent category by finding the difference in the cleanliness column of categories.\n",
    "\n",
    "Find rows of airlines with a cleanliness value not in categories and print the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "03fc615d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Unnamed: 0, id, day, airline, destination, dest_region, dest_size, boarding_area, dept_time, wait_min, cleanliness, safety, satisfaction]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Find the cleanliness category in airlines not in categories\n",
    "cat_clean = set(airlines['cleanliness']).difference(categories['cleanliness'])\n",
    "\n",
    "# Find rows with that category\n",
    "cat_clean_rows = airlines['cleanliness'].isin(cat_clean)\n",
    "\n",
    "# Print rows with inconsistent category\n",
    "print(airlines[cat_clean_rows])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc82ac27",
   "metadata": {},
   "source": [
    "Print the rows with the consistent categories of cleanliness only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce2aaf2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Unnamed: 0, id, day, airline, destination, dest_region, dest_size, boarding_area, dept_time, wait_min, cleanliness, safety, satisfaction]\n",
      "Index: []\n",
      "      Unnamed: 0    id        day        airline        destination  \\\n",
      "0              0  1351    Tuesday    UNITED INTL             KANSAI   \n",
      "1              1   373     Friday         ALASKA  SAN JOSE DEL CABO   \n",
      "2              2  2820   Thursday          DELTA        LOS ANGELES   \n",
      "3              3  1157    Tuesday      SOUTHWEST        LOS ANGELES   \n",
      "4              4  2992  Wednesday       AMERICAN              MIAMI   \n",
      "...          ...   ...        ...            ...                ...   \n",
      "2472        2804  1475    Tuesday         ALASKA       NEW YORK-JFK   \n",
      "2473        2805  2222   Thursday      SOUTHWEST            PHOENIX   \n",
      "2474        2806  2684     Friday         UNITED            ORLANDO   \n",
      "2475        2807  2549    Tuesday        JETBLUE         LONG BEACH   \n",
      "2476        2808  2162   Saturday  CHINA EASTERN            QINGDAO   \n",
      "\n",
      "        dest_region dest_size boarding_area   dept_time  wait_min  \\\n",
      "0              Asia       Hub  Gates 91-102  2018-12-31     115.0   \n",
      "1     Canada/Mexico     Small   Gates 50-59  2018-12-31     135.0   \n",
      "2           West US       Hub   Gates 40-48  2018-12-31      70.0   \n",
      "3           West US       Hub   Gates 20-39  2018-12-31     190.0   \n",
      "4           East US       Hub   Gates 50-59  2018-12-31     559.0   \n",
      "...             ...       ...           ...         ...       ...   \n",
      "2472        East US       Hub   Gates 50-59  2018-12-31     280.0   \n",
      "2473        West US       Hub   Gates 20-39  2018-12-31     165.0   \n",
      "2474        East US       Hub   Gates 70-90  2018-12-31      92.0   \n",
      "2475        West US     Small    Gates 1-12  2018-12-31      95.0   \n",
      "2476           Asia     Large    Gates 1-12  2018-12-31     220.0   \n",
      "\n",
      "         cleanliness         safety        satisfaction  \n",
      "0              Clean        Neutral      Very satisfied  \n",
      "1              Clean      Very safe      Very satisfied  \n",
      "2            Average  Somewhat safe             Neutral  \n",
      "3              Clean      Very safe  Somewhat satsified  \n",
      "4     Somewhat clean      Very safe  Somewhat satsified  \n",
      "...              ...            ...                 ...  \n",
      "2472  Somewhat clean        Neutral  Somewhat satsified  \n",
      "2473           Clean      Very safe      Very satisfied  \n",
      "2474           Clean      Very safe      Very satisfied  \n",
      "2475           Clean  Somewhat safe      Very satisfied  \n",
      "2476           Clean      Very safe  Somewhat satsified  \n",
      "\n",
      "[2477 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "# Find the cleanliness category in airlines not in categories\n",
    "cat_clean = set(airlines['cleanliness']).difference(categories['cleanliness'])\n",
    "\n",
    "# Find rows with that category\n",
    "cat_clean_rows = airlines['cleanliness'].isin(cat_clean)\n",
    "\n",
    "# Print rows with inconsistent category\n",
    "print(airlines[cat_clean_rows])\n",
    "\n",
    "# Print rows with consistent categories only\n",
    "print(airlines[cat_clean_rows==False])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4314ff7",
   "metadata": {},
   "source": [
    "#### Categorical values\n",
    "\n",
    " - Value inconsistencies\n",
    " - Collapsing too many categories to few\n",
    " - Making sure data is of type \"category\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba72bf7f",
   "metadata": {},
   "source": [
    "Inconsistent categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "144c7208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Asia' 'Canada/Mexico' 'West US' 'East US' 'Midwest US' 'EAST US'\n",
      " 'Middle East' 'Europe' 'eur' 'Central/South America'\n",
      " 'Australia/New Zealand' 'middle east']\n",
      "['Hub' 'Small' '    Hub' 'Medium' 'Large' 'Hub     ' '    Small'\n",
      " 'Medium     ' '    Medium' 'Small     ' '    Large' 'Large     ']\n",
      "['Hub' 'Small' 'Medium' 'Large']\n",
      "['asia' 'canada/mexico' 'west us' 'east us' 'midwest us' 'middle east'\n",
      " 'europe' 'central/south america' 'australia/new zealand']\n"
     ]
    }
   ],
   "source": [
    "# Print unique values of both columns\n",
    "print(airlines['dest_region'].unique())\n",
    "print(airlines['dest_size'].unique())\n",
    "\n",
    "# Lower dest_region column and then replace \"eur\" with \"europe\"\n",
    "airlines['dest_region'] = airlines['dest_region'].str.lower() \n",
    "airlines['dest_region'] = airlines['dest_region'].replace({'eur':'europe'})\n",
    "\n",
    "# Remove white spaces from `dest_size`\n",
    "airlines['dest_size'] = airlines['dest_size'].str.strip()\n",
    "\n",
    "# Verify changes have been effected\n",
    "print(airlines['dest_size'].unique())\n",
    "print(airlines['dest_region'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99dd8b02",
   "metadata": {},
   "source": [
    "Remapping categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "58a8bf54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day_week</th>\n",
       "      <th>wait_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>weekday</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>weekday</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>weekday</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>weekday</td>\n",
       "      <td>long</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>weekday</td>\n",
       "      <td>long</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  day_week wait_type\n",
       "0  weekday    medium\n",
       "1  weekday    medium\n",
       "2  weekday    medium\n",
       "3  weekday      long\n",
       "4  weekday      long"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create ranges for categories\n",
    "label_ranges = [0, 60, 180, np.inf]\n",
    "label_names = ['short', 'medium', 'long']\n",
    "\n",
    "# Create wait_type column\n",
    "airlines['wait_type'] = pd.cut(airlines['wait_min'], bins = label_ranges, \n",
    "                                labels = label_names)\n",
    "\n",
    "# Create mappings and replace\n",
    "mappings = {'Monday':'weekday', 'Tuesday':'weekday', 'Wednesday': 'weekday', \n",
    "            'Thursday': 'weekday', 'Friday': 'weekday', \n",
    "            'Saturday': 'weekend', 'Sunday': 'weekend'}\n",
    "\n",
    "airlines['day_week'] = airlines['day'].replace(mappings)\n",
    "\n",
    "airlines[['day_week','wait_type']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c768d6be",
   "metadata": {},
   "source": [
    "Cleaning text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455e277a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace \"Dr.\" with empty string \"\"\n",
    "airlines['full_name'] = airlines['full_name'].str.replace(\"Dr.\",\"\")\n",
    "\n",
    "# Replace \"Mr.\" with empty string \"\"\n",
    "airlines['full_name'] = airlines['full_name'].str.replace(\"Mr.\",\"\")\n",
    "\n",
    "# Replace \"Miss\" with empty string \"\"\n",
    "airlines['full_name'] = airlines['full_name'].str.replace(\"Miss\",\"\")\n",
    "\n",
    "\n",
    "# Replace \"Ms.\" with empty string \"\"\n",
    "airlines['full_name'] = airlines['full_name'].str.replace(\"Ms.\",\"\")\n",
    "\n",
    "# Assert that full_name has no honorifics\n",
    "assert airlines['full_name'].str.contains('Ms.|Mr.|Miss|Dr.').any() == False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475b6a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store length of each row in survey_response column\n",
    "resp_length = airlines['survey_response'].str.len()\n",
    "\n",
    "# Find rows in airlines where resp_length > 40\n",
    "airlines_survey = airlines[resp_length > 40]\n",
    "\n",
    "# Assert minimum survey_response length is > 40\n",
    "assert airlines_survey['survey_response'].str.len().min() > 40\n",
    "\n",
    "# Print new survey_response column\n",
    "print(airlines_survey['survey_response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b5bc99ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>day</th>\n",
       "      <th>airline</th>\n",
       "      <th>destination</th>\n",
       "      <th>dest_region</th>\n",
       "      <th>dest_size</th>\n",
       "      <th>boarding_area</th>\n",
       "      <th>dept_time</th>\n",
       "      <th>wait_min</th>\n",
       "      <th>cleanliness</th>\n",
       "      <th>safety</th>\n",
       "      <th>satisfaction</th>\n",
       "      <th>wait_type</th>\n",
       "      <th>day_week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1351</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>UNITED INTL</td>\n",
       "      <td>KANSAI</td>\n",
       "      <td>asia</td>\n",
       "      <td>Hub</td>\n",
       "      <td>Gates 91-102</td>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>115.0</td>\n",
       "      <td>Clean</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Very satisfied</td>\n",
       "      <td>medium</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>373</td>\n",
       "      <td>Friday</td>\n",
       "      <td>ALASKA</td>\n",
       "      <td>SAN JOSE DEL CABO</td>\n",
       "      <td>canada/mexico</td>\n",
       "      <td>Small</td>\n",
       "      <td>Gates 50-59</td>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>135.0</td>\n",
       "      <td>Clean</td>\n",
       "      <td>Very safe</td>\n",
       "      <td>Very satisfied</td>\n",
       "      <td>medium</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2820</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>DELTA</td>\n",
       "      <td>LOS ANGELES</td>\n",
       "      <td>west us</td>\n",
       "      <td>Hub</td>\n",
       "      <td>Gates 40-48</td>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>70.0</td>\n",
       "      <td>Average</td>\n",
       "      <td>Somewhat safe</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>medium</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1157</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>SOUTHWEST</td>\n",
       "      <td>LOS ANGELES</td>\n",
       "      <td>west us</td>\n",
       "      <td>Hub</td>\n",
       "      <td>Gates 20-39</td>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>190.0</td>\n",
       "      <td>Clean</td>\n",
       "      <td>Very safe</td>\n",
       "      <td>Somewhat satsified</td>\n",
       "      <td>long</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2992</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>AMERICAN</td>\n",
       "      <td>MIAMI</td>\n",
       "      <td>east us</td>\n",
       "      <td>Hub</td>\n",
       "      <td>Gates 50-59</td>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>559.0</td>\n",
       "      <td>Somewhat clean</td>\n",
       "      <td>Very safe</td>\n",
       "      <td>Somewhat satsified</td>\n",
       "      <td>long</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0    id        day      airline        destination    dest_region  \\\n",
       "0           0  1351    Tuesday  UNITED INTL             KANSAI           asia   \n",
       "1           1   373     Friday       ALASKA  SAN JOSE DEL CABO  canada/mexico   \n",
       "2           2  2820   Thursday        DELTA        LOS ANGELES        west us   \n",
       "3           3  1157    Tuesday    SOUTHWEST        LOS ANGELES        west us   \n",
       "4           4  2992  Wednesday     AMERICAN              MIAMI        east us   \n",
       "\n",
       "  dest_size boarding_area   dept_time  wait_min     cleanliness  \\\n",
       "0       Hub  Gates 91-102  2018-12-31     115.0           Clean   \n",
       "1     Small   Gates 50-59  2018-12-31     135.0           Clean   \n",
       "2       Hub   Gates 40-48  2018-12-31      70.0         Average   \n",
       "3       Hub   Gates 20-39  2018-12-31     190.0           Clean   \n",
       "4       Hub   Gates 50-59  2018-12-31     559.0  Somewhat clean   \n",
       "\n",
       "          safety        satisfaction wait_type day_week  \n",
       "0        Neutral      Very satisfied    medium  weekday  \n",
       "1      Very safe      Very satisfied    medium  weekday  \n",
       "2  Somewhat safe             Neutral    medium  weekday  \n",
       "3      Very safe  Somewhat satsified      long  weekday  \n",
       "4      Very safe  Somewhat satsified      long  weekday  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airlines.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf3a5fa",
   "metadata": {},
   "source": [
    "#### Uniformity\n",
    "\n",
    " - Uniform currencies, date formats, weight metrics, temperature scales etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc548b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert account_opened to datetime\n",
    "banking['account_opened'] = pd.to_datetime(banking['account_opened'],\n",
    "                                           # Infer datetime format\n",
    "                                           infer_datetime_format = True,\n",
    "                                           # Return missing value for error\n",
    "                                           errors = 'coerce') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0a7b14",
   "metadata": {},
   "source": [
    "#### Cross-field validation\n",
    "\n",
    " - Check the values of one column with logical reasoning using other columns' values\n",
    " - Data integrity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d8d53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store fund columns to sum against\n",
    "fund_columns = ['fund_A', 'fund_B', 'fund_C', 'fund_D']\n",
    "\n",
    "# Find rows where fund_columns row sum == inv_amount\n",
    "inv_equ = banking[fund_columns].sum(axis = 1) == banking['inv_amount']\n",
    "\n",
    "# Store consistent and inconsistent data\n",
    "consistent_inv = banking[inv_equ]\n",
    "inconsistent_inv = banking[~inv_equ]\n",
    "\n",
    "# Store consistent and inconsistent data\n",
    "print(\"Number of inconsistent investments: \", inconsistent_inv.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06435b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store today's date and find ages\n",
    "today = dt.date.today()\n",
    "ages_manual = today.year - banking['birth_date'].dt.year\n",
    "\n",
    "# Find rows where age column == ages_manual\n",
    "age_equ = banking['age'] == ages_manual\n",
    "\n",
    "# Store consistent and inconsistent data\n",
    "consistent_ages = banking[age_equ]\n",
    "inconsistent_ages = banking[~age_equ]\n",
    "\n",
    "# Store consistent and inconsistent data\n",
    "print(\"Number of inconsistent ages: \", inconsistent_ages.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741de91a",
   "metadata": {},
   "source": [
    "#### Completeness\n",
    "\n",
    " - Missingness types\n",
    "     - MCAR: missing completely at random\n",
    "         - No systematic relationship between missing data and other values\n",
    "         - Due to entry errors when inputting data\n",
    "     - MAR: missing at random\n",
    "         - Systematic relationship between missing data and other observed values\n",
    "         - Missing \"ozone\" data for \"high temperatures\"\n",
    "     - MNAR: missing not at random\n",
    "         - Systematic relationship between missing data and unobserved values\n",
    "         - Missing \"temperature\" values for \"high temperatures\"\n",
    "         - A customer satisfaction_score column with missing values for highly dissatisfied customers\n",
    "         \n",
    " \n",
    " - Dealing with missing data\n",
    "     - Drop missing data\n",
    "     - Impute using statistical measures (median, mean, mode)\n",
    "     - Impute with an algorithmic approach or a machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3feba59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "banking = pd.read_csv('assets/cleaning_data/banking_dirty.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e1bc6312",
   "metadata": {},
   "outputs": [],
   "source": [
    "import missingno as msno\n",
    "\n",
    "# Print number of missing values in banking\n",
    "print(banking.isna().sum())\n",
    "\n",
    "# Visualize missingness matrix\n",
    "msno.matrix(banking)\n",
    "plt.show()\n",
    "\n",
    "# Isolate missing and non missing values of inv_amount\n",
    "missing_investors = banking[banking['inv_amount'].isna()]\n",
    "investors = banking[~banking['inv_amount'].isna()]\n",
    "\n",
    "# Sort banking by age and visualize\n",
    "banking_sorted = banking.sort_values(by = 'age')\n",
    "msno.matrix(banking_sorted)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d5fab3",
   "metadata": {},
   "source": [
    "#### Minimum edit distance\n",
    "\n",
    "Minimum edit distance is used to identify how similar two strings are. As a reminder, minimum edit distance is the minimum number of steps needed to reach from String A to String B, with the operations available being:\n",
    "\n",
    " - Insertion of a new character\n",
    " - Deletion of an existing character\n",
    " - Substitution of an existing character\n",
    " - Transposition of two existing consecutive characters\n",
    "\n",
    "        What is the minimum edit distance from 'sign' to 'sing', and which operation(s) gets you there? - 1 by transposing 'g' with 'n'.\n",
    "\n",
    "<img src=\"assets/cleaning_data/min_edit_distance.png\" style=\"height: 200px;\"/>\n",
    "<img src=\"assets/cleaning_data/extract.png\" style=\"height: 200px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "818af5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Lets us compare between two strings\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "# Compare reeding vs reading\n",
    "fuzz.WRatio('Reeding', 'Reading')\n",
    "\n",
    "# Output: 86\n",
    "# The WRatio output is not a min distance edit score\n",
    "# 100 means exactly similar, 0 means no similar at all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81ba2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import process from fuzzywuzzy\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "# Store the unique values of cuisine_type in unique_types\n",
    "unique_types = restaurants['cuisine_type'].unique()\n",
    "\n",
    "# Calculate similarity of 'asian' to all values of unique_types\n",
    "print(process.extract('asian', unique_types, limit = len(unique_types)))\n",
    "\n",
    "# Calculate similarity of 'american' to all values of unique_types\n",
    "print(process.extract('american', unique_types, limit = len(unique_types)))\n",
    "\n",
    "# Calculate similarity of 'italian' to all values of unique_types\n",
    "print(process.extract('italian', unique_types, limit = len(unique_types)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cfee8e",
   "metadata": {},
   "source": [
    "Find matches with similarity scores equal to or higher than 80 by using fuzywuzzy.process's extract() function, for each correct cuisine type, and replacing these matches with it.\n",
    "\n",
    "Remember, when comparing a string with an array of strings using process.extract(), the output is a list of tuples where each is formatted like:\n",
    "\n",
    "(closest match, similarity score, index of match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888c1c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of matches, comparing 'italian' with the cuisine_type column\n",
    "matches = process.extract('italian',restaurants['cuisine_type'],limit=len(restaurants))\n",
    "\n",
    "# Inspect the first 5 matches\n",
    "print(matches[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1ce08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of matches, comparing 'italian' with the cuisine_type column\n",
    "matches = process.extract('italian', restaurants['cuisine_type'], limit=len(restaurants.cuisine_type))\n",
    "\n",
    "# Iterate through the list of matches to italian\n",
    "for match in matches:\n",
    "  # Check whether the similarity score is greater than or equal to 80\n",
    "  if match[1] >= 80:\n",
    "    # Select all rows where the cuisine_type is spelled this way, and set them to the correct cuisine\n",
    "    restaurants.loc[restaurants['cuisine_type'] == match[0]] = 'italian'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f02403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through categories\n",
    "for cuisine in categories:  \n",
    "  # Create a list of matches, comparing cuisine with the cuisine_type column\n",
    "  matches = process.extract(cuisine, restaurants['cuisine_type'],\n",
    "                            limit=len(restaurants.cuisine_type))\n",
    "\n",
    "  # Iterate through the list of matches\n",
    "  for match in matches:\n",
    "     # Check whether the similarity score is greater than or equal to 80\n",
    "    if match[1] >= 80:\n",
    "      # If it is, select all rows where the cuisine_type is spelled this way, and set them to the correct cuisine\n",
    "      restaurants.loc[restaurants['cuisine_type'] == match[0]] = cuisine\n",
    "\n",
    "\n",
    "# Inspect the final result\n",
    "print(restaurants['cuisine_type'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59af1dbc",
   "metadata": {},
   "source": [
    "## Missing data\n",
    "\n",
    "Workflow for treating missing values\n",
    "    1. Convert missing values to null values\n",
    "    2. Analyze the amount and type of missingness in the data\n",
    "    3. Appropriately delete or impute missing values\n",
    "    4. Evaluate and compare the performance of the treated/imputed dataset\n",
    "    \n",
    "<img src=\"assets/cleaning_data/null_value.png\" style=\"height: 400px;\"/>\n",
    "\n",
    "Use the missingno package\n",
    "\n",
    "<img src=\"assets/cleaning_data/msno_bar.png\" style=\"height: 300px;\"/>\n",
    "\n",
    "<img src=\"assets/cleaning_data/msno_matrix.png\" style=\"height: 300px;\"/>\n",
    "\n",
    "<img src=\"assets/cleaning_data/msno_dendrogram.png\" style=\"height: 300px;\"/>\n",
    "\n",
    "\n",
    "#### Imputation techniques\n",
    " - With a constant, mean, meadian, mode (most frequent)\n",
    ">  from sklearn.impute import SimpleImputer <br> diabetes_mean = diabetes.copy(deep=True)<br>mean_imputer = SimpleImputer(strategy='mean') (or 'median', 'most_frequent','constant'etc)<br>diabetes_mean.iloc[:, :] = mean_imputer.fit_transform(diabetes_mean)\n",
    "\n",
    " - Fill na (.fillna()) with backward or forward fill\n",
    " \n",
    " - Interpolate (.interpolate()) with 'linear', 'quadratic', 'nearest' as a method\n",
    " \n",
    " - Use ML algorithms to impute missing values\n",
    "     - K-nearest neighbor\n",
    "         - Select K nearest or similar data points using all the non-missing features\n",
    "         - Take average of the selected data points to fill in the misisng feature\n",
    "     - MICE or Multiple Imputation by Chained Equations\n",
    "         - Perform multiple regressions over random sample of the data\n",
    "         - Take average of the multiple regression values\n",
    "         - Impute the missing feature value for the data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a06422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN\n",
    "from fancyimpute import KNN\n",
    "knn_imputer = KNN()\n",
    "diabetes_knn = diabetes.copy(deep=True)\n",
    "diabetes_knn.iloc[:, :] = knn_imputer.fit_transform(diabetes_knn)\n",
    "\n",
    "# MICE\n",
    "from fancyimpute import IterativeImputer\n",
    "MICE_imputer = IterativeImputer()\n",
    "diabetes_MICE = diabetes.copy(deep=True)\n",
    "diabetes_MICE.iloc[:, :] = MICE_imputer.fit_transform(diabetes_MICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48295770",
   "metadata": {},
   "source": [
    "#### Imputing categorical values\n",
    "\n",
    "  - Most categorical values are strings, so to perform any operations you have to convert them\n",
    "  - Encode string to numeric values and impute\n",
    "  \n",
    "  <img src=\"assets/cleaning_data/encoding.png\" style=\"height: 200px;\"/>\n",
    "   \n",
    "**Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52de41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "users = pd.read_csv('userprofile.csv')\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "# Create Ordinal Encoder\n",
    "ambience_ord_enc = OrdinalEncoder()\n",
    "\n",
    "# Select non-null values in ambience\n",
    "ambience = users['ambience']\n",
    "ambience_not_null = ambience[ambience.notnull()]\n",
    "reshaped_vals = ambience_not_null.values.reshape(-1, 1)\n",
    "\n",
    "# Encode the non-null values of ambience\n",
    "encoded_vals = ambience_ord_enc.fit_transform(reshaped_vals)\n",
    "\n",
    "# Replace the ambience column with ordinal values\n",
    "users.loc[ambience.notnull(), 'ambience'] = np.squeeze(encoded_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794213a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary for Ordinal encoders\n",
    "ordinal_enc_dict = {}\n",
    "\n",
    "# Loop over columns to encode\n",
    "for col_name in users:\n",
    "    # Create ordinal encoder for the column\n",
    "    ordinal_enc_dict[col_name] = OrdinalEncoder()\n",
    "    \n",
    "    # Select the nin-null values in the column\n",
    "    col = users[col_name]\n",
    "    col_not_null = col[col.notnull()]\n",
    "    reshaped_vals = col_not_null.values.reshape(-1, 1)\n",
    "    \n",
    "    # Encode the non-null values of the column\n",
    "    encoded_vals = ordinal_enc_dict[col_name].fit_transform(reshaped_vals)\n",
    "    \n",
    "    # Replace the values in the column with ordinal values\n",
    "    users.loc[col.notnull(), col_name] = np.squeeze(encoded_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49201d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_KNN_imputed = users.copy(deep=True)\n",
    "\n",
    "# Create MICE imputer\n",
    "KNN_imputer = KNN()\n",
    "users_KNN_imputed.iloc[:, :] = np.round(KNN_imputer.fit_transform(imputed))\n",
    "\n",
    "for col in imputed:\n",
    "    reshaped_col = imputed[col].values.reshape(-1, 1)\n",
    "    users_KNN_imputed[col] = ordinal_enc[col].inverse_transform(reshaped_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0044dda",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "   - Use density plots (distributions) to check for bias\n",
    "   - R squared of the various machine learning models performance\n",
    "\n",
    "   <img src=\"assets/cleaning_data/density_plots.png\" style=\"height: 320px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfb65fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
