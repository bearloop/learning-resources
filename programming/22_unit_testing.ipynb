{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20cd7d20",
   "metadata": {},
   "source": [
    "## Unit Testing in Python\n",
    "\n",
    "\n",
    "#### Function life cycle\n",
    "\n",
    "   - A function is tested after the first implementation and then any time the function is modified, which happens mainly when new bugs are found, new features are implemented or the code is refactored.\n",
    "\n",
    "<img src=\"assets/unit_testing/life_cycle.png\" style=\"height: 280px;\"/>\n",
    "\n",
    "\n",
    "   - Module structure with unit tests\n",
    "<img src=\"assets/unit_testing/module_structure.png\" style=\"height: 200px;\"/>\n",
    "\n",
    "\n",
    "   - Unit testing libraries:\n",
    "       - pytest\n",
    "       - unittest\n",
    "       - nosetests\n",
    "       - doctests\n",
    "   \n",
    "   \n",
    "   - pytest is one of the most populars\n",
    "   \n",
    "   \n",
    "   - Correct IPython console command to run the tests: !pytest test_convert_to_int.py\n",
    "   \n",
    "   \n",
    "   - An exception is raised when running the unit test. This could be an AssertionError raised by the assert statement or another exception, e.g. NameError, which is raised before the assert statement can run.\n",
    "   \n",
    "   - Unit tests:\n",
    "       - Small independent piece of code, a function or class\n",
    "       - Benefits\n",
    "           - Time savings, leading to faster development of new features.\n",
    "           - Improved documentation, which will help new colleagues understand the code base better.\n",
    "           - More user trust in the software product.\n",
    "           - Better user experience due to reduced downtime.\n",
    "   - Integration tests:\n",
    "       - Check if multiple units work well together when they are connected, not just independently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80a1d998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pytest package\n",
    "import pytest\n",
    "\n",
    "def convert_to_int(string_with_comma):\n",
    "    # Fix this line so that it returns an int, not a str\n",
    "    return int(string_with_comma.replace(\",\", \"\"))\n",
    "\n",
    "# Complete the unit test name by adding a prefix\n",
    "def test_convert_to_int():\n",
    "  # Complete the assert statement\n",
    "  assert convert_to_int(\"2,081\") == 2081\n",
    "\n",
    "test_convert_to_int()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dc5f13",
   "metadata": {},
   "source": [
    "#### Print a message if assert does not pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea8d8f75",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "convert_to_int('2,081') should return the int 2081, but it actually returned 2082",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-9307545c22ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mexpected\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mactual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtest_on_string_with_one_comma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-9307545c22ae>\u001b[0m in \u001b[0;36mtest_on_string_with_one_comma\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"convert_to_int('2,081') should return the int 2081, but it actually returned {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactual\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Write the assert statement which prints message on failure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mexpected\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mactual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtest_on_string_with_one_comma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: convert_to_int('2,081') should return the int 2081, but it actually returned 2082"
     ]
    }
   ],
   "source": [
    "def test_on_string_with_one_comma():\n",
    "    test_argument = \"2,082\"\n",
    "    expected = 2081\n",
    "    actual = convert_to_int(test_argument)\n",
    "    # Format the string with the actual return value\n",
    "    message = \"convert_to_int('2,081') should return the int 2081, but it actually returned {}\".format(actual)\n",
    "    # Write the assert statement which prints message on failure\n",
    "    assert expected == actual, message\n",
    "\n",
    "test_on_string_with_one_comma()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364c8d26",
   "metadata": {},
   "source": [
    "#### Testing exceptions\n",
    "\n",
    "<img src=\"assets/unit_testing/testing_exceptions.png\" style=\"height: 200px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e63120ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytest raised an exception because no OSError was raised in the context.\n"
     ]
    }
   ],
   "source": [
    "import pytest\n",
    "\n",
    "try:\n",
    "    # Fill in with a context manager that raises Failed if no OSError is raised\n",
    "    with pytest.raises(OSError):\n",
    "        raise ValueError\n",
    "except:\n",
    "    print(\"pytest raised an exception because no OSError was raised in the context.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "967eb2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pytest.raises(ValueError) as exc_info:\n",
    "    raise ValueError(\"Silence me!\")\n",
    "    \n",
    "# Check if the raised ValueError contains the correct message\n",
    "assert exc_info.match(\"Silence me!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc708b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pytest\n",
    "from train import split_into_training_and_testing_sets\n",
    "\n",
    "def test_on_one_row():\n",
    "    test_argument = np.array([[1382.0, 390167.0]])\n",
    "    \n",
    "    # Store information about raised ValueError in exc_info\n",
    "    with pytest.raises(ValueError) as exc_info:\n",
    "        split_into_training_and_testing_sets(test_argument)\n",
    "        \n",
    "    expected_error_msg = \"Argument data_array must have at least 2 rows, it actually has just 1\"\n",
    "    \n",
    "    # Check if the raised ValueError contains the correct message\n",
    "    assert exc_info.match(expected_error_msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414eb950",
   "metadata": {},
   "source": [
    "#### Unit testing with boundary values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a125059d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "from preprocessing_helpers import row_to_list\n",
    "\n",
    "def test_on_no_tab_no_missing_value():    # (0, 0) boundary value\n",
    "    # Assign actual to the return value for the argument \"123\\n\"\n",
    "    actual = row_to_list(\"123\\n\")\n",
    "    assert actual is None, \"Expected: None, Actual: {0}\".format(actual)\n",
    "    \n",
    "def test_on_two_tabs_no_missing_value():    # (2, 0) boundary value\n",
    "    actual = row_to_list(\"123\\t4,567\\t89\\n\")\n",
    "    # Complete the assert statement\n",
    "    assert actual is None, \"Expected: None, Actual: {0}\".format(actual)\n",
    "    \n",
    "def test_on_one_tab_with_missing_value():    # (1, 1) boundary value\n",
    "    actual = row_to_list(\"\\t4,567\\n\")\n",
    "    # Format the failure message\n",
    "    assert actual is None, \"Expected: None, Actual: {0}\".format(actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8fe611",
   "metadata": {},
   "source": [
    "#### Unit testing with values triggering special logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a44feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "from preprocessing_helpers import row_to_list\n",
    "\n",
    "def test_on_no_tab_with_missing_value():    # (0, 1) case\n",
    "    # Assign to the actual return value for the argument \"\\n\"\n",
    "    actual = row_to_list(\"\\n\")\n",
    "    # Write the assert statement with a failure message\n",
    "    assert actual is None,\"Expected: None, Actual: {0}\".format(actual)\n",
    "    \n",
    "def test_on_two_tabs_with_missing_value():    # (2, 1) case\n",
    "    # Assign to the actual return value for the argument \"123\\t\\t89\\n\"\n",
    "    actual = row_to_list(\"123\\t\\t89\\n\")\n",
    "    # Write the assert statement with a failure message\n",
    "    assert actual is None, \"Expected: None, Actual: {0}\".format(actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844b121a",
   "metadata": {},
   "source": [
    "#### Unit testing with normal arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92af984",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "from preprocessing_helpers import row_to_list\n",
    "\n",
    "def test_on_normal_argument_1():\n",
    "    actual = row_to_list(\"123\\t4,567\\n\")\n",
    "    # Fill in with the expected return value for the argument \"123\\t4,567\\n\"\n",
    "    expected = [\"123\", \"4,567\"]\n",
    "    assert actual == expected, \"Expected: {0}, Actual: {1}\".format(expected, actual)\n",
    "    \n",
    "def test_on_normal_argument_2():\n",
    "    actual = row_to_list(\"1,059\\t186,606\\n\")\n",
    "    expected = [\"1,059\", \"186,606\"]\n",
    "    # Write the assert statement along with a failure message\n",
    "    assert actual == expected, \"Expected: {0}, Actual: {1}\".format(expected, actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2537942f",
   "metadata": {},
   "source": [
    "#### Test driven development (TDD)\n",
    "    \n",
    "  - In TDD, you write the tests first and implement the function later\n",
    "  - Write unit tests before implementation\n",
    "\n",
    "<img src=\"assets/unit_testing/tdd.png\" style=\"height: 400px;\"/>\n",
    "\n",
    "In TDD, you write the tests first and implement the function later.\n",
    "\n",
    "Normal arguments for convert_to_int() are integer strings with comma as thousand separators. Since the best practice is to test a function for two to three normal arguments, here are three examples with no comma, one comma and two commas respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "233ebb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_with_no_comma():\n",
    "    actual = convert_to_int(\"756\")\n",
    "    # Complete the assert statement\n",
    "    assert actual == 756, \"Expected: 756, Actual: {0}\".format(actual)\n",
    "    \n",
    "def test_with_one_comma():\n",
    "    actual = convert_to_int(\"2,081\")\n",
    "    # Complete the assert statement\n",
    "    assert actual == 2081, \"Expected: 2081, Actual: {0}\".format(actual)\n",
    "    \n",
    "def test_with_two_commas():\n",
    "    actual = convert_to_int(\"1,034,891\")\n",
    "    # Complete the assert statement\n",
    "    assert actual == 1034891, \"Expected: 1034891, Actual: {0}\".format(actual)\n",
    "\n",
    "test_with_no_comma()\n",
    "test_with_one_comma()\n",
    "test_with_two_commas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7376f3d3",
   "metadata": {},
   "source": [
    "What should convert_to_int() do if the arguments are not normal? In particular, there are three special argument types:\n",
    "\n",
    "Arguments that are missing a comma e.g. \"178100,301\".<br>\n",
    "Arguments that have the comma in the wrong place e.g. \"12,72,891\".<br>\n",
    "Float valued strings e.g. \"23,816.92\".<br>\n",
    "Also, should convert_to_int() raise an exception for specific argument values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "677cfa13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give a name to the test for an argument with missing comma\n",
    "def test_on_string_with_missing_comma():\n",
    "    actual = convert_to_int(\"178100,301\")\n",
    "    assert actual is None, \"Expected: None, Actual: {0}\".format(actual)\n",
    "test_on_string_with_missing_comma()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db5deadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_on_string_with_incorrectly_placed_comma():\n",
    "    # Assign to the actual return value for the argument \"12,72,891\"\n",
    "    actual = convert_to_int(\"12,72,891\")\n",
    "    assert actual is None, \"Expected: None, Actual: {0}\".format(actual)\n",
    "test_on_string_with_incorrectly_placed_comma()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "337a8c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_on_float_valued_string():\n",
    "    actual = convert_to_int(\"23,816.92\")\n",
    "    # Complete the assert statement\n",
    "    assert actual is None, \"Expected: None, Actual: {0}\".format(actual)\n",
    "test_on_float_valued_string()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68091005",
   "metadata": {},
   "source": [
    "Implement the function\n",
    "\n",
    "convert_to_int() returns None for the following:\n",
    "\n",
    "Arguments with missing thousands comma e.g. \"178100,301\". If you split the string at the comma using \"178100,301\".split(\",\"), then the resulting list [\"178100\", \"301\"] will have at least one entry with length greater than 3 e.g. \"178100\".\n",
    "\n",
    "Arguments with incorrectly placed comma e.g. \"12,72,891\". If you split this at the comma, then the resulting list is [\"12\", \"72\", \"891\"]. Note that the first entry is allowed to have any length between 1 and 3. But if any other entry has a length other than 3, like \"72\", then there's an incorrectly placed comma.\n",
    "\n",
    "Float valued strings e.g. \"23,816.92\". If you remove the commas and call int() on this string i.e. int(\"23816.92\"), you will get a ValueError.\n",
    "\n",
    "Re-run the tests above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "095348da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_int(integer_string_with_commas):\n",
    "    comma_separated_parts = integer_string_with_commas.split(\",\")\n",
    "    for i in range(len(comma_separated_parts)):\n",
    "        # Write an if statement for checking missing commas\n",
    "        if len(comma_separated_parts[i]) > 3:\n",
    "            return None\n",
    "        # Write the if statement for incorrectly placed commas\n",
    "        if i != 0 and len(comma_separated_parts[i]) != 3:\n",
    "            return None\n",
    "    integer_string_without_commas = \"\".join(comma_separated_parts)\n",
    "    try:\n",
    "        return int(integer_string_without_commas)\n",
    "    # Fill in with a ValueError\n",
    "    except ValueError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08152661",
   "metadata": {},
   "source": [
    "#### Code structure with unit tests\n",
    "\n",
    "<img src=\"assets/unit_testing/code_structure.png\" style=\"height: 400px;\"/>\n",
    "\n",
    "<img src=\"assets/unit_testing/test_classes.png\" style=\"height: 400px;\"/>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbc95f6",
   "metadata": {},
   "source": [
    "Test classes are containers inside test modules. They help separate tests for different functions within the test module, and serve as a structuring tool in the pytest framework.\n",
    "\n",
    "Test classes are written in CamelCase e.g. TestMyFunction as opposed to tests, which are written using underscores e.g. test_something()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "799680ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import numpy as np\n",
    "\n",
    "from models.train import split_into_training_and_testing_sets\n",
    "\n",
    "# Declare the test class\n",
    "class TestSplitIntoTrainingAndTestingSets(object):\n",
    "    \n",
    "    # Fill in with the correct mandatory argument\n",
    "    def test_on_one_row(self):\n",
    "        test\n",
    "        _argument = np.array([[1382.0, 390167.0]])\n",
    "        with pytest.raises(ValueError) as exc_info:\n",
    "            split_into_training_and_testing_sets(test_argument)\n",
    "            \n",
    "        expected_error_msg = \"Argument data_array must have at least 2 rows, it actually has just 1\"\n",
    "        \n",
    "        assert exc_info.match(expected_error_msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0431d3e",
   "metadata": {},
   "source": [
    "#### Running the tests\n",
    "<img src=\"assets/unit_testing/run_tests_folder.png\" style=\"height: 400px;\"/>\n",
    "\n",
    "<img src=\"assets/unit_testing/run_tests.png\" style=\"height: 320px;\"/>\n",
    "\n",
    "What is the correct command to run all the tests in this test class using node IDs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e9d436",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest models/test_train.py::TestSplitIntoTrainingAndTestingSets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbb5f8f",
   "metadata": {},
   "source": [
    "#### Expected failures and conditional skipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ec8f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mark the whole test class as \"expected to fail\"\n",
    "@pytest.mark.xfail\n",
    "class TestModelTest(object):\n",
    "    def test_on_linear_data(self):\n",
    "        test_input = np.array([[1.0, 3.0], [2.0, 5.0], [3.0, 7.0]])\n",
    "        expected = 1.0\n",
    "        actual = model_test(test_input, 2.0, 1.0)\n",
    "        message = \"model_test({0}) should return {1}, but it actually returned {2}\".format(test_input, expected, actual)\n",
    "        assert actual == pytest.approx(expected), message\n",
    "        \n",
    "    def test_on_one_dimensional_array(self):\n",
    "        test_input = np.array([1.0, 2.0, 3.0, 4.0])\n",
    "        with pytest.raises(ValueError) as exc_info:\n",
    "            model_test(test_input, 1.0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e5709e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mark the whole test class as \"expected to fail\"\n",
    "\n",
    "# Add a reason for the expected failure\n",
    "@pytest.mark.xfail(reason=\"Using TDD, model_test() has not yet been implemented\")\n",
    "class TestModelTest(object):\n",
    "    def test_on_linear_data(self):\n",
    "        test_input = np.array([[1.0, 3.0], [2.0, 5.0], [3.0, 7.0]])\n",
    "        expected = 1.0\n",
    "        actual = model_test(test_input, 2.0, 1.0)\n",
    "        message = \"model_test({0}) should return {1}, but it actually returned {2}\".format(test_input, expected, actual)\n",
    "        assert actual == pytest.approx(expected), message\n",
    "        \n",
    "    def test_on_one_dimensional_array(self):\n",
    "        test_input = np.array([1.0, 2.0, 3.0, 4.0])\n",
    "        with pytest.raises(ValueError) as exc_info:\n",
    "            model_test(test_input, 1.0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66912401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the sys module\n",
    "import sys\n",
    "\n",
    "class TestGetDataAsNumpyArray(object):\n",
    "    # Add a reason for skipping the test\n",
    "    @pytest.mark.skipif(sys.version_info > (2, 7), reason=\"Works only on Python 2.7 or lower\")\n",
    "    def test_on_clean_file(self):\n",
    "        expected = np.array([[2081.0, 314942.0],\n",
    "                             [1059.0, 186606.0],\n",
    "                             [1148.0, 206186.0]\n",
    "                             ]\n",
    "                            )\n",
    "        actual = get_data_as_numpy_array(\"example_clean_data.txt\", num_columns=2)\n",
    "        message = \"Expected return value: {0}, Actual return value: {1}\".format(expected, actual)\n",
    "        assert actual == pytest.approx(expected), message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa75a9ca",
   "metadata": {},
   "source": [
    "Teardown: when you have to clean up any modifications to the environment and restore it to its initial state\n",
    "\n",
    "Setup -> Assert -> Teardown\n",
    "\n",
    "The setup and teardown is placed outside the test in a function called fixture. This is a function with the pytest.fixture decorator. It uses the yield argument instead of the return argument.\n",
    "\n",
    "<img src=\"assets/unit_testing/fixture.png\" style=\"height: 320px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc31ab3",
   "metadata": {},
   "source": [
    "Use a fixture for a clean data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf0a498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a decorator to make this function a fixture\n",
    "@pytest.fixture\n",
    "def clean_data_file():\n",
    "    file_path = \"clean_data_file.txt\"\n",
    "    with open(file_path, \"w\") as f:\n",
    "        f.write(\"201\\t305671\\n7892\\t298140\\n501\\t738293\\n\")\n",
    "    yield file_path\n",
    "    os.remove(file_path)\n",
    "    \n",
    "# Pass the correct argument so that the test can use the fixture\n",
    "def test_on_clean_file(clean_data_file):\n",
    "    expected = np.array([[201.0, 305671.0], [7892.0, 298140.0], [501.0, 738293.0]])\n",
    "    # Pass the clean data file path yielded by the fixture as the first argument\n",
    "    actual = get_data_as_numpy_array(clean_data_file, 2)\n",
    "    assert actual == pytest.approx(expected), \"Expected: {0}, Actual: {1}\".format(expected, actual) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4221a6",
   "metadata": {},
   "source": [
    "Write a fixture for an empty data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1d70ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.fixture\n",
    "def empty_file():\n",
    "    # Assign the file path \"empty.txt\" to the variable\n",
    "    file_path = \"empty.txt\"\n",
    "    open(file_path, \"w\").close()\n",
    "    # Yield the variable file_path\n",
    "    yield file_path\n",
    "    # Remove the file in the teardown\n",
    "    os.remove(file_path)\n",
    "    \n",
    "def test_on_empty_file(self, empty_file):\n",
    "    expected = np.empty((0, 2))\n",
    "    actual = get_data_as_numpy_array(empty_file, 2)\n",
    "    assert actual == pytest.approx(expected), \"Expected: {0}, Actual: {1}\".format(expected, actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc2d8e7",
   "metadata": {},
   "source": [
    "Fixture chaining using tmpdir\n",
    "\n",
    "The built-in tmpdir fixture is very useful when dealing with files in setup and teardown. tmpdir combines seamlessly with user defined fixture via fixture chaining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246f15a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "@pytest.fixture\n",
    "# Add the correct argument so that this fixture can chain with the tmpdir fixture\n",
    "def empty_file(tmpdir):\n",
    "    # Use the appropriate method to create an empty file in the temporary directory\n",
    "    file_path = tmpdir.join(\"empty.txt\")\n",
    "    open(file_path, \"w\").close()\n",
    "    yield file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a497ef9b",
   "metadata": {},
   "source": [
    "In what order will the setup and teardown of empty_file() and tmpdir be executed?\n",
    "\n",
    "setup of tmpdir \n",
    "→\n",
    " setup of empty_file() \n",
    "→\n",
    " teardown of empty_file() \n",
    "→\n",
    " teardown of tmpdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b254d924",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
