{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34b20ea3",
   "metadata": {},
   "source": [
    "### Big data\n",
    "\n",
    " - Complex data sets for traditional computing resources\n",
    " - Data sets that a distributed clustered of computers might be needed to analyse\n",
    " - 3v's:\n",
    "     - Volume, the size of the data\n",
    "     - Variety, the various sources and formats\n",
    "     - Veolocity, the frequency (speed) the data are coming in or get updated\n",
    " \n",
    " - Clustered computing: collection of resources of multiple machine\n",
    " - Parallel computing: simultaneous computation\n",
    " - Distributed computing: collection of nodes (networked computers) that run in parallel\n",
    " - Batch processing: Breaking the job into smaller pieces and running them on individual machines\n",
    " - Real-time processing: immediate processing of data\n",
    "\n",
    "--------------\n",
    "**Apache Spark**\n",
    " - General purpose and lighting fast cluster computing system that is open sourced and can handle both batch and real-time data processing (by comparison Apache Hadoop/MapReduce only handles batch processing)\n",
    " - Spark distributes data and computation across multiple computers in order to execute complex (multi-stage) applications such as machine learning.\n",
    " - Spark runs most computations in-memory for better performance\n",
    " - Written in Scala but supports multiple languages\n",
    " - At the center of Spark's ecosystem is Spark Core (RDD API) which contains the basic functionality of Spark\n",
    " - On top of Spark Core there are libraries:\n",
    "     - Spark SQL: for processing structured and semi-structured data\n",
    "     - MLlib: includes common machine learning algorithms\n",
    "     - GraphX: collection of algorithms and tools for manipulating graphs and performing parallel graph computations\n",
    "     - Spark Streaming: a scalable, high-throughput processing library for real-time data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1589a64",
   "metadata": {},
   "source": [
    "--------------\n",
    "**Spark modes**\n",
    "\n",
    " - Local mode: runs on a single machine (like a laptop) / convenient for testing, debugging and demonstration purposes\n",
    " - Cluster mode: runs on a cluster of computers / used for production\n",
    "\n",
    "**Spark shells**\n",
    "\n",
    " - Spark comes with interactive shells that enable ad-hoc analysis\n",
    " - Spark shell is an interactive environment through which one can access Spark's functionality quickly and conveniently\n",
    " - Spark's shells allow interacting with data that is on disk or in memory across many machines. Spark takes care of automatically distributing this processing\n",
    " - Three different Spark shells: Spark-shell for Scala, SparkR for R and PySpark-shell for Python\n",
    "--------------\n",
    "**Spark in Python**\n",
    "\n",
    " - PySpark is a library created for running Spark in Python\n",
    " - PySpark shell is the Python-based command line tool to develop Spark's apps in Python\n",
    " - To use the Spark shell you need an entry point. An entry point is where control is transferred from the operating system to the provided program. This is the SparkContext. You can access the SparkContext in the PySpark shell as a variable named sc.\n",
    " - SparkContext attributes\n",
    "     - Use sc.version to retrieve the SparkContext version\n",
    "     - Use sc.pythonVer to retrieve the Python version of SparkContext\n",
    "     - Use sc.master to retrieve the URL of the cluster or \"local\" string if it runs in local mode\n",
    " - You an load raw data to PySpark using SparkContext's:\n",
    "     - sc.parallelize method\n",
    "     > rdd = sc.parallelize([1,2,3,4,5,6,7,8,9,10])\n",
    "     - sc.textFile() method\n",
    "     > rdd = sc.textFile(\"test.txt\")\n",
    " - A SparkContext represents the entry point to Spark functionality. It's like a key to your car. When we run any Spark application, a driver program starts, which has the main function and your SparkContext gets initiated here. PySpark automatically creates a SparkContext for you in the PySpark shell (so you don't have to create it by yourself) and is exposed via a variable sc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78a0715",
   "metadata": {},
   "source": [
    "--------------\n",
    "**RDD**\n",
    " \n",
    " - An immutable distributed collection of objects\n",
    " - Stands for resilient and distributed datasets\n",
    "     - Resilient: ability to withstand failures and recompute missing or damaged paritions\n",
    "     - Distributed: spanning the jobs across multiple nodes in the cluster for efficient computation\n",
    "     - Datasets: collection of partitioned data (e.g. arrays, tables, tuples or other objects)\n",
    " - Backbone data type in PySpark\n",
    " - When Spark starts processing data, it divides it into partitions and distributes it across cluster nodes, with each node containing a slice of data\n",
    " - 3 different methods for creating RDDs\n",
    "     - via sc.parallelize([list])\n",
    "     - from external datasets using sc.textFile('file.type')\n",
    "     - from existing RDDs\n",
    " - A partition in Spark is the division of a dataset into parts with each part being stored in multiple locations across the cluster. Spark (by default) partitions the data at the time of an RDD's creation based on several factors such as available resources, external datasets etc. This can be controlled by passing a minPartitions arguments that defines the minimum number of partitions to be created by an RDD.\n",
    " - To check the number of RDD partitions use \n",
    " >rdd_name.getNumPartitions()\n",
    "\n",
    "--------------\n",
    "**RDD operations in PySpark**\n",
    " \n",
    " - Two types of Spark operations are supported:\n",
    "     - Transformations\n",
    "     - Actions\n",
    " - Transformations are operations on RDDs that return a new RDD\n",
    " - Actions perform some computations on the RDD\n",
    " - The most important feature of Spark that helps RDDs in fault tolerance and optimising resource use is the lazy evaluation\n",
    " - Transformations follow lazy evaluation\n",
    " - Lazy evaluation:\n",
    "     - Spark creates a graph from all operations you perform on an RDD and execution of the graph starts only when an action is performed on RDD\n",
    "     \n",
    "<img src=\"assets/spark/lazy_eval.png\" style=\"width: 600px;\"/>\n",
    "\n",
    " - Basic RDD transformations\n",
    " > map(), filter(), flatMap(), union()\n",
    " - Basic RDD actions\n",
    " > collect(), take(N), first(), count()\n",
    " \n",
    "**Examples**\n",
    "#### Create map() transformation to cube numbers\n",
    "cubedRDD = numbRDD.map(lambda x: x**3)\n",
    "\n",
    "#### Collect the results\n",
    "numbers_all = cubedRDD.collect()\n",
    "\n",
    "#### Print the numbers from numbers_all\n",
    "for numb in numbers_all:\n",
    "\tprint(numb)\n",
    "\n",
    "#### Filter the fileRDD to select lines with Spark keyword\n",
    "fileRDD_filter = fileRDD.filter(lambda line: 'Spark' in line)\n",
    "\n",
    "#### How many lines are there in fileRDD?\n",
    "print(\"The total number of lines with the keyword Spark is\", fileRDD_filter.count())\n",
    "\n",
    "#### Print the first four lines of fileRDD\n",
    "for line in fileRDD_filter.take(4): \n",
    "  print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb9e96f",
   "metadata": {},
   "source": [
    "--------------\n",
    "**Pair RDDs**\n",
    " \n",
    " - A special data structure to work with datasets that are key/value pairs\n",
    " - Each row is a key and maps to one or more values\n",
    " - The key refers to the identifier whereas value refers to the data\n",
    " - Two ways to create pair RDDs:\n",
    "     - From a list of key/value tuple\n",
    "     > my_tuple = [('Sam', 23), ('Mary', 34), ('Peter', 25)]<br>\n",
    "     > pairRDD_tuple = sc.parallelize(my_tuple)\n",
    "     - From a regular RDD\n",
    "     > my_list = ['Sam 23', 'Mary 34', 'Peter 25']<br>\n",
    "     > regularRDD = sc.parallelize(my_list)<br>\n",
    "     > pairRDD_RDD = regularRDD.map(lambda s: (s.split(' ')[0], s.split(' ')[1]))\n",
    " - Operations available on RDDs are still available on pair RDDs but there are some special operations too\n",
    " - Since pair RDDs contain tuples for transformations to work we have to pass functions that operate on key/value pairs\n",
    "     - reduceByKey(func): combine values with the same key by running parallel operations for each key in the dataset\n",
    "     > regularRDD = sc.parallelize([(\"Messi\", 23), (\"Ronaldo\", 34), (\"Neymar\", 22), (\"Messi\", 24)])<br>\n",
    "     > pairRDD_reducebykey = regularRDD.reduceByKey(lambda x,y : x + y)<br>\n",
    "     > pairRDD_reducebykey.collect()<br>\n",
    "     > [('Neymar', 22), ('Ronaldo', 34), ('Messi', 47)]\n",
    "     - groupByKey(): group values with the same key\n",
    "     > airports = [(\"US\", \"JFK\"),(\"UK\", \"LHR\"),(\"FR\", \"CDG\"),(\"US\", \"SFO\")]<br>\n",
    "     > regularRDD = sc.parallelize(airports)<br>\n",
    "     > pairRDD_group = regularRDD.groupByKey().collect()<br>\n",
    "     > for cont, air in pairRDD_group: print(cont, list(air))<br>\n",
    "     > FR ['CDG']<br>\n",
    "     > US ['JFK', 'SFO']<br>\n",
    "     > UK ['LHR']\n",
    " \n",
    "     - sortByKey(): return an RDD sorted by the key\n",
    "     > pairRDD_reducebykey_rev = pairRDD_reducebykey.map(lambda x: (x[1], x[0]))<br>\n",
    "     > pairRDD_reducebykey_rev.sortByKey(ascending=False).collect()<br>\n",
    "     > [(47, 'Messi'), (34, 'Ronaldo'), (22, 'Neymar')]\n",
    " \n",
    "     - join(): join two pair RDDs based on their key\n",
    "     > RDD1 = sc.parallelize([(\"Messi\", 34),(\"Ronaldo\", 32),(\"Neymar\", 24)])<br>\n",
    "     > RDD2 = sc.parallelize([(\"Ronaldo\", 80),(\"Neymar\", 120),(\"Messi\", 100)])<br>\n",
    "     > RDD1.join(RDD2).collect()<br>\n",
    "     > [('Neymar', (24, 120)), ('Ronaldo', (32, 80)), ('Messi', (34, 100))]\n",
    " \n",
    " \n",
    " --------------\n",
    "**Advanced Actions on RDDs**\n",
    "\n",
    " - reduce(func): action for aggregating a regular RDD's elements\n",
    "     - the function should be commutative (i.e. changing the order of the operands does not change the results) and associative\n",
    " - saveAsTextFile(\"tempFile\"): action for saving RDDs into a text file inside a directory with each partition as a separate file\n",
    "     - coalesce(), a method to use for saving an RDD as a single text file\n",
    "     - RDD.coalesce(1).saveAsTextFile(\"tempFile\")\n",
    " - Action operations on pair RDDs\n",
    "     - countByKey(): counts the number of elements for each key\n",
    "     > rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])<br>\n",
    "     > for kee, val in rdd.countByKey().items(): print(kee, val)<br>\n",
    "     > ('a', 2) ('b', 1)\n",
    "   \n",
    "     - collectAsMap(): returns the key/value pairs in the RDD as a dict\n",
    "\n",
    "**Examples**\n",
    "\n",
    "**Count the unique keys**<br>\n",
    "total = Rdd.countByKey()\n",
    "\n",
    "**What is the type of total?**<br>\n",
    "print(\"The type of total is\", type(total))\n",
    "\n",
    "**Iterate over the total and print the output**<br>\n",
    "for k, v in total.items(): <br>\n",
    "  print(\"key\", k, \"has\", v, \"counts\")\n",
    "\n",
    "**Create a baseRDD from the file path**<br>\n",
    "baseRDD = sc.textFile(file_path)\n",
    "\n",
    "**Split the lines of baseRDD into words**<br>\n",
    "splitRDD = baseRDD.flatMap(lambda x: x.split())\n",
    "\n",
    "**Count the total number of words**<br>\n",
    "print(\"Total number of words in splitRDD:\", splitRDD.count())\n",
    "\n",
    "**Convert the words in lower case and remove stop words from the stop_words curated list**<br>\n",
    "splitRDD_no_stop = splitRDD.filter(lambda x: x.lower() not in stop_words)\n",
    "\n",
    "**Create a tuple of the word and 1**<br>\n",
    "splitRDD_no_stop_words = splitRDD_no_stop.map(lambda w: (w, 1))\n",
    "\n",
    "**Count of the number of occurences of each word**<br>\n",
    "resultRDD = splitRDD_no_stop_words.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "\n",
    "**Display the first 10 words and their frequencies from the input RDD**<br>\n",
    "for word in resultRDD.take(10):<br>\n",
    "\tprint(word)\n",
    "\n",
    "**Swap the keys and values from the input RDD**<br>\n",
    "resultRDD_swap = resultRDD.map(lambda x: (x[1], x[0]))\n",
    "\n",
    "**Sort the keys in descending order**<br>\n",
    "resultRDD_swap_sort = resultRDD_swap.sortByKey(ascending=False)\n",
    "\n",
    "**Show the top 10 most frequent words and their frequencies from the sorted RDD**<br>\n",
    "for word in resultRDD_swap_sort.take(10):<br>\n",
    "\tprint(\"{},{}\". format(word[1], word[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bf3189",
   "metadata": {},
   "source": [
    "--------------\n",
    "**PySpark DataFrames**\n",
    " \n",
    " - PySpark SQL is Spark's high level API for working with structured data\n",
    " - Provides a programming abstraction called DataFrames\n",
    " - DFs are immutable distributed collections of data with named columns\n",
    " - Are designed for processing both structured (e.g. relational databases) and semi-structured (e.g. JSON) data\n",
    " - Support both SQL statements and direct expressions\n",
    " - The SparkSession gives a single entry poiny to interact with Spark DataFrames (similarly to what the SparkContext does for RDDs)\n",
    " - It can create DFs, register DFs and execute SQL queries\n",
    " - Available through the PySpark SQL\n",
    " - Two methods of creating DataFrames in PySpark\n",
    "     - From existing RDDs using SparkSession's createDataFrame() method\n",
    "     - From various data sources using SparkSession's read() method \n",
    " - DF Schema provides info about the column name, data type in the column, empty values etc. If the schema is a list of column names the data type of each column will be inferred from the RDD's data. Use printSchema() to retrieve the DF's schema\n",
    "\n",
    "**Create an RDD from the list**<br>\n",
    "rdd = sc.parallelize(sample_list)\n",
    "\n",
    "**Create a PySpark DataFrame**<br>\n",
    "names_df = spark.createDataFrame(rdd, schema=['Name', 'Age'])\n",
    "\n",
    "**Check the type of names_df**<br>\n",
    "print(\"The type of names_df is\", type(names_df))\n",
    "\n",
    "**Create an DataFrame from file_path**<br>\n",
    "people_df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "**Check the type of people_df**<br>\n",
    "print(\"The type of people_df is\", type(people_df))\n",
    "\n",
    "\n",
    "--------------\n",
    "**Common DF transformations**\n",
    " - select(), filter(), groupby(), orderby(), dropDuplicates(), withColumnRenamed()\n",
    "\n",
    "**Common DF actions**\n",
    " - head(), show(), count(), columns, describe()\n",
    "\n",
    "--------------\n",
    "**SQL queries executions**\n",
    "\n",
    " - The SparkSession's sql() method can be used to execute SQL statements (returns a DF) in Spark\n",
    " - SQL queries cannot be run directly against a DataFrame. To do so use the df.createOrReplaceTempView(\"table1\") function to create a temp table which can be used to run SQL queries against\n",
    " > df.createOrReplaceTempView(\"table1\")<br>\n",
    " > df2 = spark.sql(\"SELECT field1, field2 FROM table1\")<br>\n",
    " > df2.collect()\n",
    "\n",
    "**Create a temporary table \"people\"**<br>\n",
    "people_df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "**Construct a query to select the names of the people from the temporary table \"people\"**<br>\n",
    "query = '''SELECT name FROM people'''\n",
    "\n",
    "**Assign the result of Spark's query to people_df_names**<br>\n",
    "people_df_names = spark.sql(query)\n",
    "\n",
    "**Print the top 10 names of the people**<br>\n",
    "people_df_names.show(10)\n",
    "\n",
    "**Filter the people table to select female sex**<br>\n",
    "people_female_df = spark.sql('SELECT * FROM people WHERE sex==\"female\"')\n",
    "\n",
    "**Filter the people table DataFrame to select male sex**<br>\n",
    "people_male_df = spark.sql('SELECT * FROM people WHERE sex==\"male\"')\n",
    "\n",
    "**Count the number of rows in both DataFrames**<br>\n",
    "print(\"There are {} rows in the people_female_df and {} rows in the people_male_df DataFrames\".format(people_female_df.count(), people_male_df.count()))\n",
    "\n",
    "--------------\n",
    "**Visualisations**\n",
    "\n",
    "Graphical representations or visualization of data is imperative for understanding as well as interpreting the data. Convert the names_df to Pandas DataFrame and plot the contents as horizontal bar plot with names of the people on the x-axis and their age on the y-axis.\n",
    "\n",
    "**Check the column names of names_df**<br>\n",
    "print(\"The column names of names_df are\", names_df.columns)\n",
    "\n",
    "**Convert to Pandas DataFrame**<br>\n",
    "df_pandas = names_df.toPandas()\n",
    "\n",
    "**Create a horizontal bar plot**<br>\n",
    "df_pandas.plot(kind='barh', x='Name', y='Age', colormap='winter_r')\n",
    "plt.show()\n",
    "\n",
    "--------------\n",
    "**Load the Dataframe**<br>\n",
    "fifa_df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "**Check the schema of columns**<br>\n",
    "fifa_df.printSchema()\n",
    "\n",
    "**Show the first 10 observations**<br>\n",
    "fifa_df.show(10)\n",
    "\n",
    "**Print the total number of rows**<br>\n",
    "print(\"There are {} rows in the fifa_df DataFrame\".format(fifa_df.count()))\n",
    "\n",
    "**Create a temporary view of fifa_df**<br>\n",
    "fifa_df.createOrReplaceTempView('fifa_df_table')\n",
    "\n",
    "**Construct the \"query\"**<br>\n",
    "query = '''SELECT Age FROM fifa_df_table WHERE Nationality == \"Germany\"'''\n",
    "\n",
    "**Apply the SQL \"query\"**<br>\n",
    "fifa_df_germany_age = spark.sql(query)\n",
    "\n",
    "**Generate basic statistics**<br>\n",
    "fifa_df_germany_age.describe().show()\n",
    "\n",
    "**Convert fifa_df to fifa_df_germany_age_pandas DataFrame**<br>\n",
    "fifa_df_germany_age_pandas = fifa_df_germany_age.toPandas()\n",
    "\n",
    "**Plot the 'Age' density of Germany Players**<br>\n",
    "fifa_df_germany_age_pandas.plot(kind='density')<br>\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019e9819",
   "metadata": {},
   "source": [
    "## Building data pipelines with Spark\n",
    "\n",
    " - Use Spark for data processing at scale\n",
    " - Interactive analytics\n",
    " - Machine learning\n",
    " - Do not use when you have only little data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1148a0",
   "metadata": {},
   "source": [
    "#### Spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af813787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the Spark analytics engine\n",
    "from pyspark.sql import SparkSession\n",
    "from pprint import pprint\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420e279d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema\n",
    "schema = StructType([\n",
    "  StructField(\"brand\", StringType(), nullable=False),\n",
    "  StructField(\"model\", StringType(), nullable=False),\n",
    "  StructField(\"absorption_rate\", ByteType(), nullable=True),\n",
    "  StructField(\"comfort\", ByteType(), nullable=True)\n",
    "])\n",
    "\n",
    "better_df = (spark\n",
    "             .read\n",
    "             .options(header=\"true\")\n",
    "             # Pass the predefined schema to the Reader\n",
    "             .schema(schema)\n",
    "             .csv(\"/home/repl/workspace/mnt/data_lake/landing/ratings.csv\"))\n",
    "\n",
    "pprint(better_df.dtypes)\n",
    "\n",
    "# Output:\n",
    "# [('brand', 'string'),\n",
    "#  ('model', 'string'),\n",
    "#  ('absorption_rate', 'tinyint'),\n",
    "#  ('comfort', 'tinyint')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4aae874",
   "metadata": {},
   "source": [
    "#### Droping invalid rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d6dd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "prices = (spark\n",
    "      .read\n",
    "      .options(header=\"true\", mode=\"DROPMALFORMED\")\n",
    "      .csv('landing/prices.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda07041",
   "metadata": {},
   "source": [
    "#### Cleaning data\n",
    "\n",
    "You can select the column to be transformed by using the .withColumn() method, conditionally replace those values using the pyspark.sql.functions.when function when values meet a given condition or leave them unaltered when they don’t with the .otherwise() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14944af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace nulls with arbitrary value on column subset\n",
    "ratings = ratings.fillna(4, subset=[\"comfort\"])\n",
    "\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Add/relabel the column\n",
    "categorized_ratings = ratings.withColumn(\n",
    "    \"comfort\",\n",
    "    # Express the condition in terms of column operations\n",
    "    when(col(\"comfort\") > 3, \"sufficient\").otherwise(\"insufficient\"))\n",
    "\n",
    "categorized_ratings.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c84886",
   "metadata": {},
   "source": [
    "### Running a PySpark program locally\n",
    "\n",
    "> python my_pyspark_data_pipeline.py  # script starts at least a SparkSession\n",
    " \n",
    "Conditions:\n",
    " - local installation of Spark\n",
    " - access to referenced resources\n",
    " - classpath is properly configured / the class path tells the Java Virtual Machine - which is what the Spark runs on - where to look for classes that are imported\n",
    " \n",
    " \n",
    "In daily operations you'll be using the spark-submit script\n",
    "\n",
    " - It comes with any Spark installation\n",
    " - The script sets up a launch environment for use with cluster manager and deploy mode\n",
    " - The deploy mode tells Spark where to run the driver of the Spark application: either on a dedicated master node or on one of the cluster worker nodes\n",
    " - After the setup of the launch environment spark-submit also invokes the \" main \" class or method\n",
    "\n",
    " > spark-submit \\        -> On your path, if Spark is installed <br>\n",
    " > --master \"local[*]\" \\ -> URL of the cluster manager <br>\n",
    " > --py-files PY_FILES \\ -> Comma-separated list of zip, egg or py <br>\n",
    " > MAIN_PYTHON_FILE \\    -> Path to the module to be run <br>\n",
    " > app_arguments         -> Optional arguments parsed by main scrip\n",
    "\n",
    "\n",
    "<img src=\"assets/pipelines/spark_submit.png\" style=\"width: 600px;\"/>\n",
    "\n",
    "\n",
    "Run a PySpark program locally by first zipping your code: This packaging step becomes more important when your code consists of many modules. Packaging in the zip format is done by navigating to the root folder of your pipeline using the cd command and running the following command:\n",
    "\n",
    " > zip --recurse-paths zip_file.zip pipeline_folder\n",
    "\n",
    "<img src=\"assets/pipelines/spark_pipeline.png\" style=\"height: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6d57d2",
   "metadata": {},
   "source": [
    "#### Submitting your Spark job\n",
    "\n",
    " - With the dependencies of a job ready to be distributed across a cluster’s nodes, you can submit a job to a cluster easily. To run a PySpark application locally, you need to call:\n",
    "\n",
    " > spark-submit --py-files PY_FILES MAIN_PYTHON_FILE\n",
    " \n",
    "with PY_FILES being either a zipped archive, a Python egg or separate Python files that will be placed on the PYTHONPATH environment variable of your cluster's nodes. The MAIN_PYTHON_FILE should be the entry point of your application.\n",
    "\n",
    "Example: The path of the zipped archive is spark_pipelines/pydiaper/pydiaper.zip whereas the path to your application entry point is spark_pipelines/pydiaper/pydiaper/cleaning/clean_ratings.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5d553b",
   "metadata": {},
   "source": [
    "#### Creating in-memory DataFrames\n",
    "\n",
    "Creating small datasets for unit tests is an important skill. It improves readability and understanding, because any developer looking at your code, can immediately see the inputs to some function and how they relate to the output. Additionally, you can illustrate how the function behaves with normal data and with exceptional data, like missing or incorrect fields.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014424d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "from pyspark.sql import Row\n",
    "\n",
    "Record = Row(\"country\", \"utm_campaign\", \"airtime_in_minutes\", \"start_date\", \"end_date\")\n",
    "\n",
    "# Create a tuple of records\n",
    "data = (\n",
    "  Record(\"USA\", \"DiapersFirst\", 28, date(2017, 1, 20), date(2017, 1, 27)),\n",
    "  Record(\"Germany\", \"WindelKind\", 31, date(2017, 1, 25), None),\n",
    "  Record(\"India\", \"CloseToCloth\", 32, date(2017, 1, 25), date(2017, 2, 2))\n",
    ")\n",
    "\n",
    "# Create a DataFrame from these records\n",
    "frame = spark.createDataFrame(data)\n",
    "frame.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4754b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_calculate_unit_price_in_euro():\n",
    "    record = dict(price=10, quantity=5, exchange_rate_to_euro=2.)\n",
    "    df = spark.createDataFrame([Row(**record)])\n",
    "    result = calculate_unit_price_in_euro(df)\n",
    "    expected_record = Row(**record, unit_price_in_euro=4.)\n",
    "    expected = spark.createDataFrame([expected_record])\n",
    "    assertDataFrameEqual(result, expected)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
