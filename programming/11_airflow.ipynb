{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bb2df9d",
   "metadata": {},
   "source": [
    "### Airflow\n",
    "\n",
    " - Data engineering: Taking any action involving data and turning it into a reliable, repeatable and maintanable process\n",
    " - Workflow: A set of steps to accomplish a given data engineering task e.g. copying, downloading files, filtering information, writing to a database\n",
    " - Varying levels of complexity for a workflow (from 2-3 steps to 100s)\n",
    " - Airflow is a platform to program workflows including, creation, scheduling and monitoring of tasks. It can handle complex data engineering pipelines in production\n",
    " - Airflow adds scheduling, error handling, and reporting to workflows\n",
    " - It implements workflows as DAGs - Directed Acyclic Graphs which is a set of tasks and dependencies between them\n",
    " - It is accessed via code, command-line or via web interface\n",
    " - Other workflow tools include Luigi, SSIS or Bash scripting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa908896",
   "metadata": {},
   "source": [
    "Airflow run command\n",
    " - airflow run dag_id task_id start_date\n",
    " - airflow run etl_pipeline download_file 2020-01-08\n",
    "\n",
    "Airflow help\n",
    " - \" airflow -h \" obtains further information about any Airflow command\n",
    " - \" airflow list_dags \" shows a list of the available DAGs\n",
    "\n",
    "Airflow port\n",
    " - \" airflow webserver -p PORT \" runs the server workers on PORT\n",
    " - \" airflow webserver -p 9880 \" runs the server workers on 9880"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402fe2fd",
   "metadata": {},
   "source": [
    "#### DAG\n",
    "\n",
    " - Directed: an inherent flow representing dependencies between components \n",
    " - These dependencies even implicit ones provide context on how to order the running of components\n",
    " - Acyclic: does not loop or repeat, the individual components are only executed once per run\n",
    " - Graph: a graph represents the components and their relationships between them\n",
    " - In Airflow DAGs are written in python but can use components written in other languages\n",
    " - DAGs are made up of components (typically tasks) to be executed such as operators, sensors etc\n",
    " - Dependencies are defined either explicitly or implicitly (?) so that Airflow knows which components should be run at what point within a workflow\n",
    "\n",
    "<img src=\"assets/airflow/command_line_vs_python.png\" style=\"width: 600px;\"/>\n",
    "\n",
    "#### Simple DAG examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "677e3ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the DAG object\n",
    "from airflow.models import DAG\n",
    "\n",
    "# Define the default_args dictionary\n",
    "default_args = {\n",
    "  'owner': 'dsmith',\n",
    "  'start_date': datetime(2020, 1, 14),\n",
    "  'retries': 2\n",
    "  'email':'bla@blabla.com'\n",
    "}\n",
    "\n",
    "# Instantiate the DAG object\n",
    "etl_dag = DAG(dag_id='example_etl', default_args=default_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a83690",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.models import DAG\n",
    "\n",
    "default_args = {\n",
    "  'owner': 'jdoe',\n",
    "  'email': 'jdoe@datacamp.com'\n",
    "}\n",
    "dag = DAG( 'refresh_data', default_args=default_args )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26508736",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.models import DAG\n",
    "default_args = {\n",
    "  'owner': 'jdoe',\n",
    "  'start_date': '2019-01-01'\n",
    "}\n",
    "dag = DAG( dag_id=\"etl_update\", default_args=default_args )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7ae4c5",
   "metadata": {},
   "source": [
    "#### Airflow web interface\n",
    " - A web interface that should make it easier to schedule tasks, review processes and correct issues\n",
    " - The Tree View lists the tasks and any ordering between them in a tree structure, with the ability to compress / expand the nodes.\n",
    " - The Graph View shows any tasks and their dependencies in a graph structure, along with the ability to access further details about task runs.\n",
    "- The Code view provides full access to the Python code that makes up the DAG.\n",
    "\n",
    "#### Operators\n",
    " - Represent a single task in a workflow\n",
    " - Run independently (usually), meaning that all resources needed to complete the task are contained within the operator\n",
    " - Generally do not share information (to simplify the workflows)\n",
    " - There are various operators to perform different tasks\n",
    " - For instance, the DummyOperator(task_id='example', dag=dag) can be used to represent a task for trubleshooting or a task that has not yet been implemented\n",
    " - The BashOperator executes a given Bash task or script, it requires 3 arguments and is defined as BashOperator(task_id='example', bash_command='script.sh', dag=dag)\n",
    " - Can specify environment variables for the command\n",
    " - The BashOperator allows you to specify any given Shell command or script and add it to an Airflow workflow. This can be a great start to implementing Airflow in your environment\n",
    "\n",
    "Operator \"gotchas\":\n",
    " - Not guaranteed to run in the same location (directory)\n",
    " - May require extensive use of env variables\n",
    " - Can be difficult to run tasks with elevated privileges (different user access)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98db7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the BashOperator\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "\n",
    "# Define the BashOperator \n",
    "cleanup = BashOperator(\n",
    "    task_id='cleanup_task',\n",
    "    # Define the bash_command\n",
    "    bash_command='cleanup.sh',\n",
    "    # Add the task to the dag\n",
    "    dag=analytics_dag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10379b66",
   "metadata": {},
   "source": [
    "#### Multiple BashOperators\n",
    "\n",
    "Airflow DAGs can contain many operators, each performing their defined tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2df7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a second operator to run the `consolidate_data.sh` script\n",
    "consolidate = BashOperator(\n",
    "    task_id='consolidate_task',\n",
    "    bash_command='consolidate_data.sh',\n",
    "    dag=analytics_dag)\n",
    "\n",
    "# Define a final operator to execute the `push_data.sh` script\n",
    "push_data = BashOperator(\n",
    "    task_id='pushdata_task',\n",
    "    bash_command='push_data.sh',\n",
    "    dag=analytics_dag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ba4e75",
   "metadata": {},
   "source": [
    "#### Tasks\n",
    " - Instances of operators\n",
    " - Assigned to a variable in Python\n",
    " - Within Airflow tasks are defined by their task id not the variable name\n",
    " \n",
    " \n",
    " - Tasks are either upstream or downstream\n",
    "  - Upstream tasks are those that must be completed prior other any downstream tasks\n",
    " - Dependencies can be defined using the bitshift operators\n",
    "  - \">>\" is the upstream operator\n",
    "  - \"<<\" is the downstream operator\n",
    " - Upstreams means \"before\", downstream means \"after\". Upstream tasks must be completed before downstream tasks\n",
    " \n",
    " \n",
    " - Multiple dependencies can be set like this:\n",
    "  - task1 >> task2 >> task3 >> task4\n",
    "  \n",
    "  \n",
    " - Task dependencies in the Airflow UI:\n",
    " <img src=\"assets/airflow/task_dependencies.png\" style=\"width: 600px;\"/>\n",
    " \n",
    " \n",
    "  - Chained and mixed-dependencies:\n",
    " <img src=\"assets/airflow/chained_dependencies.png\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f710b9",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Define the tasks\n",
    "task1 = BashOperator(task_id='first_task',\n",
    "                     bash_command='echo 1',\n",
    "                     dag=example_dag)\n",
    "task2 = BashOperator(task_id='second_task',\n",
    "                     bash_command='echo 2',\n",
    "                     dag=example_dag)\n",
    "# Set first_task to run before second_task\n",
    "task1 >> task2   # or task2 << task1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcfe0dd",
   "metadata": {},
   "source": [
    " - Define a BashOperator called pull_sales with a bash command of wget https://salestracking/latestinfo?json.\n",
    " - Set the pull_sales operator to run before the cleanup task.\n",
    " - Configure consolidate to run next, using the downstream operator.\n",
    " - Set push_data to run last using either bitshift operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e546478a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new pull_sales task\n",
    "pull_sales = BashOperator(\n",
    "    task_id='pullsales_task',\n",
    "    bash_command='wget https://salestracking/latestinfo?json',\n",
    "    dag=analytics_dag\n",
    ")\n",
    "\n",
    "# Set pull_sales to run prior to cleanup\n",
    "pull_sales >> cleanup\n",
    "\n",
    "# Configure consolidate to run after cleanup\n",
    "consolidate << cleanup\n",
    "\n",
    "# Set push_data to run last\n",
    "consolidate >> push_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012258a1",
   "metadata": {},
   "source": [
    "#### PythonOperator\n",
    " - Executes a Python function / callable\n",
    " - Operates similarly to BashOperator with more options\n",
    " - Can pass in arguments to the Python code\n",
    " - Arguments can be positional or keyword\n",
    " - Use the op_kwargs dictionary to pass keyword arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e8982b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.operators.python_operator import PythonOperators\n",
    "\n",
    "def printme():\n",
    "    print(\"This goes in the logs!\")\n",
    "python_task = PythonOperator(\n",
    "    task_id='simple_print',\n",
    "    python_callable=printme,\n",
    "    dag=example_dag\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47443ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sleep(length_of_time):\n",
    "    time.sleep(length_of_time)\n",
    "\n",
    "sleep_task = PythonOperator(\n",
    "    task_id='sleep',\n",
    "    python_callable=sleep,\n",
    "    op_kwargs={'length_of_time': 5}\n",
    "    dag=example_dag\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbc8c86",
   "metadata": {},
   "source": [
    "#### Additional operators\n",
    " - Found in airflow.operators or airflow.contrib.operators libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30dd69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.operators.email_operator import EmailOperator\n",
    "\n",
    "email_task = EmailOperator(\n",
    "    task_id='email_sales_report',\n",
    "    to='sales_manager@example.com',\n",
    "    subject='Automated Sales Report',\n",
    "    html_content='Attached is the latest sales report',\n",
    "    files='latest_sales.xlsx',\n",
    "    dag=example_dag\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63facf03",
   "metadata": {},
   "source": [
    "#### Example with PythonOperator and EmailOperator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba0624f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "\n",
    "def pull_file(URL, savepath):\n",
    "    r = requests.get(URL)\n",
    "    with open(savepath, 'wb') as f:\n",
    "        f.write(r.content)   \n",
    "    # Use the print method for logging\n",
    "    print(f\"File pulled from {URL} and saved to {savepath}\")\n",
    "\n",
    "\n",
    "    \n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "\n",
    "# Create the task\n",
    "pull_file_task = PythonOperator(\n",
    "    task_id='pull_file',\n",
    "    # Add the callable\n",
    "    python_callable=pull_file,\n",
    "    # Define the arguments\n",
    "    op_kwargs={'URL':'http://dataserver/sales.json', 'savepath':'latestsales.json'},\n",
    "    dag=process_sales_dag\n",
    ")\n",
    "\n",
    "\n",
    "# Add another Python task\n",
    "parse_file_task = PythonOperator(\n",
    "    task_id='parse_file',\n",
    "    # Set the function to call\n",
    "    python_callable=parse_file,\n",
    "    # Add the arguments\n",
    "    op_kwargs={'inputfile':'latestsales.json', 'outputfile':'parsedfile.json'},\n",
    "    # Add the DAG\n",
    "    dag=process_sales_dag\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416e6e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Operator\n",
    "from airflow.operators.email_operator import EmailOperator\n",
    "\n",
    "# Define the task\n",
    "email_manager_task = EmailOperator(\n",
    "    task_id='email_manager',\n",
    "    to='manager@datacamp.com',\n",
    "    subject='Latest sales JSON',\n",
    "    html_content='Attached is the latest sales JSON file as requested.',\n",
    "    files='parsedfile.json',\n",
    "    dag=process_sales_dag\n",
    ")\n",
    "\n",
    "# Set the order of tasks\n",
    "pull_file_task >> parse_file_task >> email_manager_task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bda93bd",
   "metadata": {},
   "source": [
    "#### DAG Runs\n",
    "\n",
    " - A specific instance of a workflow at a point in time\n",
    " - Can be run manually or via schedule_interval\n",
    " - Maintain state for each workflow and the tasks within\n",
    "     - running, failed, success\n",
    " - In the web interface you can find them at \"Browse Dag Runs\"\n",
    " \n",
    " - Scheduling details:\n",
    "     - start_date: datetime Python object for initial schedule of the DAG run\n",
    "     - end_date: optional attribute to stop running new DAG instances\n",
    "     - max_tries: optional attribute for how many attempts to make\n",
    "     - schedule_interval: how often to run / schedule the DAG for execution, it occurs between the start_date and end_date\n",
    "     - scheduler presets: @once, @hourly, @daily, @weekly or you can use cron format\n",
    "\n",
    "Example:\n",
    " - Set the start date of the DAG to November 1, 2019.\n",
    " - Configure the retry_delay to 20 minutes. You will learn more about the timedelta object in Chapter 3. For now, you just need to know it expects an integer value.\n",
    " - Use the cron syntax to configure a schedule of every Wednesday at 12:30pm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f0b1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the scheduling arguments as defined\n",
    "\n",
    "default_args = {\n",
    "  'owner': 'Engineering',\n",
    "  'start_date': datetime(2019, 11, 1),\n",
    "  'email': ['airflowresults@datacamp.com'],\n",
    "  'email_on_failure': False,\n",
    "  'email_on_retry': False,\n",
    "  'retries': 3,\n",
    "  'retry_delay': timedelta(minutes=20)\n",
    "}\n",
    "\n",
    "dag = DAG('update_dataflows',\n",
    "          default_args=default_args,\n",
    "          schedule_interval='30 12 * * 3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c8eb53",
   "metadata": {},
   "source": [
    "#### Full Example\n",
    " - Note that this will not be triggered before a month has passed after the start_date of Feb 15, 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49575aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from datetime import datetime\n",
    "from airflow.models import DAG\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from airflow.operators.email_operator import EmailOperator\n",
    "\n",
    "\n",
    "default_args = {\n",
    "    'owner':'sales_eng',\n",
    "    'start_date': datetime(2020, 2, 15),\n",
    "}\n",
    "\n",
    "process_sales_dag = DAG(dag_id='process_sales', default_args=default_args, schedule_interval='@monthly')\n",
    "\n",
    "\n",
    "def pull_file(URL, savepath):\n",
    "    r = requests.get(URL)\n",
    "    with open(savepath, 'w') as f:\n",
    "        f.write(r.content)\n",
    "    print(f\"File pulled from {URL} and saved to {savepath}\")\n",
    "    \n",
    "\n",
    "pull_file_task = PythonOperator(\n",
    "    task_id='pull_file',\n",
    "    # Add the callable\n",
    "    python_callable=pull_file,\n",
    "    # Define the arguments\n",
    "    op_kwargs={'URL':'http://dataserver/sales.json', 'savepath':'latestsales.json'},\n",
    "    dag=process_sales_dag\n",
    ")\n",
    "\n",
    "def parse_file(inputfile, outputfile):\n",
    "    with open(inputfile) as infile:\n",
    "        data=json.load(infile)\n",
    "        with open(outputfile, 'w') as outfile:\n",
    "            json.dump(data, outfile)\n",
    "        \n",
    "parse_file_task = PythonOperator(\n",
    "    task_id='parse_file',\n",
    "    # Set the function to call\n",
    "    python_callable=parse_file,\n",
    "    # Add the arguments\n",
    "    op_kwargs={'inputfile':'latestsales.json', 'outputfile':'parsedfile.json'},\n",
    "    # Add the DAG\n",
    "    dag=process_sales_dag\n",
    ")\n",
    "\n",
    "email_manager_task = EmailOperator(\n",
    "    task_id='email_manager',\n",
    "    to='manager@datacamp.com',\n",
    "    subject='Latest sales JSON',\n",
    "    html_content='Attached is the latest sales JSON file as requested.',\n",
    "    files='parsedfile.json',\n",
    "    dag=process_sales_dag\n",
    ")\n",
    "\n",
    "pull_file_task >> parse_file_task >> email_manager_task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7a93d0",
   "metadata": {},
   "source": [
    "#### Sensors\n",
    " - Use sensors when:\n",
    "     - uncertain when a condition will be true\n",
    "     - if you don't want to fail the intire DAG immediately but want to continue checking if a condition has been met\n",
    "     - add task repetition wihout loops\n",
    "     \n",
    "     \n",
    " - Sensors are special operators that wait for a certain condition to be true\n",
    " - Conditions can include the creation of a file, upload of a database record, certain response from a web request\n",
    " - Can define how often to check for the condition to be true\n",
    "     - mode determines how to check for the condition\n",
    "     - mode = 'poke' means it checks repeatedly\n",
    "     - mode = 'reschedule' means it give up task slot and try again later\n",
    " - Are assigned to tasks like normal operators\n",
    " - Derived from \" airflow.sensors.base_sensor_operator \"\n",
    " - poke_interval refers to how often to wait between checks\n",
    " - timeout refers to how long to wait before failing task (timeout must be significantly shorter than the schedule interval)\n",
    "\n",
    "#### Useful sensors\n",
    "\n",
    "File sensor:\n",
    " - checks for the existence of a file at a certain location\n",
    " - can check if any files exist within a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "506ab514",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.contrib.sensors.file_sensor import FileSensor\n",
    "\n",
    "file_sensor_task = FileSensor(task_id='file_sense',\n",
    "                              filepath='salesdata.csv',\n",
    "                              poke_interval=300,\n",
    "                              dag=sales_report_dag)\n",
    "\n",
    "init_sales_cleanup >> file_sensor_task >> generate_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7767e12c",
   "metadata": {},
   "source": [
    "ExternalTaskSensor\n",
    " - waits for a task in another DAG to complete\n",
    "\n",
    "HttpSensor\n",
    " - makes requests to a web URL and checks for content\n",
    "\n",
    "SqlSensor\n",
    " - runs a SQL query to check for content\n",
    "\n",
    "Find more sensor operators under\n",
    " - airflow.sensors\n",
    " - airflow.contrib.sensors\n",
    "\n",
    "\n",
    "#### Full sensor example\n",
    " - Note that this DAG is waiting for the file salesdata_ready.csv to be present before it can start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e614cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.models import DAG\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from airflow.operators.http_operator import SimpleHttpOperator\n",
    "from airflow.contrib.sensors.file_sensor import FileSensor\n",
    "\n",
    "dag = DAG(\n",
    "   dag_id = 'update_state',\n",
    "   default_args={\"start_date\": \"2019-10-01\"}\n",
    ")\n",
    "\n",
    "precheck = FileSensor(\n",
    "   task_id='check_for_datafile',\n",
    "   filepath='salesdata_ready.csv',\n",
    "   dag=dag)\n",
    "\n",
    "part1 = BashOperator(\n",
    "   task_id='generate_random_number',\n",
    "   bash_command='echo $RANDOM',\n",
    "   dag=dag\n",
    ")\n",
    "\n",
    "import sys\n",
    "def python_version():\n",
    "    return sys.version\n",
    "\n",
    "part2 = PythonOperator(\n",
    "   task_id='get_python_version',\n",
    "   python_callable=python_version,\n",
    "   dag=dag)\n",
    "   \n",
    "part3 = SimpleHttpOperator(\n",
    "   task_id='query_server_for_external_ip',\n",
    "   endpoint='https://api.ipify.org',\n",
    "   method='GET',\n",
    "   dag=dag)\n",
    "   \n",
    "precheck >> part3 >> part2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc9fa70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.models import DAG\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "from airflow.contrib.sensors.file_sensor import FileSensor\n",
    "from datetime import datetime\n",
    "\n",
    "report_dag = DAG(\n",
    "    dag_id = 'execute_report',\n",
    "    schedule_interval = \"0 0 * * *\"\n",
    ")\n",
    "\n",
    "precheck = FileSensor(\n",
    "    task_id='check_for_datafile',\n",
    "    filepath='salesdata_ready.csv',\n",
    "    start_date=datetime(2020,2,20),\n",
    "    mode='reschedule',\n",
    "    dag=report_dag\n",
    ")\n",
    "\n",
    "generate_report_task = BashOperator(\n",
    "    task_id='generate_report',\n",
    "    bash_command='generate_report.sh',\n",
    "    start_date=datetime(2020,2,20),\n",
    "    dag=report_dag\n",
    ")\n",
    "\n",
    "precheck >> generate_report_task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc366cbe",
   "metadata": {},
   "source": [
    "#### Airflow executors\n",
    " - An executor is the component that runs the tasks defined in a workflow\n",
    " - Different executors handle running the tasks differently, some may run a single task at a time on a local system, while others might split individual tasks among all the systems in a cluster\n",
    " - This is oftenr referred to as the number of worker slots available\n",
    " - Example executors:\n",
    "   - SequentialExecutor\n",
    "   - LocalExecutor\n",
    "   - CeleryExecutor\n",
    "\n",
    "SequentialExecutor:\n",
    " - default Airflow executor\n",
    " - runs one task at a time\n",
    " - useful for debugging\n",
    " - not recommended for production due to its limitations of task resources\n",
    "\n",
    "LocalExecutor:\n",
    " - runs on a single system\n",
    " - treats each task as a process on the local system and can start as many concurrent tasks as desired, requested and permitted by the system resources (CPU cores, memory etc)\n",
    " - concurrency allows for parallelism as defined by the user either unlimited or limited to a certain number of simultaneous tasks\n",
    " - can utilise all the resources of a given host system\n",
    "\n",
    "CeleryExecutor\n",
    " - uses a Celery backend as task manager\n",
    " - Celery is a general queuing system written in Python that allows multiple systems to communicate as a basic cluster\n",
    " - using a CeleryExecutor multiple Airflow systems can be configured as workflows for a given set of tasks\n",
    " - is difficult to setup and configure\n",
    " - it is a powerful choice however when one expects to have large number of DAGs or expects their processing needs to grow\n",
    "\n",
    "Determining which executor is being used:\n",
    " - in the command line use << cat airflow/airflow.cfg | grep \"executor=\" >>\n",
    " - in the command line use << airflow list_dags >> and look for the INFO output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af4b67d",
   "metadata": {},
   "source": [
    "#### Troubleshooting\n",
    "\n",
    "Common issues\n",
    " - DAGs won't run on schedule\n",
    "     - Check if the scheduler is running (the Airflow scheduler handles DAG run and task scheduling, if it is not running no tasks can run)\n",
    "     - An Error that says \" the scheduler does not appear to be running \" will normally show up\n",
    "     - Fix by running \" airflow scheduler \" from the command line\n",
    "     - Another reason might be that the \" schedule_interval \" argument hasn't passed.\n",
    "     - Modify accordingly to meet your requirements\n",
    "     - Lastly, it might be that there are not enought free tasks within the executor to run\n",
    "     - If so change the executor type (or add more resources!)\n",
    "     \n",
    "     \n",
    " - DAGs won't load\n",
    "     - DAG not in web UI\n",
    "     - DAG not in \" airflow list_dags \"\n",
    "     - Verify DAG files are in the correct folder\n",
    "     - Determine the DAG folder by examining the airflow.cfg file\n",
    "     - Use \" head airflow/airflow.cfg \", the dags_folder shows the path\n",
    "     \n",
    "     <img src=\"assets/airflow/airflow_cfg.png\" style=\"width: 600px;\"/>\n",
    "\n",
    "\n",
    " - Syntax errors\n",
    "     - Most common reason a DAG file won't appear\n",
    "     - Kinda difficult to find errors in DAG\n",
    "     - Run \" airflow list_dags \" -> python3 <dag_file.py>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9b33cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the command line run:\n",
    "airflow list_dags\n",
    "\n",
    "# you'll find out what is the dags_folder\n",
    "cd /home/repl/workspace/dags/\n",
    "\n",
    "# open the dag to figure out what is going on\n",
    "nano dag_file.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412880e2",
   "metadata": {},
   "source": [
    "#### Missing DAG\n",
    "\n",
    "Lol: Your manager calls you before you're about to leave for the evening and wants to know why a new DAG workflow she's created isn't showing up in the system. She needs this DAG called execute_report to appear in the system so she can properly schedule it for some tests before she leaves on a trip.\n",
    "\n",
    " - Airflow is configured using the ~/airflow/airflow.cfg file.\n",
    "\n",
    " - Examine the DAG for any errors and fix those.\n",
    " - Determine if the DAG has loaded after fixing the errors.\n",
    " - If not, determine why the DAG has not loaded and fix the final issue. - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b36adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.models import DAG\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "from datetime import datetime\n",
    "\n",
    "sample_dag = DAG(\n",
    "    dag_id = 'sample_dag',\n",
    "    schedule_interval = \"0 0 * * *\"\n",
    ")\n",
    "\n",
    "sample_task = BashOperator(\n",
    "    task_id='sample',\n",
    "    bash_command='generate_sample.sh',\n",
    "    start_date=datetime(2020,2,20),\n",
    "    dag=sample_dag\n",
    ")\n",
    "\n",
    "\n",
    "from airflow.models import DAG\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "from airflow.contrib.sensors.file_sensor import FileSensor\n",
    "from datetime import datetime\n",
    "\n",
    "report_dag = DAG(\n",
    "    dag_id = 'execute_report',\n",
    "    schedule_interval = \"0 0 * * *\"\n",
    ")\n",
    "\n",
    "precheck = FileSensor(\n",
    "    task_id='check_for_datafile',\n",
    "    filepath='salesdata_ready.csv',\n",
    "    start_date=datetime(2020,2,20),\n",
    "    mode='poke',\n",
    "    dag=report_dag)\n",
    "\n",
    "generate_report_task = BashOperator(\n",
    "    task_id='generate_report',\n",
    "    bash_command='generate_report.sh',\n",
    "    start_date=datetime(2020,2,20),\n",
    "    dag=report_dag\n",
    ")\n",
    "\n",
    "precheck >> generate_report_task\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c9c3ad",
   "metadata": {},
   "source": [
    "#### SLAs\n",
    " - SLA = service level agreement\n",
    " - Within Airflow, this is the amount of time a task or a DAG should require to run\n",
    " - An SLA miss is any time the task or the dag does not meet the expected timing\n",
    " - If an SLA is missed, an email is sent out and a log is stored\n",
    " - SLA misses can be viewed in the web UI (Browse -> SLA Misses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528fb578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to define an SLA\n",
    "task1 = BashOperator(task_id='sla_task',\n",
    "                   bash_command='runcode.sh',\n",
    "                   sla=timedelta(seconds=30), dag=dag)\n",
    "\n",
    "default_args={\n",
    " 'sla': timedelta(minutes=20)\n",
    " 'start_date': datetime(2020,2,20)\n",
    "}\n",
    "\n",
    "dag = DAG('sla_dag', default_args=default_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a456bc",
   "metadata": {},
   "source": [
    "#### SLA example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cabc4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the timedelta object\n",
    "from datetime import timedelta\n",
    "### SLA example 1\n",
    "# Create the dictionary entry\n",
    "default_args = {\n",
    "  'start_date': datetime(2020, 2, 20),\n",
    "  'sla': timedelta(minutes=30)\n",
    "}\n",
    "\n",
    "# Add to the DAG\n",
    "test_dag = DAG('test_workflow', default_args=default_args, schedule_interval='@None')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3de1b15",
   "metadata": {},
   "source": [
    "#### SLA example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54c5d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the timedelta object\n",
    "from datetime import timedelta\n",
    "\n",
    "test_dag = DAG('test_workflow', start_date=datetime(2020,2,20), schedule_interval='@None')\n",
    "\n",
    "# Create the task with the SLA\n",
    "task1 = BashOperator(task_id='first_task',\n",
    "                     sla=timedelta(hours=3),\n",
    "                     bash_command='initialize_data.sh',\n",
    "                     dag=test_dag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3418a159",
   "metadata": {},
   "source": [
    "#### SLA example 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb6123d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the email task\n",
    "email_report = EmailOperator(\n",
    "        task_id='email_report',\n",
    "        to='airflow@datacamp.com',\n",
    "        subject='Airflow Monthly Report',\n",
    "        html_content=\"\"\"Attached is your monthly workflow report - please refer to it for more detail\"\"\",\n",
    "        files=['monthly_report.pdf'],\n",
    "        dag=report_dag\n",
    ")\n",
    "\n",
    "# Set the email task to run after the report is generated\n",
    "email_report << generate_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c4df26",
   "metadata": {},
   "source": [
    "#### SLA example 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9f6430",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.models import DAG\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "from airflow.contrib.sensors.file_sensor import FileSensor\n",
    "from datetime import datetime\n",
    "\n",
    "default_args={\n",
    "    'email': ['airflowalerts@datacamp.com','airflowadmin@datacamp.com'],\n",
    "    'email_on_failure': True,\n",
    "    'email_on_success': True,\n",
    "}\n",
    "\n",
    "report_dag = DAG(\n",
    "    dag_id = 'execute_report',\n",
    "    schedule_interval = \"0 0 * * *\",\n",
    "    default_args=default_args\n",
    ")\n",
    "\n",
    "precheck = FileSensor(\n",
    "    task_id='check_for_datafile',\n",
    "    filepath='salesdata_ready.csv',\n",
    "    start_date=datetime(2020,2,20),\n",
    "    mode='reschedule',\n",
    "    dag=report_dag)\n",
    "\n",
    "generate_report_task = BashOperator(\n",
    "    task_id='generate_report',\n",
    "    bash_command='generate_report.sh',\n",
    "    start_date=datetime(2020,2,20),\n",
    "    dag=report_dag\n",
    ")\n",
    "\n",
    "precheck >> generate_report_task\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc994ca",
   "metadata": {},
   "source": [
    "### Templates\n",
    " - Airflow templates are created using the Jinja template language\n",
    " - Allow for substituting information during a DAG run\n",
    " - Provide added flexibility when defining tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37c9cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "templated_command=\"\"\"\n",
    "  echo \"Reading {{ params.filename }}\"\n",
    "\"\"\"\n",
    "\n",
    "t1 = BashOperator(task_id='template_task',\n",
    "       bash_command=templated_command,\n",
    "       params={'filename': 'file1.txt'}\n",
    "       dag=example_dag)\n",
    "\n",
    "t2 = BashOperator(task_id='template_task',\n",
    "       bash_command=templated_command,\n",
    "       params={'filename': 'file2.txt'}\n",
    "       dag=example_dag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee110f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.models import DAG\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "from datetime import datetime\n",
    "\n",
    "default_args = {\n",
    "  'start_date': datetime(2020, 4, 15),\n",
    "}\n",
    "\n",
    "cleandata_dag = DAG('cleandata',\n",
    "                    default_args=default_args,\n",
    "                    schedule_interval='@daily')\n",
    "\n",
    "# Create a templated command to execute\n",
    "# 'bash cleandata.sh datestring'\n",
    "templated_command = \"\"\" bash cleandata.sh {{ ds_nodash }} \"\"\"\n",
    "\n",
    "\n",
    "# Modify clean_task to use the templated command\n",
    "clean_task = BashOperator(task_id='cleandata_task',\n",
    "                          bash_command=templated_command,\n",
    "                          dag=cleandata_dag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ddac45",
   "metadata": {},
   "source": [
    " - Modify the templated command to handle a second argument called filename\n",
    " - Change the first BashOperator to pass the filename salesdata.txt to the command\n",
    " - Add a new BashOperator called clean_task2 to use a second filename supportdata.txt\n",
    " - Set clean_task2 downstream of clean_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ca53ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.models import DAG\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "from datetime import datetime\n",
    "\n",
    "default_args = {\n",
    "  'start_date': datetime(2020, 4, 15),\n",
    "}\n",
    "\n",
    "cleandata_dag = DAG('cleandata',\n",
    "                    default_args=default_args,\n",
    "                    schedule_interval='@daily')\n",
    "\n",
    "# Modify the templated command to handle a\n",
    "# second argument called filename.\n",
    "templated_command = \"\"\"\n",
    "  bash cleandata.sh {{ ds_nodash }} {{params.filename}}\n",
    "\"\"\"\n",
    "\n",
    "# Modify clean_task to pass the new argument\n",
    "clean_task = BashOperator(task_id='cleandata_task',\n",
    "                          bash_command=templated_command,\n",
    "                          params={'filename': 'salesdata.txt'},\n",
    "                          dag=cleandata_dag)\n",
    "\n",
    "# Create a new BashOperator clean_task2\n",
    "clean_task2 = BashOperator(task_id='cleandata_task2',\n",
    "                           bash_command=templated_command,\n",
    "                          params={'filename': 'supportdata.txt'},\n",
    "                          dag=cleandata_dag)\n",
    "                           \n",
    "# Set the operator dependencies\n",
    "clean_task >> clean_task2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93df73cb",
   "metadata": {},
   "source": [
    "#### More complex templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d6abde",
   "metadata": {},
   "outputs": [],
   "source": [
    "templated_command=\"\"\"\n",
    "{% for filename in params.filenames %}\n",
    "  echo \"Reading {{ filename }}\"\n",
    "{% endfor %}\n",
    "\"\"\"\n",
    "t1 = BashOperator(task_id='template_task',\n",
    "       bash_command=templated_command,\n",
    "       params={'filenames': ['file1.txt', 'file2.txt']}\n",
    "       dag=example_dag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce02f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.models import DAG\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "from datetime import datetime\n",
    "\n",
    "filelist = [f'file{x}.txt' for x in range(30)]\n",
    "\n",
    "default_args = {\n",
    "  'start_date': datetime(2020, 4, 15),\n",
    "}\n",
    "\n",
    "cleandata_dag = DAG('cleandata',\n",
    "                    default_args=default_args,\n",
    "                    schedule_interval='@daily')\n",
    "\n",
    "# Modify the template to handle multiple files in a \n",
    "# single run.\n",
    "templated_command = \"\"\"\n",
    "  <% for filename in params.filenames %>\n",
    "  bash cleandata.sh {{ ds_nodash }} {{ filename }};\n",
    "  <% endfor %>\n",
    "\"\"\"\n",
    "\n",
    "# Modify clean_task to use the templated command\n",
    "clean_task = BashOperator(task_id='cleandata_task',\n",
    "                          bash_command=templated_command,\n",
    "                          params={'filenames': filelist},\n",
    "                          dag=cleandata_dag)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0842e261",
   "metadata": {},
   "source": [
    "#### Variables\n",
    " - Airflow built-in runtime variables\n",
    " - Provides assorted information about DAG runs, tasks and even the system configuration\n",
    " - Examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdc9bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YYYY-MM-DD\n",
    "Execution Date: {{ ds }}\n",
    "\n",
    "# YYYYMMDD\n",
    "Execution Date, no dashes: {{ ds_nodash }}\n",
    "\n",
    "# YYYY-MM-DD\n",
    "Previous Execution date: {{ prev_ds }}\n",
    "\n",
    "# YYYYMMDD\n",
    "Prev Execution date, no dashes: {{ prev_ds_nodash }}  \n",
    "\n",
    "DAG object: {{ dag }}\n",
    "\n",
    "Airflow config object: {{ conf }}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8e1498",
   "metadata": {},
   "source": [
    "#### Macros\n",
    "\n",
    "<img src=\"assets/airflow/macros.png\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cd436b",
   "metadata": {},
   "source": [
    " - Create a Python string that represents the email content you wish to send. Use the substitutions for the current date string (with dashes) and a variable called username.\n",
    " - Create the EmailOperator task using the template string for the html_content.\n",
    " - Set the subject field to a macro call using macros.uuid.uuid4(). This simply provides a string of a universally unique identifier as the subject field.\n",
    " - Assign the params dictionary as appropriate with the username of testemailuser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1725ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.models import DAG\n",
    "from airflow.operators.email_operator import EmailOperator\n",
    "from datetime import datetime\n",
    "\n",
    "# Create the string representing the html email content\n",
    "html_email_str = \"\"\"\n",
    "Date: {{ ds }}\n",
    "Username: {{ params.username }}\n",
    "\"\"\"\n",
    "\n",
    "email_dag = DAG('template_email_test',\n",
    "                default_args={'start_date': datetime(2020, 4, 15)},\n",
    "                schedule_interval='@weekly')\n",
    "                \n",
    "email_task = EmailOperator(task_id='email_task',\n",
    "                           to='testuser@datacamp.com',\n",
    "                           subject=\"{{ macros.uuid.uuid4() }}\",\n",
    "                           html_content=html_email_str,\n",
    "                           params={'username': 'testemailuser'},\n",
    "                           dag=email_dag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b513e4",
   "metadata": {},
   "source": [
    "#### Branching\n",
    " - Provides conditional logic\n",
    " - Uses BranchPythonOperator\n",
    " - from airflow.operators.python_operator import BranchPythonOperator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5431c1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def branch_test(**kwargs):\n",
    "    if int(kwargs['ds_nodash']) % 2 == 0:\n",
    "        return 'even_day_task'\n",
    "    else:\n",
    "        return 'odd_day_task'\n",
    "    \n",
    "branch_task = BranchPythonOperator(task_id='branch_task',dag=dag,\n",
    "       provide_context=True,\n",
    "       python_callable=branch_test)\n",
    "\n",
    "start_task >> branch_task >> even_day_task >> even_day_task2\n",
    "branch_task >> odd_day_task >> odd_day_task2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59273d5f",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72acd1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to determine if years are different\n",
    "def year_check(**kwargs):\n",
    "    current_year = int(kwargs['ds_nodash'][0:4])\n",
    "    previous_year = int(kwargs['prev_ds_nodash'][0:4])\n",
    "    if current_year == previous_year:\n",
    "        return 'current_year_task'\n",
    "    else:\n",
    "        return 'new_year_task'\n",
    "\n",
    "# Define the BranchPythonOperator\n",
    "branch_task = BranchPythonOperator(task_id='branch_task', dag=branch_dag,\n",
    "                                   python_callable=year_check, provide_context=True)\n",
    "# Define the dependencies\n",
    "branch_dag >> current_year_task\n",
    "branch_dag >> new_year_task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb92907",
   "metadata": {},
   "source": [
    "<img src=\"assets/airflow/running_dags.png\" style=\"width: 600px;\"/>\n",
    "<img src=\"assets/airflow/operators.png\" style=\"width: 600px;\"/>\n",
    "<img src=\"assets/airflow/templates.png\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09afc32c",
   "metadata": {},
   "source": [
    "#### Pipeline Demo\n",
    " - Update the DAG in pipeline.py to import the needed operators.\n",
    " - Run the sense_file task from the command line and look for any errors. Use the command airflow test and the appropriate arguments to run the command. For the last argument, use a -1 instead of a specific date.\n",
    " - Determine why the sense_file task does not complete and remedy this using the editor.\n",
    " - Re-test the sense_file task and verify the problem is fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d0f40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the following to test the etl_update dag\n",
    "# repl:~/workspace$ airflow test etl_update sense_file -1\n",
    "\n",
    "# Fix by creating the missing startprocess.txt file\n",
    "\n",
    "# Then run pipeline.py\n",
    "\n",
    "from airflow.models import DAG\n",
    "from airflow.contrib.sensors.file_sensor import FileSensor\n",
    "\n",
    "# Import the needed operators\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from datetime import date, datetime\n",
    "\n",
    "def process_data(**context):\n",
    "    file = open('/home/repl/workspace/processed_data.tmp', 'w')\n",
    "    file.write(f'Data processed on {date.today()}')\n",
    "    file.close()\n",
    "\n",
    "    \n",
    "dag = DAG(dag_id='etl_update', default_args={'start_date': datetime(2020,4,1)})\n",
    "\n",
    "sensor = FileSensor(task_id='sense_file', \n",
    "                    filepath='/home/repl/workspace/startprocess.txt',\n",
    "                    poke_interval=5,\n",
    "                    timeout=15,\n",
    "                    dag=dag)\n",
    "\n",
    "bash_task = BashOperator(task_id='cleanup_tempfiles', \n",
    "                         bash_command='rm -f /home/repl/*.tmp',\n",
    "                         dag=dag)\n",
    "\n",
    "python_task = PythonOperator(task_id='run_processing', \n",
    "                             python_callable=process_data,\n",
    "                             dag=dag)\n",
    "\n",
    "sensor >> bash_task >> python_task\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235dde6f",
   "metadata": {},
   "source": [
    " - Add an SLA of 90 minutes to the DAG.\n",
    " - Update the FileSensor object to check for files every 45 seconds.\n",
    " - Modify the python_task to send Airflow variables to the callable. Note that the callable is configured to accept the variables using the provide_context argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb95bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.models import DAG\n",
    "from airflow.contrib.sensors.file_sensor import FileSensor\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from dags.process import process_data\n",
    "from datetime import timedelta, datetime\n",
    "\n",
    "# Update the default arguments and apply them to the DAG\n",
    "default_args = {\n",
    "  'start_date': datetime(2019,1,1),\n",
    "  'sla':timedelta(minutes=90)\n",
    "}\n",
    "\n",
    "dag = DAG(dag_id='etl_update', default_args=default_args)\n",
    "\n",
    "sensor = FileSensor(task_id='sense_file', \n",
    "                    filepath='/home/repl/workspace/startprocess.txt',\n",
    "                    poke_interval=45,\n",
    "                    dag=dag)\n",
    "\n",
    "bash_task = BashOperator(task_id='cleanup_tempfiles', \n",
    "                         bash_command='rm -f /home/repl/*.tmp',\n",
    "                         dag=dag)\n",
    "\n",
    "python_task = PythonOperator(task_id='run_processing', \n",
    "                             python_callable=process_data,\n",
    "                             provide_context=True,\n",
    "                             dag=dag)\n",
    "\n",
    "sensor >> bash_task >> python_task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dc5bcb",
   "metadata": {},
   "source": [
    " - Import the necessary operators.\n",
    " - Configure the EmailOperator to provide the specific data to the callable.\n",
    " - Complete the branch callable as necessary to point to the email_report_task or no_email_task.\n",
    " - Configure the branch operator to properly check for the condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3dfe23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process.py\n",
    "\n",
    "from datetime import date\n",
    "\n",
    "def process_data(**kwargs):\n",
    "    file = open(\"/home/repl/workspace/processed_data-\" + kwargs['ds'] + \".tmp\", \"w\")\n",
    "    file.write(f\"Data processed on {date.today()}\")\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f082e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline.py\n",
    "\n",
    "from airflow.models import DAG\n",
    "from airflow.contrib.sensors.file_sensor import FileSensor\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from airflow.operators.python_operator import BranchPythonOperator\n",
    "from airflow.operators.dummy_operator import DummyOperator\n",
    "from airflow.operators.email_operator import EmailOperator\n",
    "from dags.process import process_data\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Update the default arguments and apply them to the DAG.\n",
    "\n",
    "default_args = {\n",
    "  'start_date': datetime(2019,1,1),\n",
    "  'sla': timedelta(minutes=90)\n",
    "}\n",
    "    \n",
    "dag = DAG(dag_id='etl_update', default_args=default_args)\n",
    "\n",
    "sensor = FileSensor(task_id='sense_file', \n",
    "                    filepath='/home/repl/workspace/startprocess.txt',\n",
    "                    poke_interval=45,\n",
    "                    dag=dag)\n",
    "\n",
    "bash_task = BashOperator(task_id='cleanup_tempfiles', \n",
    "                         bash_command='rm -f /home/repl/*.tmp',\n",
    "                         dag=dag)\n",
    "\n",
    "python_task = PythonOperator(task_id='run_processing', \n",
    "                             python_callable=process_data,\n",
    "                             provide_context=True,\n",
    "                             dag=dag)\n",
    "\n",
    "email_subject=\"\"\"\n",
    "  Email report for {{ params.department }} on {{ ds_nodash }}\n",
    "\"\"\"\n",
    "\n",
    "email_report_task = EmailOperator(task_id='email_report_task',\n",
    "                                  to='sales@mycompany.com',\n",
    "                                  subject=email_subject,\n",
    "                                  html_content='',\n",
    "                                  params={'department': 'Data subscription services'},\n",
    "                                  dag=dag)\n",
    "\n",
    "no_email_task = DummyOperator(task_id='no_email_task', dag=dag)\n",
    "\n",
    "def check_weekend(**kwargs):\n",
    "    dt = datetime.strptime(kwargs['execution_date'],\"%Y-%m-%d\")\n",
    "    # If dt.weekday() is 0-4, it's Monday - Friday. If 5 or 6, it's Sat / Sun.\n",
    "    if (dt.weekday() < 5):\n",
    "        return 'email_report_task'\n",
    "    else:\n",
    "        return 'no_email_task'\n",
    "    \n",
    "branch_task = BranchPythonOperator(task_id='check_if_weekend',\n",
    "                                   python_callable=check_weekend,\n",
    "                                   provide_context=True,\n",
    "                                   dag=dag)\n",
    "\n",
    "    \n",
    "sensor >> bash_task >> python_task\n",
    "\n",
    "python_task >> branch_task >> [email_report_task, no_email_task]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
